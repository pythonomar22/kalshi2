{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "import base64\n",
    "import json\n",
    "from pathlib import Path\n",
    "from cryptography.hazmat.primitives import serialization, hashes\n",
    "from cryptography.hazmat.primitives.asymmetric import padding, rsa\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import re\n",
    "import glob # For finding files\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logger_name = f\"kalshi_NTM_fetch_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "logger = logging.getLogger(logger_name)\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    # Optional: Add file handler for persistent logs\n",
    "    # log_file_path = f\"{logger_name}.log\"\n",
    "    # fh = logging.FileHandler(log_file_path)\n",
    "    # fh.setFormatter(formatter)\n",
    "    # logger.addHandler(fh)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "KALSHI_API_KEY_ID = os.getenv(\"KALSHI_API_KEY_ID\")\n",
    "KALSHI_PRIVATE_KEY_PATH = os.getenv(\"KALSHI_PRIVATE_KEY_PATH\")\n",
    "KALSHI_BASE_URL = \"\"\n",
    "\n",
    "IS_DEMO_MODE = os.getenv(\"KALSHI_DEMO_MODE\", \"false\").lower() == \"true\"\n",
    "if IS_DEMO_MODE:\n",
    "    logger.info(\"KALSHI: Running in DEMO mode.\")\n",
    "    KALSHI_BASE_URL = \"https://demo-api.kalshi.co\"\n",
    "    KALSHI_DEMO_API_KEY_ID = os.getenv(\"KALSHI_DEMO_API_KEY_ID\")\n",
    "    KALSHI_DEMO_PRIVATE_KEY_PATH = os.getenv(\"KALSHI_DEMO_PRIVATE_KEY_PATH\")\n",
    "    if KALSHI_DEMO_API_KEY_ID: KALSHI_API_KEY_ID = KALSHI_DEMO_API_KEY_ID\n",
    "    if KALSHI_DEMO_PRIVATE_KEY_PATH: KALSHI_PRIVATE_KEY_PATH = KALSHI_DEMO_PRIVATE_KEY_PATH\n",
    "else:\n",
    "    logger.info(\"KALSHI: Running in PRODUCTION mode.\")\n",
    "    KALSHI_BASE_URL = \"https://api.elections.kalshi.com\"\n",
    "\n",
    "# --- Auth Functions (copied from discover.ipynb) ---\n",
    "private_key_global = None\n",
    "\n",
    "def load_private_key(file_path: str) -> rsa.RSAPrivateKey | None:\n",
    "    global private_key_global\n",
    "    if not file_path:\n",
    "        logger.error(\"Private key file path is not provided.\")\n",
    "        return None\n",
    "    try:\n",
    "        expanded_path = Path(file_path).expanduser().resolve()\n",
    "        if not expanded_path.exists():\n",
    "            logger.error(f\"Private key file does not exist at resolved path: {expanded_path}\")\n",
    "            private_key_global = None\n",
    "            return None\n",
    "        with open(expanded_path, \"rb\") as key_file:\n",
    "            private_key_global = serialization.load_pem_private_key(key_file.read(), password=None)\n",
    "        logger.info(f\"Private key loaded successfully from {expanded_path}\")\n",
    "        return private_key_global\n",
    "    except FileNotFoundError: # Should be caught by exists() check, but good to have\n",
    "        logger.error(f\"Private key file not found: {file_path}\")\n",
    "        private_key_global = None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading private key from {file_path}: {e}\")\n",
    "        private_key_global = None\n",
    "        return None\n",
    "\n",
    "def sign_pss_text(private_key: rsa.RSAPrivateKey, text: str) -> str | None:\n",
    "    # ... (same as your Cell 1) ...\n",
    "    if not private_key:\n",
    "        logger.error(\"Private key not available for signing.\")\n",
    "        return None\n",
    "    message = text.encode('utf-8')\n",
    "    try:\n",
    "        signature = private_key.sign(\n",
    "            message, padding.PSS(mgf=padding.MGF1(hashes.SHA256()), salt_length=padding.PSS.DIGEST_LENGTH),\n",
    "            hashes.SHA256()\n",
    "        )\n",
    "        return base64.b64encode(signature).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during signing: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_kalshi_auth_headers(method: str, path: str) -> dict | None:\n",
    "    # ... (same as your Cell 1) ...\n",
    "    if not private_key_global:\n",
    "        logger.error(\"Global private_key_global not loaded. Cannot create auth headers.\")\n",
    "        return None\n",
    "    if not KALSHI_API_KEY_ID:\n",
    "        logger.error(\"Global KALSHI_API_KEY_ID not set. Cannot create auth headers.\")\n",
    "        return None\n",
    "        \n",
    "    timestamp_ms_str = str(int(time.time() * 1000))\n",
    "    if not path.startswith('/'):\n",
    "        path = '/' + path\n",
    "    message_to_sign = timestamp_ms_str + method.upper() + path\n",
    "    signature = sign_pss_text(private_key_global, message_to_sign)\n",
    "    if signature is None: return None\n",
    "    \n",
    "    return {\n",
    "        'accept': 'application/json',\n",
    "        'KALSHI-ACCESS-KEY': KALSHI_API_KEY_ID,\n",
    "        'KALSHI-ACCESS-SIGNATURE': signature,\n",
    "        'KALSHI-ACCESS-TIMESTAMP': timestamp_ms_str\n",
    "    }\n",
    "\n",
    "# --- Initialize private key ---\n",
    "if not (KALSHI_API_KEY_ID and KALSHI_PRIVATE_KEY_PATH):\n",
    "    logger.critical(\"CRITICAL: KALSHI_API_KEY_ID or KALSHI_PRIVATE_KEY_PATH not found.\")\n",
    "else:\n",
    "    load_private_key(KALSHI_PRIVATE_KEY_PATH)\n",
    "\n",
    "if private_key_global:\n",
    "    logger.info(\"Kalshi client setup complete. Private key loaded.\")\n",
    "else:\n",
    "    logger.error(\"Kalshi client setup failed: Private key could not be loaded. API calls will fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Kalshi API Request Function\n",
    "\n",
    "def kalshi_api_get_request(endpoint_path: str, params: dict = None, timeout: int = 20) -> dict | None:\n",
    "    \"\"\"\n",
    "    Makes an authenticated GET request to the Kalshi API.\n",
    "    Allows overriding timeout.\n",
    "    \"\"\"\n",
    "    if not private_key_global:\n",
    "        logger.error(\"Private key is not loaded. Cannot make API request.\")\n",
    "        return None\n",
    "    if not KALSHI_BASE_URL:\n",
    "        logger.error(\"KALSHI_BASE_URL is not set. Cannot make API request.\")\n",
    "        return None\n",
    "\n",
    "    if not endpoint_path.startswith('/'):\n",
    "        endpoint_path = '/' + endpoint_path\n",
    "        \n",
    "    full_url = f\"{KALSHI_BASE_URL}{endpoint_path}\"\n",
    "    auth_headers = get_kalshi_auth_headers(\"GET\", endpoint_path)\n",
    "    if not auth_headers:\n",
    "        logger.error(f\"Failed to generate authentication headers for path: {endpoint_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # logger.info(f\"Making GET request to: {full_url} with params: {params}\") # Can be too verbose\n",
    "        response = requests.get(full_url, headers=auth_headers, params=params, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error: {http_err} | Status: {response.status_code if 'response' in locals() else 'N/A'} | URL: {full_url} | Response: {response.text if 'response' in locals() else 'N/A'}\")\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        logger.error(f\"Connection error: {conn_err} | URL: {full_url}\")\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        logger.error(f\"Timeout error: {timeout_err} | URL: {full_url}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        logger.error(f\"Request error: {req_err} | URL: {full_url}\")\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"JSON decode error | URL: {full_url} | Response: {response.text[:500] if 'response' in locals() else 'N/A'}...\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration & Utility Functions for NTM Download\n",
    "\n",
    "# --- Date Range for Historical Kalshi Data ---\n",
    "# This date range is NOT used by the current Cell 4 logic, which processes a predefined CSV.\n",
    "# Kept here for context or if you switch back to date-range based discovery.\n",
    "START_DATE_HISTORICAL_STR = \"2024-04-01\"\n",
    "YESTERDAY = (dt.datetime.now(timezone.utc) - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE_HISTORICAL_STR = YESTERDAY\n",
    "\n",
    "# --- NTM (Near-The-Money) Filter Configuration ---\n",
    "NTM_PERCENTAGE_THRESHOLD = 0.025 # e.g., 7.5% around BTC price.\n",
    "logger.info(f\"Using NTM Percentage Threshold: {NTM_PERCENTAGE_THRESHOLD*100:.2f}%\")\n",
    "\n",
    "# --- Kalshi Event Title Filter (Not directly used if processing a pre-filtered ticker list, but good to have) ---\n",
    "EVENT_TITLE_REGEX_PATTERN = r\"Bitcoin price .*? at \\d{1,2}(?:am|pm)? EDT\\?\"\n",
    "EVENT_TITLE_REGEX = re.compile(EVENT_TITLE_REGEX_PATTERN, re.IGNORECASE)\n",
    "# logger.info(f\"Using event title regex pattern: '{EVENT_TITLE_REGEX_PATTERN}'\") # Less relevant for CSV processing\n",
    "\n",
    "# --- Data Directories ---\n",
    "BINANCE_DATA_BASE_DIR = Path(\"./binance_data\")\n",
    "KALSHI_ORGANIZED_NTM_DATA_DIR = Path(\"./kalshi_data\") # Changed as per your request\n",
    "KALSHI_ORGANIZED_NTM_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"NTM Kalshi data will be saved under: {KALSHI_ORGANIZED_NTM_DATA_DIR}\")\n",
    "\n",
    "# --- Binance Data Cache & Loading ---\n",
    "_binance_day_data_cache = {}\n",
    "\n",
    "def clear_binance_day_cache():\n",
    "    global _binance_day_data_cache\n",
    "    _binance_day_data_cache = {}\n",
    "\n",
    "def load_binance_day_data(date_str_yyyy_mm_dd: str) -> pd.DataFrame | None:\n",
    "    global _binance_day_data_cache\n",
    "    if date_str_yyyy_mm_dd in _binance_day_data_cache:\n",
    "        return _binance_day_data_cache[date_str_yyyy_mm_dd]\n",
    "    filename_base = f\"BTCUSDT-1m-{date_str_yyyy_mm_dd}\"\n",
    "    filepath = BINANCE_DATA_BASE_DIR / filename_base / f\"{filename_base}.csv\"\n",
    "    if not filepath.exists():\n",
    "        logger.warning(f\"Binance data file not found for {date_str_yyyy_mm_dd} at {filepath}\")\n",
    "        _binance_day_data_cache[date_str_yyyy_mm_dd] = None\n",
    "        return None\n",
    "    try:\n",
    "        column_names = [\"open_time_raw\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "                        \"close_time_ms\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "                        \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"]\n",
    "        df = pd.read_csv(filepath, header=None, names=column_names)\n",
    "        if df.empty:\n",
    "            logger.warning(f\"Binance data file {filepath} is empty.\")\n",
    "            _binance_day_data_cache[date_str_yyyy_mm_dd] = None\n",
    "            return None\n",
    "        df['timestamp_s'] = df['open_time_raw'] // 1_000_000\n",
    "        df.set_index('timestamp_s', inplace=True)\n",
    "        df['close'] = pd.to_numeric(df['close'])\n",
    "        _binance_day_data_cache[date_str_yyyy_mm_dd] = df\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading Binance data from {filepath}: {e}\")\n",
    "        _binance_day_data_cache[date_str_yyyy_mm_dd] = None\n",
    "        return None\n",
    "\n",
    "def get_btc_price_at_timestamp(target_timestamp_s: int) -> float | None:\n",
    "    target_dt = dt.datetime.fromtimestamp(target_timestamp_s, tz=timezone.utc)\n",
    "    date_str_needed = target_dt.strftime(\"%Y-%m-%d\")\n",
    "    binance_df = load_binance_day_data(date_str_needed)\n",
    "    if binance_df is None or binance_df.empty: return None\n",
    "    try:\n",
    "        idx_pos = binance_df.index.searchsorted(target_timestamp_s, side='right')\n",
    "        if idx_pos == 0:\n",
    "            if target_dt.hour == 0 and target_dt.minute < 5:\n",
    "                prev_date_dt = target_dt - timedelta(days=1)\n",
    "                prev_date_str = prev_date_dt.strftime(\"%Y-%m-%d\")\n",
    "                binance_df_prev = load_binance_day_data(prev_date_str)\n",
    "                if binance_df_prev is not None and not binance_df_prev.empty:\n",
    "                    idx_pos_prev = binance_df_prev.index.searchsorted(target_timestamp_s, side='right')\n",
    "                    if idx_pos_prev > 0: return binance_df_prev.iloc[idx_pos_prev - 1]['close']\n",
    "            return None\n",
    "        return float(binance_df.iloc[idx_pos - 1]['close'])\n",
    "    except Exception: return None # Simplified error handling for brevity\n",
    "\n",
    "# --- Kalshi Ticker Parsing & Date Utilities ---\n",
    "def get_event_resolution_details(ticker_string: str | None):\n",
    "    if not ticker_string: return None\n",
    "    event_match = re.match(r\"^(.*?)-(\\d{2}[A-Z]{3}\\d{2})(\\d{2})$\", ticker_string)\n",
    "    market_match = re.match(r\"^(.*?)-(\\d{2}[A-Z]{3}\\d{2})(\\d{2})-(T(\\d+\\.?\\d*))$\", ticker_string)\n",
    "    match_to_use = market_match if market_match else event_match\n",
    "    if not match_to_use: return None\n",
    "    groups = match_to_use.groups()\n",
    "    series, date_str_yymmmdd, hour_str_edt = groups[0], groups[1], groups[2]\n",
    "    strike_price = float(groups[4]) if market_match and len(groups) > 4 and groups[4] else None\n",
    "    try:\n",
    "        year_int = 2000 + int(date_str_yymmmdd[:2])\n",
    "        month_str = date_str_yymmmdd[2:5].upper()\n",
    "        day_int = int(date_str_yymmmdd[5:])\n",
    "        month_map = {'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,\n",
    "                     'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12}\n",
    "        month_int = month_map[month_str]\n",
    "        hour_edt_int = int(hour_str_edt)\n",
    "        event_resolution_dt_naive_edt = dt.datetime(year_int, month_int, day_int, hour_edt_int, 0, 0)\n",
    "        utc_offset_hours = 4\n",
    "        event_resolution_dt_utc_aware = event_resolution_dt_naive_edt.replace(tzinfo=timezone(timedelta(hours=-utc_offset_hours)))\n",
    "        event_resolution_dt_utc = event_resolution_dt_utc_aware.astimezone(timezone.utc)\n",
    "        return {\"series\": series, \"date_str_yymmmdd\": date_str_yymmmdd, \"hour_str_edt\": hour_str_edt,\n",
    "                \"strike_price\": strike_price, \"event_resolution_dt_utc\": event_resolution_dt_utc}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing ticker {ticker_string} in get_event_resolution_details: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_iso_to_unix_timestamp(date_string: str | None) -> int | None:\n",
    "    if not date_string: return None\n",
    "    try:\n",
    "        if date_string.endswith('Z'): dt_obj = dt.datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n",
    "        else:\n",
    "            dt_obj = dt.datetime.fromisoformat(date_string)\n",
    "            if dt_obj.tzinfo is None: dt_obj = dt_obj.replace(tzinfo=timezone.utc)\n",
    "        return int(dt_obj.timestamp())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing date string '{date_string}' to Unix ts: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Candlestick Fetching Function ---\n",
    "_first_candle_structure_logged_session = False\n",
    "def fetch_candlesticks_for_market(market_ticker: str, series_ticker: str, start_ts_s: int,\n",
    "                                  end_ts_s: int, period_minutes: int = 1, api_delay_seconds: float = 0.6\n",
    "                                 ) -> pd.DataFrame | None:\n",
    "    global _first_candle_structure_logged_session\n",
    "    all_candlesticks_data = []\n",
    "    if not all([market_ticker, series_ticker, isinstance(start_ts_s, int), isinstance(end_ts_s, int), period_minutes > 0]):\n",
    "        logger.error(f\"Invalid params for fetch_candlesticks_for_market for {market_ticker}.\")\n",
    "        return None\n",
    "    if start_ts_s >= end_ts_s: return pd.DataFrame()\n",
    "    current_start_ts = start_ts_s\n",
    "    max_periods_per_request = 4900\n",
    "    with tqdm(total=(end_ts_s - start_ts_s) // (period_minutes * 60) + 1, desc=f\"Candles: {market_ticker[:20]}...\", leave=False, unit=\"cndl\") as pbar:\n",
    "        while current_start_ts < end_ts_s:\n",
    "            chunk_max_duration_seconds = (max_periods_per_request - 1) * period_minutes * 60\n",
    "            chunk_end_ts = min(end_ts_s, current_start_ts + chunk_max_duration_seconds)\n",
    "            if chunk_end_ts <= current_start_ts: break\n",
    "            api_path = f\"/trade-api/v2/series/{series_ticker}/markets/{market_ticker}/candlesticks\"\n",
    "            params = {\"start_ts\": current_start_ts, \"end_ts\": chunk_end_ts, \"period_interval\": period_minutes}\n",
    "            response_json = kalshi_api_get_request(api_path, params, timeout=30)\n",
    "            time.sleep(api_delay_seconds) # Moved delay here to ensure it happens after every API call\n",
    "\n",
    "            if response_json and \"candlesticks\" in response_json:\n",
    "                candlesticks_from_api = response_json[\"candlesticks\"]\n",
    "                if not _first_candle_structure_logged_session and candlesticks_from_api:\n",
    "                    logger.info(f\"First candlestick structure: {json.dumps(candlesticks_from_api[0], indent=2)}\")\n",
    "                    _first_candle_structure_logged_session = True\n",
    "                chunk_data = []\n",
    "                if candlesticks_from_api:\n",
    "                    for candle in candlesticks_from_api:\n",
    "                        ts = candle.get(\"end_period_ts\")\n",
    "                        if ts is None or ts > chunk_end_ts or ts < current_start_ts: continue\n",
    "                        trade_price_info = candle.get(\"price\", {}) or {}\n",
    "                        yes_bid_info = candle.get(\"yes_bid\", {}) or {}\n",
    "                        yes_ask_info = candle.get(\"yes_ask\", {}) or {}\n",
    "                        chunk_data.append({\n",
    "                            \"market_ticker\": market_ticker, \"series_ticker\": series_ticker, \"timestamp_s\": ts,\n",
    "                            \"datetime_utc\": dt.datetime.fromtimestamp(ts, tz=timezone.utc).isoformat(),\n",
    "                            \"trade_open_cents\": trade_price_info.get(\"open\"), \"trade_high_cents\": trade_price_info.get(\"high\"),\n",
    "                            \"trade_low_cents\": trade_price_info.get(\"low\"), \"trade_close_cents\": trade_price_info.get(\"close\"),\n",
    "                            \"yes_bid_open_cents\": yes_bid_info.get(\"open\"), \"yes_bid_high_cents\": yes_bid_info.get(\"high\"),\n",
    "                            \"yes_bid_low_cents\": yes_bid_info.get(\"low\"), \"yes_bid_close_cents\": yes_bid_info.get(\"close\"),\n",
    "                            \"yes_ask_open_cents\": yes_ask_info.get(\"open\"), \"yes_ask_high_cents\": yes_ask_info.get(\"high\"),\n",
    "                            \"yes_ask_low_cents\": yes_ask_info.get(\"low\"), \"yes_ask_close_cents\": yes_ask_info.get(\"close\"),\n",
    "                            \"volume\": candle.get(\"volume\"), \"open_interest\": candle.get(\"open_interest\")\n",
    "                        })\n",
    "                    all_candlesticks_data.extend(chunk_data)\n",
    "                    pbar.update(len(chunk_data) if chunk_data else max(1, (chunk_end_ts - current_start_ts) // (period_minutes*60)))\n",
    "                    last_ts_in_chunk = candlesticks_from_api[-1].get(\"end_period_ts\", chunk_end_ts) if candlesticks_from_api else chunk_end_ts\n",
    "                    current_start_ts = last_ts_in_chunk + (period_minutes * 60)\n",
    "                else: # No candlesticks in API response for this chunk\n",
    "                    pbar.update(max(1, (chunk_end_ts - current_start_ts) // (period_minutes*60)))\n",
    "                    current_start_ts = chunk_end_ts + (period_minutes * 60)\n",
    "            else: # API request failed or bad response\n",
    "                logger.warning(f\"No candlestick data or API error for {market_ticker} chunk: {params}\")\n",
    "                pbar.update(max(1, (chunk_end_ts - current_start_ts) // (period_minutes*60)))\n",
    "                current_start_ts = chunk_end_ts + (period_minutes * 60)\n",
    "                time.sleep(api_delay_seconds * 2) # Longer delay on error\n",
    "    if not all_candlesticks_data: return pd.DataFrame()\n",
    "    df = pd.DataFrame(all_candlesticks_data)\n",
    "    df.sort_values(\"timestamp_s\", inplace=True)\n",
    "    df.drop_duplicates(subset=[\"timestamp_s\"], keep=\"first\", inplace=True)\n",
    "    return df\n",
    "\n",
    "logger.info(\"Cell 3: Configurations and Utility Functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Read Master Ticker List, Filter for NTM, and Prepare for Download\n",
    "\n",
    "# --- Configuration for this Cell ---\n",
    "# Path to your CSV file containing all historical market tickers\n",
    "# Find the latest one if multiple exist, or specify the exact path.\n",
    "try:\n",
    "    # Assuming TARGET_SERIES_TICKER is defined in Cell 3 (e.g., \"KXBTCD\")\n",
    "    if 'TARGET_SERIES_TICKER' not in globals(): # Define if not from previous cell\n",
    "        TARGET_SERIES_TICKER = \"KXBTCD\" # Default if not defined by Cell 3 (though it should be)\n",
    "        logger.info(f\"TARGET_SERIES_TICKER was not defined in Cell 3, set to default: {TARGET_SERIES_TICKER}\")\n",
    "\n",
    "    list_of_ticker_files = glob.glob(f\"kalshi_btc_hourly_market_tickers_{TARGET_SERIES_TICKER}_*.csv\")\n",
    "    if not list_of_ticker_files:\n",
    "        raise FileNotFoundError(f\"No master ticker CSV files found matching pattern: kalshi_btc_hourly_market_tickers_{TARGET_SERIES_TICKER}_*.csv. Please ensure Cell 3 of discover.ipynb (or equivalent) has been run to generate this file.\")\n",
    "    MASTER_TICKER_CSV_PATH = max(list_of_ticker_files, key=os.path.getctime)\n",
    "    logger.info(f\"Using master ticker list from: {MASTER_TICKER_CSV_PATH}\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.critical(str(e))\n",
    "    raise\n",
    "except NameError as e: # Catches if TARGET_SERIES_TICKER isn't defined\n",
    "    logger.critical(f\"A required variable (likely TARGET_SERIES_TICKER) for finding the ticker CSV is not defined: {e}. Ensure Cell 3 has run or define it.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# API call delay for fetching market details\n",
    "API_DELAY_MARKET_DETAILS = 0.05  # seconds - can be small if calls are infrequent per second overall\n",
    "                                # Original was 0.5. If you hit rate limits, increase this.\n",
    "\n",
    "# --- Lists to store NTM markets and outcomes ---\n",
    "ntm_markets_to_download_info = [] # List of tuples: (market_ticker, series_ticker, open_ts, close_ts, event_date_dir, event_hour_dir)\n",
    "ntm_market_outcomes_data = []   # List of dicts for outcomes CSV\n",
    "\n",
    "# --- Load the master list of market tickers ---\n",
    "try:\n",
    "    master_tickers_df = pd.read_csv(MASTER_TICKER_CSV_PATH)\n",
    "    if 'market_ticker' not in master_tickers_df.columns:\n",
    "        logger.critical(f\"'market_ticker' column not found in {MASTER_TICKER_CSV_PATH}.\")\n",
    "        raise ValueError(\"CSV must contain 'market_ticker' column\")\n",
    "    all_market_tickers_from_csv = master_tickers_df['market_ticker'].dropna().unique().tolist()\n",
    "    logger.info(f\"Loaded {len(all_market_tickers_from_csv)} unique market tickers from {MASTER_TICKER_CSV_PATH}.\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error loading or processing master ticker CSV {MASTER_TICKER_CSV_PATH}: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Helper function to fetch specific market details (open/close times, result) ---\n",
    "# This function now RELIES on get_event_resolution_details being defined in Cell 3.\n",
    "def fetch_specific_market_details_for_ntm(market_ticker: str) -> dict | None:\n",
    "    \"\"\"Fetches open_time, close_time, and result for a specific market_ticker.\"\"\"\n",
    "    api_path = f\"/trade-api/v2/markets/{market_ticker}\"\n",
    "    response_data = kalshi_api_get_request(api_path, timeout=15)\n",
    "    time.sleep(API_DELAY_MARKET_DETAILS)\n",
    "\n",
    "    if response_data and \"market\" in response_data:\n",
    "        market_data_from_api = response_data[\"market\"]\n",
    "        \n",
    "        # get_event_resolution_details is now expected to be globally available from Cell 3\n",
    "        parsed_ticker_for_datetime_and_series = get_event_resolution_details(market_ticker)\n",
    "        if not parsed_ticker_for_datetime_and_series:\n",
    "            logger.warning(f\"Could not parse {market_ticker} with get_event_resolution_details. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        derived_series_ticker = parsed_ticker_for_datetime_and_series.get(\"series\")\n",
    "\n",
    "        details = {\n",
    "            \"market_ticker\": market_ticker,\n",
    "            \"series_ticker\": derived_series_ticker,\n",
    "            \"open_ts\": parse_iso_to_unix_timestamp(market_data_from_api.get(\"open_time\")),\n",
    "            \"close_ts\": parse_iso_to_unix_timestamp(market_data_from_api.get(\"close_time\")),\n",
    "            \"result\": market_data_from_api.get(\"result\"),\n",
    "            \"status\": market_data_from_api.get(\"status\"),\n",
    "            \"strike_price\": parsed_ticker_for_datetime_and_series.get(\"strike_price\"),\n",
    "            \"event_resolution_dt_utc\": parsed_ticker_for_datetime_and_series.get(\"event_resolution_dt_utc\"),\n",
    "            \"date_str_yymmmdd_for_dir\": parsed_ticker_for_datetime_and_series.get(\"date_str_yymmmdd\"),\n",
    "            \"hour_str_edt_close_for_dir\": parsed_ticker_for_datetime_and_series.get(\"hour_str_edt\")\n",
    "        }\n",
    "        \n",
    "        required_keys = [\"open_ts\", \"close_ts\", \"series_ticker\", \"event_resolution_dt_utc\", \n",
    "                         \"date_str_yymmmdd_for_dir\", \"hour_str_edt_close_for_dir\"]\n",
    "        missing_fields = [k for k in required_keys if details.get(k) is None]\n",
    "        if details[\"strike_price\"] is None: missing_fields.append(\"strike_price\")\n",
    "            \n",
    "        if missing_fields:\n",
    "            logger.warning(f\"Incomplete market details for {market_ticker}. Missing or None fields: {missing_fields}. API Data: {str(market_data_from_api)[:200]}. Parsed: {details}. Skipping.\")\n",
    "            return None\n",
    "        return details\n",
    "    else:\n",
    "        logger.error(f\"Failed to fetch details for market {market_ticker}. Response: {str(response_data)[:200]}\")\n",
    "        return None\n",
    "\n",
    "# --- Iterate through each market ticker from the CSV ---\n",
    "if 'NTM_PERCENTAGE_THRESHOLD' not in globals(): # From Cell 3\n",
    "    logger.critical(\"NTM_PERCENTAGE_THRESHOLD not defined. Please run Cell 3.\")\n",
    "    raise NameError(\"NTM_PERCENTAGE_THRESHOLD not defined\")\n",
    "\n",
    "logger.info(f\"Processing {len(all_market_tickers_from_csv)} market tickers for NTM status (NTM Threshold: {NTM_PERCENTAGE_THRESHOLD*100:.2f}%)...\")\n",
    "\n",
    "# TEST_LIMIT_TICKERS = 50 # Keep small for testing\n",
    "TEST_LIMIT_TICKERS = None \n",
    "\n",
    "processed_ticker_count = 0\n",
    "for market_ticker_from_csv in tqdm(all_market_tickers_from_csv, desc=\"Filtering NTM Markets\"):\n",
    "    if TEST_LIMIT_TICKERS is not None and processed_ticker_count >= TEST_LIMIT_TICKERS:\n",
    "        logger.info(f\"Reached test limit of {TEST_LIMIT_TICKERS} tickers. Stopping NTM identification.\")\n",
    "        break\n",
    "    processed_ticker_count += 1\n",
    "\n",
    "    market_details = fetch_specific_market_details_for_ntm(market_ticker_from_csv)\n",
    "    if not market_details:\n",
    "        continue\n",
    "\n",
    "    event_resolution_dt_utc = market_details[\"event_resolution_dt_utc\"]\n",
    "    market_strike_price = market_details[\"strike_price\"]\n",
    "    reference_btc_time_ts = int(event_resolution_dt_utc.timestamp()) - 3600 # 1hr before event resolution\n",
    "    \n",
    "    # get_btc_price_at_timestamp is defined in Cell 3\n",
    "    reference_btc_price = get_btc_price_at_timestamp(reference_btc_time_ts)\n",
    "\n",
    "    if reference_btc_price is None:\n",
    "        ref_time_str = dt.datetime.fromtimestamp(reference_btc_time_ts, timezone.utc).isoformat()\n",
    "        logger.warning(f\"No Binance BTC price near {ref_time_str} for NTM check of {market_ticker_from_csv}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    is_ntm = False\n",
    "    if reference_btc_price > 0:\n",
    "        price_diff_percentage = abs(reference_btc_price - market_strike_price) / reference_btc_price\n",
    "        if price_diff_percentage <= NTM_PERCENTAGE_THRESHOLD:\n",
    "            is_ntm = True\n",
    "    \n",
    "    if is_ntm:\n",
    "        if market_details[\"status\"] not in [\"closed\", \"settled\", \"finalized\"]:\n",
    "            # logger.info(f\"  NTM market {market_ticker_from_csv} status '{market_details['status']}', not fully historical. Skipping.\")\n",
    "            continue # Skip if not yet settled, as outcome might not be final\n",
    "            \n",
    "        logger.info(f\"  NTM MATCH: {market_ticker_from_csv} (Strike: {market_strike_price:,.2f}, RefBTC: {reference_btc_price:,.2f}). Queuing.\")\n",
    "        ntm_markets_to_download_info.append((\n",
    "            market_details[\"market_ticker\"], market_details[\"series_ticker\"],\n",
    "            market_details[\"open_ts\"], market_details[\"close_ts\"],\n",
    "            market_details[\"date_str_yymmmdd_for_dir\"], market_details[\"hour_str_edt_close_for_dir\"]\n",
    "        ))\n",
    "        ntm_market_outcomes_data.append({\n",
    "            \"market_ticker\": market_details[\"market_ticker\"], \"result\": market_details[\"result\"],\n",
    "            \"event_resolution_time_iso\": event_resolution_dt_utc.isoformat(),\n",
    "            \"reference_btc_price_for_ntm\": reference_btc_price, \"kalshi_strike_price\": market_strike_price,\n",
    "            \"market_open_time_iso\": dt.datetime.fromtimestamp(market_details[\"open_ts\"], timezone.utc).isoformat(),\n",
    "            \"market_close_time_iso\": dt.datetime.fromtimestamp(market_details[\"close_ts\"], timezone.utc).isoformat(),\n",
    "            \"event_ticker_parent\": market_ticker_from_csv.split('-T')[0]\n",
    "        })\n",
    "\n",
    "logger.info(f\"\\n--- NTM Market Identification from CSV Complete ---\")\n",
    "logger.info(f\"Processed {processed_ticker_count} tickers from CSV.\")\n",
    "logger.info(f\"Found {len(ntm_markets_to_download_info)} NTM markets to download.\")\n",
    "logger.info(f\"Collected {len(ntm_market_outcomes_data)} NTM market outcomes.\")\n",
    "\n",
    "if ntm_markets_to_download_info:\n",
    "    print(f\"\\nSample of NTM markets queued for download (first 5):\")\n",
    "    for item in ntm_markets_to_download_info[:5]: print(f\"  {item}\")\n",
    "else: print(\"\\nNo NTM markets identified for download from the CSV.\")\n",
    "\n",
    "if ntm_market_outcomes_data:\n",
    "    df_ntm_outcomes_preview = pd.DataFrame(ntm_market_outcomes_data)\n",
    "    if not df_ntm_outcomes_preview.empty:\n",
    "        print(\"\\nPreview of NTM Outcomes DataFrame (first 5 rows):\")\n",
    "        print(df_ntm_outcomes_preview.head().to_string())\n",
    "else: print(\"\\nNo NTM market outcomes data collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Downloading Candlestick Data for Filtered NTM Markets & Saving Outcomes\n",
    "\n",
    "total_ntm_markets = len(ntm_markets_to_download_info)\n",
    "logger.info(f\"\\n--- Starting Candlestick Download for {total_ntm_markets} NTM Markets (identified from CSV) ---\")\n",
    "\n",
    "downloaded_count = 0\n",
    "skipped_already_exists_count = 0\n",
    "failed_download_count = 0\n",
    "\n",
    "# Define the path for the consolidated outcomes CSV\n",
    "outcomes_csv_filename = f\"kalshi_btc_hourly_NTM_filtered_market_outcomes_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "# Ensure KALSHI_ORGANIZED_NTM_DATA_DIR is defined from Cell 3\n",
    "if 'KALSHI_ORGANIZED_NTM_DATA_DIR' not in globals():\n",
    "    logger.critical(\"KALSHI_ORGANIZED_NTM_DATA_DIR not defined. Please run Cell 3.\")\n",
    "    raise NameError(\"KALSHI_ORGANIZED_NTM_DATA_DIR not defined.\")\n",
    "outcomes_csv_filepath = KALSHI_ORGANIZED_NTM_DATA_DIR / outcomes_csv_filename # Save INSIDE kalshi_data\n",
    "\n",
    "# Use tqdm for the outer loop of markets\n",
    "for market_info_tuple in tqdm(ntm_markets_to_download_info, desc=\"Downloading NTM Market Candles\"):\n",
    "    market_ticker, series_ticker, market_open_ts, market_close_ts, event_date_dir_str, event_hour_dir_str = market_info_tuple\n",
    "    \n",
    "    target_day_dir = KALSHI_ORGANIZED_NTM_DATA_DIR / event_date_dir_str\n",
    "    target_hour_dir = target_day_dir / event_hour_dir_str.zfill(2)\n",
    "    target_hour_dir.mkdir(parents=True, exist_ok=True)\n",
    "    market_csv_filepath = target_hour_dir / f\"{market_ticker}.csv\"\n",
    "\n",
    "    if market_csv_filepath.exists() and market_csv_filepath.stat().st_size > 0:\n",
    "        skipped_already_exists_count += 1\n",
    "        continue\n",
    "        \n",
    "    # fetch_candlesticks_for_market is defined in Cell 3\n",
    "    candlesticks_df = fetch_candlesticks_for_market(\n",
    "        market_ticker=market_ticker, series_ticker=series_ticker, \n",
    "        start_ts_s=market_open_ts, end_ts_s=market_close_ts, period_minutes=1 \n",
    "    )\n",
    "\n",
    "    if candlesticks_df is not None and not candlesticks_df.empty:\n",
    "        try:\n",
    "            candlesticks_df.to_csv(market_csv_filepath, index=False)\n",
    "            downloaded_count += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data for {market_ticker} to {market_csv_filepath}: {e}\")\n",
    "            failed_download_count += 1\n",
    "    elif candlesticks_df is not None and candlesticks_df.empty:\n",
    "        logger.info(f\"No candle data from API for NTM market {market_ticker}. No CSV created.\")\n",
    "        failed_download_count +=1 \n",
    "    else: # candlesticks_df is None (API error)\n",
    "        logger.warning(f\"Failed to fetch candle data for {market_ticker}. No CSV created.\")\n",
    "        failed_download_count += 1\n",
    "    \n",
    "logger.info(\"\\n--- NTM Candlestick Download Complete (from CSV list) ---\")\n",
    "logger.info(f\"Total NTM markets for download: {total_ntm_markets}\")\n",
    "logger.info(f\"Successfully downloaded new data for: {downloaded_count} markets\")\n",
    "logger.info(f\"Skipped (already existed): {skipped_already_exists_count} markets\")\n",
    "logger.info(f\"Failed to download/save (or empty from API): {failed_download_count} markets\")\n",
    "logger.info(f\"Data saved in: {KALSHI_ORGANIZED_NTM_DATA_DIR}\")\n",
    "\n",
    "# --- Save NTM Market Outcomes ---\n",
    "if ntm_market_outcomes_data:\n",
    "    outcomes_df = pd.DataFrame(ntm_market_outcomes_data)\n",
    "    try:\n",
    "        outcomes_df.to_csv(outcomes_csv_filepath, index=False)\n",
    "        logger.info(f\"Successfully saved {len(outcomes_df)} NTM market outcomes to: {outcomes_csv_filepath}\")\n",
    "        print(f\"\\nNTM Market outcomes saved to: {outcomes_csv_filepath}\")\n",
    "        if not outcomes_df.empty:\n",
    "            print(\"Sample of outcomes data (first 5 rows):\")\n",
    "            print(outcomes_df.head().to_string())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving NTM market outcomes to {outcomes_csv_filepath}: {e}\")\n",
    "else:\n",
    "    logger.info(\"No NTM market outcomes data to save.\")\n",
    "\n",
    "print(f\"\\n\\nAll NTM data processing finished. Check logs and the directory: {KALSHI_ORGANIZED_NTM_DATA_DIR}\")\n",
    "if ntm_market_outcomes_data:\n",
    "    print(f\"Consolidated outcomes CSV at: {outcomes_csv_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
