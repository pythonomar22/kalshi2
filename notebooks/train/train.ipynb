{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Load Data for Classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "import logging\n",
    "import json # For saving feature_columns_list\n",
    "import joblib # For saving the model and scaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split # We'll do a chronological split manually\n",
    "from sklearn.linear_model import LogisticRegression # CHANGED: For classification\n",
    "from sklearn.preprocessing import StandardScaler # For feature scaling\n",
    "# CHANGED: Classification metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, confusion_matrix \n",
    "\n",
    "# --- Logging Setup ---\n",
    "logger_name = f\"model_training_classifier_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\" # Updated logger name\n",
    "logger = logging.getLogger(logger_name)\n",
    "if not logger.handlers: # Avoid adding handlers if re-running cell\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration ---\n",
    "current_notebook_dir = Path.cwd() # Assumes notebook is in notebooks/train/\n",
    "# Adjust FEATURES_DIR if your features are not in ../features relative to this notebook's parent\n",
    "# For example, if train.ipynb is in ./notebooks/train and features are in ./notebooks/features:\n",
    "FEATURES_DIR = current_notebook_dir.parent.parent / \"features\" # Assuming features are in project_root/features\n",
    "# If features are in ./notebooks/features:\n",
    "# FEATURES_DIR = current_notebook_dir.parent / \"features\" \n",
    "\n",
    "logger.info(f\"Attempting to find feature files in: {FEATURES_DIR.resolve()}\")\n",
    "\n",
    "try:\n",
    "    if not FEATURES_DIR.exists():\n",
    "        # Let's try another common location if the above doesn't exist, e.g. within notebooks/\n",
    "        alt_features_dir = current_notebook_dir.parent / \"features\"\n",
    "        if alt_features_dir.exists():\n",
    "            FEATURES_DIR = alt_features_dir\n",
    "            logger.info(f\"Primary FEATURES_DIR not found, using alternative: {FEATURES_DIR.resolve()}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"The directory {FEATURES_DIR.resolve()} (and {alt_features_dir.resolve()}) does not exist. Please check the path.\")\n",
    "\n",
    "    # Assuming feature files might still use the 'v1' from previous regression task,\n",
    "    # or you might have new ones. Adjust pattern if needed.\n",
    "    feature_files = sorted(FEATURES_DIR.glob(\"kalshi_btc_features_target_v1_*.csv\"), key=os.path.getctime, reverse=True)\n",
    "    if not feature_files:\n",
    "        raise FileNotFoundError(f\"No feature CSV files found in {FEATURES_DIR.resolve()} matching pattern 'kalshi_btc_features_target_v1_*.csv'\")\n",
    "    FEATURES_CSV_PATH = feature_files[0]\n",
    "    logger.info(f\"Using features CSV: {FEATURES_CSV_PATH.resolve()}\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.critical(str(e))\n",
    "    FEATURES_CSV_PATH = None\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error finding features CSV: {e}\")\n",
    "    FEATURES_CSV_PATH = None\n",
    "\n",
    "# Output directory for trained classifier models\n",
    "MODEL_OUTPUT_DIR = current_notebook_dir.parent / \"trained_models/rf\" # Keeps trained_models within notebooks/\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Trained classifier models will be saved in: {MODEL_OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "# --- Load the Features DataFrame ---\n",
    "df_model_data = pd.DataFrame()\n",
    "\n",
    "if FEATURES_CSV_PATH and FEATURES_CSV_PATH.exists():\n",
    "    try:\n",
    "        df_model_data = pd.read_csv(FEATURES_CSV_PATH)\n",
    "        logger.info(f\"Successfully loaded features data from: {FEATURES_CSV_PATH.resolve()}\")\n",
    "        logger.info(f\"Shape of loaded data: {df_model_data.shape}\")\n",
    "        \n",
    "        print(\"--- Data Head (Raw from CSV) ---\")\n",
    "        print(df_model_data.head())\n",
    "        print(\"\\n--- Data Info (Raw from CSV) ---\")\n",
    "        df_model_data.info()\n",
    "        print(\"\\n--- Data Description (Numerical, Raw from CSV) ---\")\n",
    "        print(df_model_data.describe().to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error loading features CSV {FEATURES_CSV_PATH.resolve()}: {e}\")\n",
    "else:\n",
    "    if FEATURES_CSV_PATH:\n",
    "         logger.critical(f\"Features CSV file not found at the specified path: {FEATURES_CSV_PATH.resolve()}\")\n",
    "    else:\n",
    "         logger.critical(\"FEATURES_CSV_PATH was not set (likely due to an error finding the file). Cannot load data.\")\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.warning(\"DataFrame df_model_data is empty. Subsequent cells might fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preprocessing, Target Transformation, Feature Selection, and Splitting\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.error(\"df_model_data is empty. Cannot proceed with preprocessing and splitting. Please ensure Cell 1 ran correctly and loaded data.\")\n",
    "else:\n",
    "    logger.info(f\"Starting preprocessing for df_model_data with shape: {df_model_data.shape}\")\n",
    "\n",
    "    # --- 1. Ensure Chronological Order ---\n",
    "    df_model_data.sort_values(by='decision_point_ts_utc', inplace=True)\n",
    "    df_model_data.reset_index(drop=True, inplace=True)\n",
    "    logger.info(\"Data sorted by 'decision_point_ts_utc'.\")\n",
    "\n",
    "    # --- 2. Define NEW Target Variable for Classification ---\n",
    "    # Original target: 'TARGET_btc_diff_from_strike'\n",
    "    # New target: 1 if (BTC price at resolution > strike price), 0 otherwise.\n",
    "    # This means the Kalshi market for \"YES\" would win.\n",
    "    original_target_col = 'TARGET_btc_diff_from_strike'\n",
    "    classification_target_col = 'TARGET_market_resolves_yes' # New binary target\n",
    "\n",
    "    if original_target_col not in df_model_data.columns:\n",
    "        logger.critical(f\"Original target column '{original_target_col}' not found in DataFrame. Cannot create classification target.\")\n",
    "        # Stop execution or handle error appropriately\n",
    "        raise ValueError(f\"Missing required column: {original_target_col}\")\n",
    "    \n",
    "    # Create the binary target: 1 if positive difference (YES wins), 0 if non-positive (NO wins or ties)\n",
    "    df_model_data[classification_target_col] = (df_model_data[original_target_col] > 0).astype(int)\n",
    "    logger.info(f\"Created binary classification target '{classification_target_col}'.\")\n",
    "    logger.info(f\"Value counts for '{classification_target_col}':\\n{df_model_data[classification_target_col].value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "    # --- 3. Handle Missing Values (NaNs) in Features ---\n",
    "    identifier_cols = ['kalshi_market_ticker', 'decision_point_ts_utc', 'kalshi_strike_price']\n",
    "    # Feature columns: exclude identifiers, original regression target, and new classification target\n",
    "    feature_columns = [\n",
    "        col for col in df_model_data.columns \n",
    "        if col not in identifier_cols + [original_target_col, classification_target_col]\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Potential feature columns ({len(feature_columns)}): {feature_columns[:10]}...\") # Log first 10\n",
    "\n",
    "    nan_summary = df_model_data[feature_columns].isnull().sum()\n",
    "    nan_summary = nan_summary[nan_summary > 0].sort_values(ascending=False)\n",
    "    if not nan_summary.empty:\n",
    "        logger.warning(f\"NaN values found in feature columns:\\n{nan_summary}\")\n",
    "        \n",
    "        # --- Imputation Strategy (Same as before, review if needed for classification) ---\n",
    "        cols_to_fill_zero = [\n",
    "            col for col in feature_columns if 'kalshi_mid_chg' in col or \\\n",
    "            'btc_mom' in col \n",
    "        ]\n",
    "        cols_to_fill_median = [ \n",
    "            col for col in feature_columns if 'btc_vol' in col or \\\n",
    "            'btc_sma' in col or 'btc_ema' in col \n",
    "        ]\n",
    "        cols_to_fill_rsi_neutral = [col for col in feature_columns if 'btc_rsi' in col]\n",
    "        \n",
    "        if 'kalshi_yes_bid' in df_model_data.columns and 'kalshi_yes_bid' in feature_columns: # Check if it's a feature\n",
    "            df_model_data['kalshi_yes_bid'] = df_model_data['kalshi_yes_bid'].fillna(0)\n",
    "            logger.info(\"Filled NaNs in 'kalshi_yes_bid' with 0.\")\n",
    "        if 'kalshi_yes_ask' in df_model_data.columns and 'kalshi_yes_ask' in feature_columns:\n",
    "            df_model_data['kalshi_yes_ask'] = df_model_data['kalshi_yes_ask'].fillna(100)\n",
    "            logger.info(\"Filled NaNs in 'kalshi_yes_ask' with 100.\")\n",
    "        \n",
    "        if 'kalshi_yes_bid' in feature_columns and 'kalshi_yes_ask' in feature_columns:\n",
    "            if 'kalshi_spread' in feature_columns:\n",
    "                df_model_data['kalshi_spread'] = df_model_data['kalshi_yes_ask'] - df_model_data['kalshi_yes_bid']\n",
    "                logger.info(\"Recalculated 'kalshi_spread' after filling bid/ask.\")\n",
    "            if 'kalshi_mid_price' in feature_columns:\n",
    "                 df_model_data['kalshi_mid_price'] = (df_model_data['kalshi_yes_bid'] + df_model_data['kalshi_yes_ask']) / 2\n",
    "                 logger.info(\"Recalculated 'kalshi_mid_price' after filling bid/ask.\")\n",
    "        \n",
    "        for col in cols_to_fill_zero:\n",
    "            if col in df_model_data.columns and col in feature_columns:\n",
    "                df_model_data[col] = df_model_data[col].fillna(0)\n",
    "                logger.info(f\"Filled NaNs in '{col}' with 0.\")\n",
    "\n",
    "        for col in cols_to_fill_median:\n",
    "            if col in df_model_data.columns and col in feature_columns:\n",
    "                median_val = df_model_data[col].median()\n",
    "                df_model_data[col] = df_model_data[col].fillna(median_val)\n",
    "                logger.info(f\"Filled NaNs in '{col}' with its median ({median_val:.4f}).\")\n",
    "\n",
    "        for col in cols_to_fill_rsi_neutral:\n",
    "            if col in df_model_data.columns and col in feature_columns:\n",
    "                df_model_data[col] = df_model_data[col].fillna(50)\n",
    "                logger.info(f\"Filled NaNs in '{col}' with 50.\")\n",
    "\n",
    "        original_row_count = len(df_model_data)\n",
    "        df_model_data.dropna(subset=feature_columns, inplace=True) # Drop rows with NaNs in any *feature* column\n",
    "        # Also drop rows where the classification target might be NaN (though astype(int) should handle it from boolean)\n",
    "        df_model_data.dropna(subset=[classification_target_col], inplace=True) \n",
    "        logger.info(f\"Dropped {original_row_count - len(df_model_data)} rows due to remaining NaNs in features or target after imputation attempts.\")\n",
    "        \n",
    "        final_nan_summary = df_model_data[feature_columns].isnull().sum()\n",
    "        final_nan_summary = final_nan_summary[final_nan_summary > 0]\n",
    "        if not final_nan_summary.empty:\n",
    "            logger.error(f\"Still have NaNs after processing feature columns! Columns:\\n{final_nan_summary}\")\n",
    "        else:\n",
    "            logger.info(\"Successfully handled NaNs in feature columns.\")\n",
    "    else:\n",
    "        logger.info(\"No NaNs found in the selected feature columns.\")\n",
    "        \n",
    "    # --- 4. Define Features (X) and New Target (y) ---\n",
    "    if not df_model_data.empty:\n",
    "        X = df_model_data[feature_columns].copy()\n",
    "        y = df_model_data[classification_target_col].copy() # Use the new binary target\n",
    "        logger.info(f\"Defined X (features) with shape: {X.shape}\")\n",
    "        logger.info(f\"Defined y (binary target) with shape: {y.shape}\")\n",
    "        logger.info(f\"Target y value counts:\\n{y.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "        # --- 5. Split Data (Chronological) ---\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(X) * split_ratio)\n",
    "\n",
    "        X_train = X.iloc[:split_index]\n",
    "        y_train = y.iloc[:split_index] # y_train is now binary\n",
    "        X_test = X.iloc[split_index:]\n",
    "        y_test = y.iloc[split_index:]   # y_test is now binary\n",
    "\n",
    "        logger.info(f\"Data split chronologically:\")\n",
    "        logger.info(f\"  X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        logger.info(f\"  X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        train_start_ts = df_model_data['decision_point_ts_utc'].iloc[0]\n",
    "        train_end_ts = df_model_data['decision_point_ts_utc'].iloc[split_index - 1]\n",
    "        test_start_ts = df_model_data['decision_point_ts_utc'].iloc[split_index]\n",
    "        test_end_ts = df_model_data['decision_point_ts_utc'].iloc[-1]\n",
    "\n",
    "        logger.info(f\"  Training data from: {dt.datetime.fromtimestamp(train_start_ts, tz=timezone.utc).isoformat()} to {dt.datetime.fromtimestamp(train_end_ts, tz=timezone.utc).isoformat()}\")\n",
    "        logger.info(f\"  Test data from:     {dt.datetime.fromtimestamp(test_start_ts, tz=timezone.utc).isoformat()} to {dt.datetime.fromtimestamp(test_end_ts, tz=timezone.utc).isoformat()}\")\n",
    "        \n",
    "        # --- 6. Feature Scaling ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        logger.info(\"Features scaled using StandardScaler.\")\n",
    "        print(\"\\nSample of scaled training features (X_train_scaled_df head):\")\n",
    "        print(X_train_scaled_df.head())\n",
    "        \n",
    "        # Save the scaler (filename indicates it's for classifier v1)\n",
    "        scaler_path = MODEL_OUTPUT_DIR / \"feature_scaler_classifier_v1.joblib\"\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        logger.info(f\"Scaler saved to: {scaler_path}\")\n",
    "        \n",
    "        # Save the list of feature columns (filename indicates it's for classifier v1)\n",
    "        # This list *should* be the same as for regression if using same features,\n",
    "        # but good to save it associated with this model run.\n",
    "        feature_columns_list_path = MODEL_OUTPUT_DIR / \"feature_columns_classifier_v1.json\"\n",
    "        with open(feature_columns_list_path, 'w') as f:\n",
    "            json.dump(feature_columns, f) # feature_columns is already a list here\n",
    "        logger.info(f\"List of feature columns saved to: {feature_columns_list_path}\")\n",
    "\n",
    "    else:\n",
    "        logger.error(\"df_model_data is empty after NaN handling. Cannot proceed to define X, y, or split.\")\n",
    "        X, y, X_train, y_train, X_test, y_test, X_train_scaled_df, X_test_scaled_df = [pd.DataFrame()]*8 \n",
    "        scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Classification Model Training and Evaluation\n",
    "\n",
    "# NEW: Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# NEW: Consider importing calibration if you want to try it later\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "if 'X_train_scaled_df' not in globals() or X_train_scaled_df.empty:\n",
    "    logger.error(\"Scaled training data (X_train_scaled_df) not found or is empty. Please ensure Cell 2 ran successfully.\")\n",
    "else:\n",
    "    logger.info(\"--- Starting Classification Model Training (Random Forest) ---\") # UPDATED MODEL NAME\n",
    "\n",
    "    # --- 1. Initialize and Train RandomForestClassifier Model ---\n",
    "    # Key parameters for RandomForestClassifier:\n",
    "    #   n_estimators: Number of trees in the forest. More is usually better but increases training time.\n",
    "    #   max_depth: Maximum depth of each tree. Deeper trees can overfit.\n",
    "    #   min_samples_split: Minimum number of samples required to split an internal node.\n",
    "    #   min_samples_leaf: Minimum number of samples required to be at a leaf node.\n",
    "    #   class_weight: 'balanced' or 'balanced_subsample' can be very useful for imbalanced datasets.\n",
    "    #   random_state: For reproducibility.\n",
    "    #   n_jobs: -1 to use all available cores for training (can speed things up significantly).\n",
    "    \n",
    "    # Start with some reasonable defaults. These often need tuning (e.g., with GridSearchCV or RandomizedSearchCV).\n",
    "    classifier_model = RandomForestClassifier(\n",
    "        n_estimators=200,         # Increased from default 100 for potentially better performance\n",
    "        max_depth=15,             # Limit depth to prevent overfitting; adjust based on data\n",
    "        min_samples_split=10,     # Require at least 10 samples to split a node\n",
    "        min_samples_leaf=5,       # Require at least 5 samples in a leaf\n",
    "        class_weight='balanced_subsample', # Good option for large datasets, each tree gets balanced bootstrap sample\n",
    "        random_state=42,\n",
    "        n_jobs=-1,                # Use all available cores\n",
    "        oob_score=True            # Use out-of-bag samples to estimate generalization accuracy\n",
    "    )\n",
    "    \n",
    "    model_name = \"RandomForest_classifier_v1\" # For saving artifacts\n",
    "    logger.info(f\"Training {model_name} model on {X_train_scaled_df.shape[0]} samples...\")\n",
    "    logger.info(f\"Model parameters: {classifier_model.get_params()}\")\n",
    "    \n",
    "    if 'y_train' not in globals() or y_train.empty:\n",
    "        logger.error(\"y_train (binary target) is not available. Cannot train model.\")\n",
    "    else:\n",
    "        try:\n",
    "            classifier_model.fit(X_train_scaled_df, y_train)\n",
    "            logger.info(f\"{model_name} model training complete.\")\n",
    "            if hasattr(classifier_model, 'oob_score_'):\n",
    "                 logger.info(f\"  Out-of-Bag (OOB) Score: {classifier_model.oob_score_:.4f}\")\n",
    "\n",
    "\n",
    "            # --- 2. Make Predictions on the Test Set ---\n",
    "            logger.info(f\"Making predictions on the test set ({X_test_scaled_df.shape[0]} samples)...\")\n",
    "            y_pred_test_class = classifier_model.predict(X_test_scaled_df)\n",
    "            y_pred_test_proba = classifier_model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "\n",
    "            # --- 3. Evaluate Model Performance (Classification Metrics) ---\n",
    "            if 'y_test' not in globals() or y_test.empty:\n",
    "                logger.error(\"y_test (binary target) is not available. Cannot evaluate model.\")\n",
    "            else:\n",
    "                accuracy = accuracy_score(y_test, y_pred_test_class)\n",
    "                precision = precision_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                recall = recall_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                f1 = f1_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                try:\n",
    "                    roc_auc = roc_auc_score(y_test, y_pred_test_proba)\n",
    "                except ValueError as e:\n",
    "                    logger.warning(f\"Could not calculate ROC AUC: {e}\")\n",
    "                    roc_auc = np.nan\n",
    "                logloss = log_loss(y_test, y_pred_test_proba)\n",
    "\n",
    "                logger.info(f\"\\n--- {model_name} Evaluation Metrics (Test Set) ---\")\n",
    "                logger.info(f\"  Accuracy:          {accuracy:.4f}\")\n",
    "                logger.info(f\"  Precision:         {precision:.4f}\")\n",
    "                logger.info(f\"  Recall (TPR):      {recall:.4f}\")\n",
    "                logger.info(f\"  F1-Score:          {f1:.4f}\")\n",
    "                logger.info(f\"  ROC AUC:           {roc_auc:.4f}\")\n",
    "                logger.info(f\"  Log Loss:          {logloss:.4f}\")\n",
    "\n",
    "                logger.info(f\"\\n--- Confusion Matrix (Test Set) - {model_name} ---\")\n",
    "                cm = confusion_matrix(y_test, y_pred_test_class)\n",
    "                logger.info(f\"\\n{cm}\")\n",
    "                try:\n",
    "                    tn, fp, fn, tp = cm.ravel()\n",
    "                    logger.info(f\"  True Negatives (TN):  {tn}\")\n",
    "                    logger.info(f\"  False Positives (FP): {fp} (Type I Error)\")\n",
    "                    logger.info(f\"  False Negatives (FN): {fn} (Type II Error)\")\n",
    "                    logger.info(f\"  True Positives (TP):  {tp}\")\n",
    "                except ValueError:\n",
    "                    logger.warning(\"Could not unpack full confusion matrix.\")\n",
    "\n",
    "                df_results_class = pd.DataFrame({\n",
    "                    'actual_target_resolves_yes': y_test,\n",
    "                    'predicted_class_resolves_yes': y_pred_test_class,\n",
    "                    'predicted_proba_resolves_yes': y_pred_test_proba\n",
    "                })\n",
    "                if 'original_target_col' in globals() and original_target_col in df_model_data.columns:\n",
    "                    df_results_class['original_target_diff'] = df_model_data.loc[y_test.index, original_target_col].values\n",
    "                if 'kalshi_market_ticker' in df_model_data.columns:\n",
    "                    df_results_class['kalshi_market_ticker'] = df_model_data.loc[y_test.index, 'kalshi_market_ticker'].values\n",
    "                if 'decision_point_ts_utc' in df_model_data.columns:\n",
    "                    df_results_class['decision_point_ts_utc'] = df_model_data.loc[y_test.index, 'decision_point_ts_utc'].values\n",
    "                \n",
    "                print(f\"\\n--- Sample of Test Set Predictions vs Actuals ({model_name}) ---\")\n",
    "                print(df_results_class.head(10).to_string())\n",
    "\n",
    "                # --- Detailed Analysis by Actual Outcome (Test Set) ---\n",
    "                logger.info(f\"\\n\\n--- Detailed Analysis by Actual Outcome (Test Set) - {model_name} ---\")\n",
    "                # (Your existing detailed analysis code for Actual NO and Actual YES markets goes here unchanged)\n",
    "                # ... (copy the section from your previous Cell 3 here) ...\n",
    "                 # --- Analysis for Actual 'NO' markets (target = 0) ---\n",
    "                df_actual_no = df_results_class[df_results_class['actual_target_resolves_yes'] == 0].copy()\n",
    "                if not df_actual_no.empty:\n",
    "                    logger.info(f\"\\n  --- For Actual 'NO' Markets (Total in Test Set: {len(df_actual_no)}) ---\")\n",
    "                    fp_count_manual = len(df_actual_no[df_actual_no['predicted_class_resolves_yes'] == 1])\n",
    "                    tn_count_manual = len(df_actual_no[df_actual_no['predicted_class_resolves_yes'] == 0])\n",
    "                    \n",
    "                    if len(df_actual_no) > 0:\n",
    "                        logger.info(f\"    Predicted as YES (False Positives): {fp_count_manual} ({fp_count_manual/len(df_actual_no):.2%})\")\n",
    "                        logger.info(f\"    Predicted as NO (True Negatives):   {tn_count_manual} ({tn_count_manual/len(df_actual_no):.2%})\")\n",
    "                    else:\n",
    "                        logger.info(\"    No actual 'NO' markets to calculate percentages.\")\n",
    "\n",
    "                    logger.info(f\"    Distribution of P(model predicts YES) when actual is NO:\")\n",
    "                    logger.info(f\"{df_actual_no['predicted_proba_resolves_yes'].describe().to_string()}\")\n",
    "                    \n",
    "                    high_fp_threshold = 0.7 \n",
    "                    df_high_prob_fp = df_actual_no[\n",
    "                        (df_actual_no['predicted_class_resolves_yes'] == 1) & \n",
    "                        (df_actual_no['predicted_proba_resolves_yes'] > high_fp_threshold)\n",
    "                    ].sort_values(by='predicted_proba_resolves_yes', ascending=False)\n",
    "                    \n",
    "                    if not df_high_prob_fp.empty:\n",
    "                        logger.warning(f\"    Examples of high-confidence False Positives (Actual NO, P(model YES) > {high_fp_threshold}):\")\n",
    "                        with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.width', 1000): \n",
    "                            logger.warning(f\"\\n{df_high_prob_fp.head(10).to_string()}\") \n",
    "                    else:\n",
    "                        logger.info(f\"    No False Positives found with P(model YES) > {high_fp_threshold} when actual was NO.\")\n",
    "                else:\n",
    "                    logger.info(\"\\n  No 'Actual NO' markets found in the test set for this detailed analysis.\")\n",
    "\n",
    "                # --- Analysis for Actual 'YES' markets (target = 1) ---\n",
    "                df_actual_yes = df_results_class[df_results_class['actual_target_resolves_yes'] == 1].copy()\n",
    "                if not df_actual_yes.empty:\n",
    "                    logger.info(f\"\\n  --- For Actual 'YES' Markets (Total in Test Set: {len(df_actual_yes)}) ---\")\n",
    "                    tp_count_manual = len(df_actual_yes[df_actual_yes['predicted_class_resolves_yes'] == 1])\n",
    "                    fn_count_manual = len(df_actual_yes[df_actual_yes['predicted_class_resolves_yes'] == 0])\n",
    "\n",
    "                    if len(df_actual_yes) > 0:\n",
    "                        logger.info(f\"    Predicted as YES (True Positives):  {tp_count_manual} ({tp_count_manual/len(df_actual_yes):.2%})\")\n",
    "                        logger.info(f\"    Predicted as NO (False Negatives):  {fn_count_manual} ({fn_count_manual/len(df_actual_yes):.2%})\")\n",
    "                    else:\n",
    "                        logger.info(\"    No actual 'YES' markets to calculate percentages.\")\n",
    "                        \n",
    "                    logger.info(f\"    Distribution of P(model predicts YES) when actual is YES:\")\n",
    "                    logger.info(f\"{df_actual_yes['predicted_proba_resolves_yes'].describe().to_string()}\")\n",
    "                    \n",
    "                    low_fn_threshold = 0.3 \n",
    "                    df_low_prob_fn = df_actual_yes[\n",
    "                        (df_actual_yes['predicted_class_resolves_yes'] == 0) & \n",
    "                        (df_actual_yes['predicted_proba_resolves_yes'] < low_fn_threshold)\n",
    "                    ].sort_values(by='predicted_proba_resolves_yes', ascending=True)\n",
    "\n",
    "                    if not df_low_prob_fn.empty:\n",
    "                        logger.warning(f\"    Examples of high-confidence False Negatives (Actual YES, P(model YES) < {low_fn_threshold}):\")\n",
    "                        with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.width', 1000):\n",
    "                            logger.warning(f\"\\n{df_low_prob_fn.head(10).to_string()}\")\n",
    "                    else:\n",
    "                        logger.info(f\"    No False Negatives found with P(model YES) < {low_fn_threshold} when actual was YES.\")\n",
    "                else:\n",
    "                    logger.info(\"\\n  No 'Actual YES' markets found in the test set for this detailed analysis.\")\n",
    "\n",
    "\n",
    "                # --- 4. Inspect Model Feature Importances (for RandomForest) ---\n",
    "                logger.info(f\"\\n\\n--- {model_name} Feature Importances ---\")\n",
    "                if hasattr(classifier_model, 'feature_importances_'):\n",
    "                    if 'feature_columns' not in globals(): # Load if not already loaded\n",
    "                        feature_columns_list_path = MODEL_OUTPUT_DIR / \"feature_columns_classifier_v1.json\" # Assuming this filename is still relevant\n",
    "                        if feature_columns_list_path.exists():\n",
    "                            with open(feature_columns_list_path, 'r') as f:\n",
    "                                feature_columns = json.load(f)\n",
    "                            logger.info(f\"Loaded feature_columns list from {feature_columns_list_path}\")\n",
    "                        else:\n",
    "                            logger.warning(\"feature_columns list not found. Cannot display importance names.\")\n",
    "                            feature_columns = [f\"feature_{i}\" for i in range(len(X_train_scaled_df.columns))]\n",
    "                    \n",
    "                    importances = pd.DataFrame({\n",
    "                        'feature': feature_columns, # Use columns from X_train_scaled_df\n",
    "                        'importance': classifier_model.feature_importances_\n",
    "                    })\n",
    "                    importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "                    \n",
    "                    print(\"\\nTop Feature Importances:\")\n",
    "                    print(importances.head(20).to_string())\n",
    "                else:\n",
    "                    logger.warning(f\"Could not retrieve feature importances from the {model_name} model.\")\n",
    "\n",
    "                # --- 5. Save the Trained Model ---\n",
    "                # Update filenames to reflect the new model type\n",
    "                model_path = MODEL_OUTPUT_DIR / f\"{model_name}.joblib\"\n",
    "                joblib.dump(classifier_model, model_path)\n",
    "                logger.info(f\"Trained {model_name} model saved to: {model_path}\")\n",
    "\n",
    "                # For RandomForest, saving parameters like intercept/coefficients isn't directly applicable\n",
    "                # The .joblib file contains the entire ensemble of trees.\n",
    "                # If you need to store hyperparameters, you can do so:\n",
    "                model_hyperparams = classifier_model.get_params()\n",
    "                params_path = MODEL_OUTPUT_DIR / f\"{model_name}_hyperparams.json\"\n",
    "                with open(params_path, 'w') as f:\n",
    "                    # Convert numpy types to native Python types for JSON serialization if any exist\n",
    "                    serializable_params = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in model_hyperparams.items()}\n",
    "                    json.dump(serializable_params, f, indent=4)\n",
    "                logger.info(f\"{model_name} hyperparameters saved to: {params_path}\")\n",
    "                \n",
    "                # Note: The feature_scaler_classifier_v1.joblib and feature_columns_classifier_v1.json\n",
    "                # are still relevant and will be used by the strategy for preprocessing before prediction.\n",
    "                # No need to resave them here unless the feature set changes.\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"An error occurred during {model_name} model training or evaluation: {e}\", exc_info=True)\n",
    "            if 'classifier_model' in locals():\n",
    "                 logger.info(f\"{model_name} model training might have partially completed or failed during evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
