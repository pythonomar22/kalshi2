{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Load Data for Classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "import logging\n",
    "import json # For saving feature_columns_list\n",
    "import joblib # For saving the model and scaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split # We'll do a chronological split manually\n",
    "from sklearn.linear_model import LogisticRegression # CHANGED: For classification\n",
    "from sklearn.preprocessing import StandardScaler # For feature scaling\n",
    "# CHANGED: Classification metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, confusion_matrix \n",
    "\n",
    "# --- Logging Setup ---\n",
    "logger_name = f\"model_training_classifier_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\" # Updated logger name\n",
    "logger = logging.getLogger(logger_name)\n",
    "if not logger.handlers: # Avoid adding handlers if re-running cell\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration ---\n",
    "current_notebook_dir = Path.cwd() # Assumes notebook is in notebooks/train/\n",
    "# Adjust FEATURES_DIR if your features are not in ../features relative to this notebook's parent\n",
    "# For example, if train.ipynb is in ./notebooks/train and features are in ./notebooks/features:\n",
    "FEATURES_DIR = current_notebook_dir.parent.parent / \"features\" # Assuming features are in project_root/features\n",
    "# If features are in ./notebooks/features:\n",
    "# FEATURES_DIR = current_notebook_dir.parent / \"features\" \n",
    "\n",
    "logger.info(f\"Attempting to find feature files in: {FEATURES_DIR.resolve()}\")\n",
    "\n",
    "try:\n",
    "    if not FEATURES_DIR.exists():\n",
    "        # Let's try another common location if the above doesn't exist, e.g. within notebooks/\n",
    "        alt_features_dir = current_notebook_dir.parent / \"features\"\n",
    "        if alt_features_dir.exists():\n",
    "            FEATURES_DIR = alt_features_dir\n",
    "            logger.info(f\"Primary FEATURES_DIR not found, using alternative: {FEATURES_DIR.resolve()}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"The directory {FEATURES_DIR.resolve()} (and {alt_features_dir.resolve()}) does not exist. Please check the path.\")\n",
    "\n",
    "    # Assuming feature files might still use the 'v1' from previous regression task,\n",
    "    # or you might have new ones. Adjust pattern if needed.\n",
    "    feature_files = sorted(FEATURES_DIR.glob(\"kalshi_btc_features_target_v1_*.csv\"), key=os.path.getctime, reverse=True)\n",
    "    if not feature_files:\n",
    "        raise FileNotFoundError(f\"No feature CSV files found in {FEATURES_DIR.resolve()} matching pattern 'kalshi_btc_features_target_v1_*.csv'\")\n",
    "    FEATURES_CSV_PATH = feature_files[0]\n",
    "    logger.info(f\"Using features CSV: {FEATURES_CSV_PATH.resolve()}\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.critical(str(e))\n",
    "    FEATURES_CSV_PATH = None\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error finding features CSV: {e}\")\n",
    "    FEATURES_CSV_PATH = None\n",
    "\n",
    "# Output directory for trained classifier models\n",
    "MODEL_OUTPUT_DIR = current_notebook_dir.parent / \"trained_models\" # Keeps trained_models within notebooks/\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Trained classifier models will be saved in: {MODEL_OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "# --- Load the Features DataFrame ---\n",
    "df_model_data = pd.DataFrame()\n",
    "\n",
    "if FEATURES_CSV_PATH and FEATURES_CSV_PATH.exists():\n",
    "    try:\n",
    "        df_model_data = pd.read_csv(FEATURES_CSV_PATH)\n",
    "        logger.info(f\"Successfully loaded features data from: {FEATURES_CSV_PATH.resolve()}\")\n",
    "        logger.info(f\"Shape of loaded data: {df_model_data.shape}\")\n",
    "        \n",
    "        print(\"--- Data Head (Raw from CSV) ---\")\n",
    "        print(df_model_data.head())\n",
    "        print(\"\\n--- Data Info (Raw from CSV) ---\")\n",
    "        df_model_data.info()\n",
    "        print(\"\\n--- Data Description (Numerical, Raw from CSV) ---\")\n",
    "        print(df_model_data.describe().to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error loading features CSV {FEATURES_CSV_PATH.resolve()}: {e}\")\n",
    "else:\n",
    "    if FEATURES_CSV_PATH:\n",
    "         logger.critical(f\"Features CSV file not found at the specified path: {FEATURES_CSV_PATH.resolve()}\")\n",
    "    else:\n",
    "         logger.critical(\"FEATURES_CSV_PATH was not set (likely due to an error finding the file). Cannot load data.\")\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.warning(\"DataFrame df_model_data is empty. Subsequent cells might fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preprocessing, Target Transformation, Feature Selection, and Splitting\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.error(\"df_model_data is empty. Cannot proceed with preprocessing and splitting. Please ensure Cell 1 ran correctly and loaded data.\")\n",
    "else:\n",
    "    logger.info(f\"Starting preprocessing for df_model_data with shape: {df_model_data.shape}\")\n",
    "\n",
    "    # --- 1. Ensure Chronological Order ---\n",
    "    df_model_data.sort_values(by='decision_point_ts_utc', inplace=True)\n",
    "    df_model_data.reset_index(drop=True, inplace=True)\n",
    "    logger.info(\"Data sorted by 'decision_point_ts_utc'.\")\n",
    "\n",
    "    # --- 2. Define NEW Target Variable for Classification ---\n",
    "    # Original target: 'TARGET_btc_diff_from_strike'\n",
    "    # New target: 1 if (BTC price at resolution > strike price), 0 otherwise.\n",
    "    # This means the Kalshi market for \"YES\" would win.\n",
    "    original_target_col = 'TARGET_btc_diff_from_strike'\n",
    "    classification_target_col = 'TARGET_market_resolves_yes' # New binary target\n",
    "\n",
    "    if original_target_col not in df_model_data.columns:\n",
    "        logger.critical(f\"Original target column '{original_target_col}' not found in DataFrame. Cannot create classification target.\")\n",
    "        # Stop execution or handle error appropriately\n",
    "        raise ValueError(f\"Missing required column: {original_target_col}\")\n",
    "    \n",
    "    # Create the binary target: 1 if positive difference (YES wins), 0 if non-positive (NO wins or ties)\n",
    "    df_model_data[classification_target_col] = (df_model_data[original_target_col] > 0).astype(int)\n",
    "    logger.info(f\"Created binary classification target '{classification_target_col}'.\")\n",
    "    logger.info(f\"Value counts for '{classification_target_col}':\\n{df_model_data[classification_target_col].value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "    # --- 3. Handle Missing Values (NaNs) in Features ---\n",
    "    identifier_cols = ['kalshi_market_ticker', 'decision_point_ts_utc', 'kalshi_strike_price']\n",
    "    # Feature columns: exclude identifiers, original regression target, and new classification target\n",
    "    feature_columns = [\n",
    "        col for col in df_model_data.columns \n",
    "        if col not in identifier_cols + [original_target_col, classification_target_col]\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Potential feature columns ({len(feature_columns)}): {feature_columns[:10]}...\") # Log first 10\n",
    "\n",
    "    nan_summary = df_model_data[feature_columns].isnull().sum()\n",
    "    nan_summary = nan_summary[nan_summary > 0].sort_values(ascending=False)\n",
    "    if not nan_summary.empty:\n",
    "        logger.warning(f\"NaN values found in feature columns:\\n{nan_summary}\")\n",
    "        \n",
    "        # --- Imputation Strategy (Same as before, review if needed for classification) ---\n",
    "        cols_to_fill_zero = [\n",
    "            col for col in feature_columns if 'kalshi_mid_chg' in col or \\\n",
    "            'btc_mom' in col \n",
    "        ]\n",
    "        cols_to_fill_median = [ \n",
    "            col for col in feature_columns if 'btc_vol' in col or \\\n",
    "            'btc_sma' in col or 'btc_ema' in col \n",
    "        ]\n",
    "        cols_to_fill_rsi_neutral = [col for col in feature_columns if 'btc_rsi' in col]\n",
    "        \n",
    "        if 'kalshi_yes_bid' in df_model_data.columns and 'kalshi_yes_bid' in feature_columns: # Check if it's a feature\n",
    "            df_model_data['kalshi_yes_bid'] = df_model_data['kalshi_yes_bid'].fillna(0)\n",
    "            logger.info(\"Filled NaNs in 'kalshi_yes_bid' with 0.\")\n",
    "        if 'kalshi_yes_ask' in df_model_data.columns and 'kalshi_yes_ask' in feature_columns:\n",
    "            df_model_data['kalshi_yes_ask'] = df_model_data['kalshi_yes_ask'].fillna(100)\n",
    "            logger.info(\"Filled NaNs in 'kalshi_yes_ask' with 100.\")\n",
    "        \n",
    "        if 'kalshi_yes_bid' in feature_columns and 'kalshi_yes_ask' in feature_columns:\n",
    "            if 'kalshi_spread' in feature_columns:\n",
    "                df_model_data['kalshi_spread'] = df_model_data['kalshi_yes_ask'] - df_model_data['kalshi_yes_bid']\n",
    "                logger.info(\"Recalculated 'kalshi_spread' after filling bid/ask.\")\n",
    "            if 'kalshi_mid_price' in feature_columns:\n",
    "                 df_model_data['kalshi_mid_price'] = (df_model_data['kalshi_yes_bid'] + df_model_data['kalshi_yes_ask']) / 2\n",
    "                 logger.info(\"Recalculated 'kalshi_mid_price' after filling bid/ask.\")\n",
    "        \n",
    "        for col in cols_to_fill_zero:\n",
    "            if col in df_model_data.columns and col in feature_columns:\n",
    "                df_model_data[col] = df_model_data[col].fillna(0)\n",
    "                logger.info(f\"Filled NaNs in '{col}' with 0.\")\n",
    "\n",
    "        for col in cols_to_fill_median:\n",
    "            if col in df_model_data.columns and col in feature_columns:\n",
    "                median_val = df_model_data[col].median()\n",
    "                df_model_data[col] = df_model_data[col].fillna(median_val)\n",
    "                logger.info(f\"Filled NaNs in '{col}' with its median ({median_val:.4f}).\")\n",
    "\n",
    "        for col in cols_to_fill_rsi_neutral:\n",
    "            if col in df_model_data.columns and col in feature_columns:\n",
    "                df_model_data[col] = df_model_data[col].fillna(50)\n",
    "                logger.info(f\"Filled NaNs in '{col}' with 50.\")\n",
    "\n",
    "        original_row_count = len(df_model_data)\n",
    "        df_model_data.dropna(subset=feature_columns, inplace=True) # Drop rows with NaNs in any *feature* column\n",
    "        # Also drop rows where the classification target might be NaN (though astype(int) should handle it from boolean)\n",
    "        df_model_data.dropna(subset=[classification_target_col], inplace=True) \n",
    "        logger.info(f\"Dropped {original_row_count - len(df_model_data)} rows due to remaining NaNs in features or target after imputation attempts.\")\n",
    "        \n",
    "        final_nan_summary = df_model_data[feature_columns].isnull().sum()\n",
    "        final_nan_summary = final_nan_summary[final_nan_summary > 0]\n",
    "        if not final_nan_summary.empty:\n",
    "            logger.error(f\"Still have NaNs after processing feature columns! Columns:\\n{final_nan_summary}\")\n",
    "        else:\n",
    "            logger.info(\"Successfully handled NaNs in feature columns.\")\n",
    "    else:\n",
    "        logger.info(\"No NaNs found in the selected feature columns.\")\n",
    "        \n",
    "    # --- 4. Define Features (X) and New Target (y) ---\n",
    "    if not df_model_data.empty:\n",
    "        X = df_model_data[feature_columns].copy()\n",
    "        y = df_model_data[classification_target_col].copy() # Use the new binary target\n",
    "        logger.info(f\"Defined X (features) with shape: {X.shape}\")\n",
    "        logger.info(f\"Defined y (binary target) with shape: {y.shape}\")\n",
    "        logger.info(f\"Target y value counts:\\n{y.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "        # --- 5. Split Data (Chronological) ---\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(X) * split_ratio)\n",
    "\n",
    "        X_train = X.iloc[:split_index]\n",
    "        y_train = y.iloc[:split_index] # y_train is now binary\n",
    "        X_test = X.iloc[split_index:]\n",
    "        y_test = y.iloc[split_index:]   # y_test is now binary\n",
    "\n",
    "        logger.info(f\"Data split chronologically:\")\n",
    "        logger.info(f\"  X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        logger.info(f\"  X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        train_start_ts = df_model_data['decision_point_ts_utc'].iloc[0]\n",
    "        train_end_ts = df_model_data['decision_point_ts_utc'].iloc[split_index - 1]\n",
    "        test_start_ts = df_model_data['decision_point_ts_utc'].iloc[split_index]\n",
    "        test_end_ts = df_model_data['decision_point_ts_utc'].iloc[-1]\n",
    "\n",
    "        logger.info(f\"  Training data from: {dt.datetime.fromtimestamp(train_start_ts, tz=timezone.utc).isoformat()} to {dt.datetime.fromtimestamp(train_end_ts, tz=timezone.utc).isoformat()}\")\n",
    "        logger.info(f\"  Test data from:     {dt.datetime.fromtimestamp(test_start_ts, tz=timezone.utc).isoformat()} to {dt.datetime.fromtimestamp(test_end_ts, tz=timezone.utc).isoformat()}\")\n",
    "        \n",
    "        # --- 6. Feature Scaling ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        logger.info(\"Features scaled using StandardScaler.\")\n",
    "        print(\"\\nSample of scaled training features (X_train_scaled_df head):\")\n",
    "        print(X_train_scaled_df.head())\n",
    "        \n",
    "        # Save the scaler (filename indicates it's for classifier v1)\n",
    "        scaler_path = MODEL_OUTPUT_DIR / \"feature_scaler_classifier_v1.joblib\"\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        logger.info(f\"Scaler saved to: {scaler_path}\")\n",
    "        \n",
    "        # Save the list of feature columns (filename indicates it's for classifier v1)\n",
    "        # This list *should* be the same as for regression if using same features,\n",
    "        # but good to save it associated with this model run.\n",
    "        feature_columns_list_path = MODEL_OUTPUT_DIR / \"feature_columns_classifier_v1.json\"\n",
    "        with open(feature_columns_list_path, 'w') as f:\n",
    "            json.dump(feature_columns, f) # feature_columns is already a list here\n",
    "        logger.info(f\"List of feature columns saved to: {feature_columns_list_path}\")\n",
    "\n",
    "    else:\n",
    "        logger.error(\"df_model_data is empty after NaN handling. Cannot proceed to define X, y, or split.\")\n",
    "        X, y, X_train, y_train, X_test, y_test, X_train_scaled_df, X_test_scaled_df = [pd.DataFrame()]*8 \n",
    "        scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Classification Model Training and Evaluation\n",
    "\n",
    "if 'X_train_scaled_df' not in globals() or X_train_scaled_df.empty:\n",
    "    logger.error(\"Scaled training data (X_train_scaled_df) not found or is empty. Please ensure Cell 2 ran successfully.\")\n",
    "    # Optionally, raise an error or stop notebook execution\n",
    "else:\n",
    "    logger.info(\"--- Starting Classification Model Training (Logistic Regression) ---\")\n",
    "\n",
    "    # --- 1. Initialize and Train Logistic Regression Model ---\n",
    "    # You can adjust parameters like C (inverse of regularization strength) or solver.\n",
    "    # Using class_weight='balanced' can be helpful if classes are imbalanced, though ours are fairly balanced.\n",
    "    # sag solver is good for large datasets, liblinear for smaller. lbfgs is a good default.\n",
    "    classifier_model = LogisticRegression(\n",
    "        solver='lbfgs', # A good default solver\n",
    "        max_iter=1000,  # Increased for convergence with potentially many features\n",
    "        random_state=42,\n",
    "        C=1.0, # Regularization strength\n",
    "        class_weight='balanced' # Optional: helps if classes are imbalanced\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Training LogisticRegression model on {X_train_scaled_df.shape[0]} samples...\")\n",
    "    \n",
    "    if 'y_train' not in globals() or y_train.empty:\n",
    "        logger.error(\"y_train (binary target) is not available. Cannot train model.\")\n",
    "    else:\n",
    "        try:\n",
    "            classifier_model.fit(X_train_scaled_df, y_train)\n",
    "            logger.info(\"LogisticRegression model training complete.\")\n",
    "\n",
    "            # --- 2. Make Predictions on the Test Set ---\n",
    "            logger.info(f\"Making predictions on the test set ({X_test_scaled_df.shape[0]} samples)...\")\n",
    "            y_pred_test_class = classifier_model.predict(X_test_scaled_df) # Predicts class labels (0 or 1)\n",
    "            y_pred_test_proba = classifier_model.predict_proba(X_test_scaled_df)[:, 1] # Probabilities for the positive class (class 1)\n",
    "\n",
    "            # --- 3. Evaluate Model Performance (Classification Metrics) ---\n",
    "            if 'y_test' not in globals() or y_test.empty:\n",
    "                logger.error(\"y_test (binary target) is not available. Cannot evaluate model.\")\n",
    "            else:\n",
    "                accuracy = accuracy_score(y_test, y_pred_test_class)\n",
    "                precision = precision_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                recall = recall_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                f1 = f1_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                try:\n",
    "                    roc_auc = roc_auc_score(y_test, y_pred_test_proba) # Use probabilities for AUC\n",
    "                except ValueError as e:\n",
    "                    logger.warning(f\"Could not calculate ROC AUC, possibly due to only one class present in y_test or y_pred_test_proba. Error: {e}\")\n",
    "                    roc_auc = np.nan\n",
    "                logloss = log_loss(y_test, y_pred_test_proba) # Use probabilities for log loss\n",
    "\n",
    "                logger.info(\"\\n--- Classification Model Evaluation Metrics (Test Set) ---\")\n",
    "                logger.info(f\"  Accuracy:          {accuracy:.4f}\")\n",
    "                logger.info(f\"  Precision:         {precision:.4f} (Portion of predicted YES that were actually YES)\")\n",
    "                logger.info(f\"  Recall (TPR):      {recall:.4f} (Portion of actual YES that were correctly identified)\")\n",
    "                logger.info(f\"  F1-Score:          {f1:.4f}\")\n",
    "                logger.info(f\"  ROC AUC:           {roc_auc:.4f}\")\n",
    "                logger.info(f\"  Log Loss:          {logloss:.4f}\")\n",
    "\n",
    "                logger.info(\"\\n--- Confusion Matrix (Test Set) ---\")\n",
    "                # Rows: Actual, Columns: Predicted\n",
    "                # [[TN, FP],\n",
    "                #  [FN, TP]]\n",
    "                cm = confusion_matrix(y_test, y_pred_test_class)\n",
    "                logger.info(f\"\\n{cm}\")\n",
    "                try:\n",
    "                    tn, fp, fn, tp = cm.ravel()\n",
    "                    logger.info(f\"  True Negatives (TN) - Actual NO, Predicted NO:  {tn}\")\n",
    "                    logger.info(f\"  False Positives (FP) - Actual NO, Predicted YES: {fp} (Type I Error)\")\n",
    "                    logger.info(f\"  False Negatives (FN) - Actual YES, Predicted NO: {fn} (Type II Error)\")\n",
    "                    logger.info(f\"  True Positives (TP) - Actual YES, Predicted YES: {tp}\")\n",
    "                except ValueError: # If cm doesn't have 4 values (e.g. predicts only one class)\n",
    "                    logger.warning(\"Could not unpack full confusion matrix (TN,FP,FN,TP).\")\n",
    "\n",
    "\n",
    "                # Create a DataFrame for easier analysis of predictions vs actuals\n",
    "                df_results_class = pd.DataFrame({\n",
    "                    'actual_target_resolves_yes': y_test,\n",
    "                    'predicted_class_resolves_yes': y_pred_test_class,\n",
    "                    'predicted_proba_resolves_yes': y_pred_test_proba\n",
    "                })\n",
    "                # Add back the original regression target for context if needed\n",
    "                if original_target_col in df_model_data.columns: # From Cell 2\n",
    "                    df_results_class['original_target_diff'] = df_model_data.loc[y_test.index, original_target_col]\n",
    "\n",
    "                print(\"\\n--- Sample of Test Set Predictions vs Actuals (Classification) ---\")\n",
    "                print(df_results_class.head(10).to_string())\n",
    "\n",
    "                # --- 4. Inspect Model Coefficients (for Logistic Regression) ---\n",
    "                logger.info(\"\\n--- Logistic Regression Model Coefficients ---\")\n",
    "                # For Logistic Regression, intercept is an array if multi-class, or a float if binary.\n",
    "                # coef_ is also 2D for multi-class, (1, n_features) for binary.\n",
    "                if hasattr(classifier_model, 'intercept_') and hasattr(classifier_model, 'coef_'):\n",
    "                    logger.info(f\"Intercept: {classifier_model.intercept_[0]:.4f}\") # Assuming binary classification, intercept_ is array of one\n",
    "                    \n",
    "                    # Ensure feature_columns is available\n",
    "                    if 'feature_columns' not in globals(): # Should be defined in Cell 2\n",
    "                        feature_columns_list_path = MODEL_OUTPUT_DIR / \"feature_columns_classifier_v1.json\"\n",
    "                        if feature_columns_list_path.exists():\n",
    "                            with open(feature_columns_list_path, 'r') as f:\n",
    "                                feature_columns = json.load(f)\n",
    "                            logger.info(f\"Loaded feature_columns list from {feature_columns_list_path}\")\n",
    "                        else:\n",
    "                            logger.warning(\"feature_columns list not found. Cannot display coefficient names.\")\n",
    "                            feature_columns = [f\"feature_{i}\" for i in range(classifier_model.coef_.shape[1])]\n",
    "                    \n",
    "                    # Coefficients are for the positive class (class 1) in binary classification\n",
    "                    coefficients = pd.DataFrame({'feature': feature_columns, 'coefficient': classifier_model.coef_[0]})\n",
    "                    coefficients['abs_coefficient'] = np.abs(coefficients['coefficient'])\n",
    "                    coefficients.sort_values(by='abs_coefficient', ascending=False, inplace=True)\n",
    "                    \n",
    "                    print(\"\\nTop Coefficients (by absolute value) for P(TARGET_market_resolves_yes = 1):\")\n",
    "                    print(coefficients.head(20).to_string())\n",
    "                else:\n",
    "                    logger.warning(\"Could not retrieve coefficients from the trained classifier model.\")\n",
    "\n",
    "                # --- 5. Save the Trained Model ---\n",
    "                model_path = MODEL_OUTPUT_DIR / \"logistic_regression_btc_classifier_v1.joblib\" # New name\n",
    "                joblib.dump(classifier_model, model_path)\n",
    "                logger.info(f\"Trained Logistic Regression model saved to: {model_path}\")\n",
    "\n",
    "                # Save model parameters for potential use in a strategy that doesn't load the full joblib\n",
    "                # For Logistic Regression, this includes coefficients and intercept.\n",
    "                # The strategy will need to apply the sigmoid function to the raw score if using these.\n",
    "                model_params_for_backtest = {\n",
    "                    'model_type': 'logistic_regression',\n",
    "                    'intercept': classifier_model.intercept_[0].tolist() if isinstance(classifier_model.intercept_, np.ndarray) else classifier_model.intercept_,\n",
    "                    'coefficients': dict(zip(feature_columns, classifier_model.coef_[0])),\n",
    "                    'feature_order': feature_columns, # From Cell 2\n",
    "                    'classes': classifier_model.classes_.tolist() # [0, 1] for binary\n",
    "                }\n",
    "                params_path = MODEL_OUTPUT_DIR / \"logreg_model_params_v1.json\" # New name\n",
    "                with open(params_path, 'w') as f:\n",
    "                    json.dump(model_params_for_backtest, f, indent=4)\n",
    "                logger.info(f\"Logistic Regression model parameters saved to: {params_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"An error occurred during classification model training or evaluation: {e}\", exc_info=True)\n",
    "            if 'classifier_model' in locals():\n",
    "                 logger.info(\"Model training might have partially completed or failed during evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
