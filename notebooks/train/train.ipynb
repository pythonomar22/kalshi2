{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Load Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta # <<<<<<<<<<<< ADD timezone HERE\n",
    "import logging\n",
    "import json # For saving feature_columns_list\n",
    "import joblib # For saving the model and scaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split # We'll do a chronological split manually\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler # For feature scaling\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# ... (rest of logging setup as before) ...\n",
    "logger_name = f\"model_training_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "logger = logging.getLogger(logger_name)\n",
    "if not logger.handlers: # Avoid adding handlers if re-running cell\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration ---\n",
    "# ... (rest of configuration as before) ...\n",
    "current_notebook_dir = Path.cwd()\n",
    "FEATURES_DIR = current_notebook_dir.parent / \"features\"\n",
    "logger.info(f\"Attempting to find feature files in: {FEATURES_DIR.resolve()}\")\n",
    "\n",
    "try:\n",
    "    if not FEATURES_DIR.exists():\n",
    "        raise FileNotFoundError(f\"The directory {FEATURES_DIR.resolve()} does not exist. Please check the path.\")\n",
    "    feature_files = sorted(FEATURES_DIR.glob(\"kalshi_btc_features_target_v1_*.csv\"), key=os.path.getctime, reverse=True)\n",
    "    if not feature_files:\n",
    "        raise FileNotFoundError(f\"No feature CSV files found in {FEATURES_DIR.resolve()} matching pattern 'kalshi_btc_features_target_v1_*.csv'\")\n",
    "    FEATURES_CSV_PATH = feature_files[0]\n",
    "    logger.info(f\"Using features CSV: {FEATURES_CSV_PATH.resolve()}\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.critical(str(e))\n",
    "    FEATURES_CSV_PATH = None\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error finding features CSV: {e}\")\n",
    "    FEATURES_CSV_PATH = None\n",
    "\n",
    "MODEL_OUTPUT_DIR = current_notebook_dir.parent / \"trained_models\"\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Trained models will be saved in: {MODEL_OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "# --- Load the Features DataFrame ---\n",
    "# ... (rest of data loading as before) ...\n",
    "df_model_data = pd.DataFrame()\n",
    "\n",
    "if FEATURES_CSV_PATH and FEATURES_CSV_PATH.exists():\n",
    "    try:\n",
    "        df_model_data = pd.read_csv(FEATURES_CSV_PATH)\n",
    "        logger.info(f\"Successfully loaded features data from: {FEATURES_CSV_PATH.resolve()}\")\n",
    "        logger.info(f\"Shape of loaded data: {df_model_data.shape}\")\n",
    "        \n",
    "        print(\"--- Data Head ---\")\n",
    "        print(df_model_data.head())\n",
    "        print(\"\\n--- Data Info ---\")\n",
    "        df_model_data.info()\n",
    "        print(\"\\n--- Data Description (Numerical) ---\")\n",
    "        print(df_model_data.describe().to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error loading features CSV {FEATURES_CSV_PATH.resolve()}: {e}\")\n",
    "else:\n",
    "    if FEATURES_CSV_PATH:\n",
    "         logger.critical(f\"Features CSV file not found at the specified path: {FEATURES_CSV_PATH.resolve()}\")\n",
    "    else:\n",
    "         logger.critical(\"FEATURES_CSV_PATH was not set (likely due to an error finding the file). Cannot load data.\")\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.warning(\"DataFrame df_model_data is empty. Subsequent cells might fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preprocessing, Feature Selection, and Splitting\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.error(\"df_model_data is empty. Cannot proceed with preprocessing and splitting. Please ensure Cell 1 ran correctly and loaded data.\")\n",
    "    # Depending on your workflow, you might want to raise an error or stop execution here\n",
    "    # For now, we'll let it proceed, but subsequent steps will likely fail or do nothing.\n",
    "else:\n",
    "    logger.info(f\"Starting preprocessing for df_model_data with shape: {df_model_data.shape}\")\n",
    "\n",
    "    # --- 1. Ensure Chronological Order ---\n",
    "    # This should have been done when saving, but good to double-check or re-apply\n",
    "    df_model_data.sort_values(by='decision_point_ts_utc', inplace=True)\n",
    "    df_model_data.reset_index(drop=True, inplace=True)\n",
    "    logger.info(\"Data sorted by 'decision_point_ts_utc'.\")\n",
    "\n",
    "    # --- 2. Handle Missing Values (NaNs) ---\n",
    "    # Review NaN counts from Cell 1's output (df_model_data.info() and describe())\n",
    "    # and the feature generation process.\n",
    "\n",
    "    # Let's list the columns we expect to be features\n",
    "    # Exclude identifiers and the target variable itself\n",
    "    identifier_cols = ['kalshi_market_ticker', 'decision_point_ts_utc', 'kalshi_strike_price']\n",
    "    target_col = 'TARGET_btc_diff_from_strike'\n",
    "    \n",
    "    # All other columns are potential features\n",
    "    feature_columns = [col for col in df_model_data.columns if col not in identifier_cols + [target_col]]\n",
    "    \n",
    "    logger.info(f\"Potential feature columns ({len(feature_columns)}): {feature_columns}\")\n",
    "\n",
    "    # NaN Handling Strategy:\n",
    "    # For a first pass with Linear Regression, we typically need complete data.\n",
    "    # Option A: Drop rows with any NaNs in the selected feature_columns.\n",
    "    # Option B: Impute.\n",
    "    \n",
    "    # Let's check NaN counts for our selected feature_columns\n",
    "    nan_summary = df_model_data[feature_columns].isnull().sum()\n",
    "    nan_summary = nan_summary[nan_summary > 0].sort_values(ascending=False)\n",
    "    if not nan_summary.empty:\n",
    "        logger.warning(f\"NaN values found in feature columns:\\n{nan_summary}\")\n",
    "        \n",
    "        # --- Imputation Strategy (Example - can be refined) ---\n",
    "        # For Kalshi price features (bid, ask, spread, mid, changes), NaNs often mean no market activity.\n",
    "        # Imputing with 0 or a special value might be an option.\n",
    "        # For TA indicators, initial NaNs are expected.\n",
    "        \n",
    "        cols_to_fill_zero = [\n",
    "            col for col in feature_columns if 'kalshi_mid_chg' in col or \\\n",
    "            'btc_mom' in col # Momentum can be zero if no change or at start\n",
    "        ]\n",
    "        cols_to_fill_median = [ # Median is often more robust to outliers than mean\n",
    "            col for col in feature_columns if 'btc_vol' in col or \\\n",
    "            'btc_sma' in col or 'btc_ema' in col \n",
    "        ]\n",
    "        cols_to_fill_rsi_neutral = [col for col in feature_columns if 'btc_rsi' in col]\n",
    "\n",
    "        # Kalshi bid/ask/spread/mid_price NaNs are tricky.\n",
    "        # If NaN, it means no quote. For now, let's fill with a value that might indicate this.\n",
    "        # Or, consider creating a binary feature \"kalshi_quotes_available\".\n",
    "        # For simplicity, let's fill yes_bid with 0, yes_ask with 100 (max spread, low confidence)\n",
    "        # and mid_price with 50. Spread would then be 100.\n",
    "        # This is a very basic strategy and might introduce bias or noise.\n",
    "        \n",
    "        if 'kalshi_yes_bid' in df_model_data.columns:\n",
    "            df_model_data['kalshi_yes_bid'] = df_model_data['kalshi_yes_bid'].fillna(0) # Assign back\n",
    "            logger.info(\"Filled NaNs in 'kalshi_yes_bid' with 0.\")\n",
    "        if 'kalshi_yes_ask' in df_model_data.columns:\n",
    "            df_model_data['kalshi_yes_ask'] = df_model_data['kalshi_yes_ask'].fillna(100) # Assign back\n",
    "            logger.info(\"Filled NaNs in 'kalshi_yes_ask' with 100.\")\n",
    "        \n",
    "        # Re-calculate spread and mid_price if they existed and bids/asks were filled\n",
    "        if 'kalshi_yes_bid' in df_model_data.columns and 'kalshi_yes_ask' in df_model_data.columns:\n",
    "            if 'kalshi_spread' in df_model_data.columns:\n",
    "                df_model_data['kalshi_spread'] = df_model_data['kalshi_yes_ask'] - df_model_data['kalshi_yes_bid']\n",
    "                logger.info(\"Recalculated 'kalshi_spread' after filling bid/ask.\")\n",
    "            if 'kalshi_mid_price' in df_model_data.columns:\n",
    "                 df_model_data['kalshi_mid_price'] = (df_model_data['kalshi_yes_bid'] + df_model_data['kalshi_yes_ask']) / 2\n",
    "                 logger.info(\"Recalculated 'kalshi_mid_price' after filling bid/ask.\")\n",
    "        \n",
    "        for col in cols_to_fill_zero:\n",
    "            if col in df_model_data.columns:\n",
    "                df_model_data[col] = df_model_data[col].fillna(0) # Assign back\n",
    "                logger.info(f\"Filled NaNs in '{col}' with 0.\")\n",
    "\n",
    "        for col in cols_to_fill_median:\n",
    "            if col in df_model_data.columns:\n",
    "                median_val = df_model_data[col].median()\n",
    "                df_model_data[col] = df_model_data[col].fillna(median_val) # Assign back\n",
    "                logger.info(f\"Filled NaNs in '{col}' with its median ({median_val:.4f}).\")\n",
    "\n",
    "        for col in cols_to_fill_rsi_neutral:\n",
    "            if col in df_model_data.columns:\n",
    "                df_model_data[col] = df_model_data[col].fillna(50) # Assign back\n",
    "                logger.info(f\"Filled NaNs in '{col}' with 50.\")\n",
    "\n",
    "        # For remaining NaNs in features (e.g., volume, open interest if sparse), drop rows\n",
    "        original_row_count = len(df_model_data)\n",
    "        df_model_data.dropna(subset=feature_columns, inplace=True)\n",
    "        logger.info(f\"Dropped {original_row_count - len(df_model_data)} rows due to remaining NaNs in feature columns after imputation attempts.\")\n",
    "        \n",
    "        final_nan_summary = df_model_data[feature_columns].isnull().sum()\n",
    "        final_nan_summary = final_nan_summary[final_nan_summary > 0]\n",
    "        if not final_nan_summary.empty:\n",
    "            logger.error(f\"Still have NaNs after processing! Columns:\\n{final_nan_summary}\")\n",
    "        else:\n",
    "            logger.info(\"Successfully handled NaNs in feature columns.\")\n",
    "\n",
    "    else:\n",
    "        logger.info(\"No NaNs found in the selected feature columns.\")\n",
    "        \n",
    "    # --- 3. Define Features (X) and Target (y) ---\n",
    "    if not df_model_data.empty:\n",
    "        X = df_model_data[feature_columns].copy() # Ensure we use the cleaned feature_columns\n",
    "        y = df_model_data[target_col].copy()\n",
    "        logger.info(f\"Defined X (features) with shape: {X.shape}\")\n",
    "        logger.info(f\"Defined y (target) with shape: {y.shape}\")\n",
    "\n",
    "        # --- 4. Split Data (Chronological) ---\n",
    "        # We'll use roughly 80% for training, 20% for testing.\n",
    "        # The data is already sorted by 'decision_point_ts_utc'.\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(X) * split_ratio)\n",
    "\n",
    "        X_train = X.iloc[:split_index]\n",
    "        y_train = y.iloc[:split_index]\n",
    "        X_test = X.iloc[split_index:]\n",
    "        y_test = y.iloc[split_index:]\n",
    "\n",
    "        logger.info(f\"Data split chronologically:\")\n",
    "        logger.info(f\"  X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        logger.info(f\"  X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        # Log the time range for train and test sets\n",
    "        train_start_ts = df_model_data['decision_point_ts_utc'].iloc[0]\n",
    "        train_end_ts = df_model_data['decision_point_ts_utc'].iloc[split_index - 1]\n",
    "        test_start_ts = df_model_data['decision_point_ts_utc'].iloc[split_index]\n",
    "        test_end_ts = df_model_data['decision_point_ts_utc'].iloc[-1]\n",
    "\n",
    "        logger.info(f\"  Training data from: {dt.datetime.fromtimestamp(train_start_ts, tz=timezone.utc).isoformat()} to {dt.datetime.fromtimestamp(train_end_ts, tz=timezone.utc).isoformat()}\")\n",
    "        logger.info(f\"  Test data from:     {dt.datetime.fromtimestamp(test_start_ts, tz=timezone.utc).isoformat()} to {dt.datetime.fromtimestamp(test_end_ts, tz=timezone.utc).isoformat()}\")\n",
    "        \n",
    "        # --- 5. Feature Scaling ---\n",
    "        # Linear models often benefit from scaling.\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test) # Use the scaler fitted on training data\n",
    "\n",
    "        # Convert scaled arrays back to DataFrames with original column names for easier inspection (optional)\n",
    "        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        logger.info(\"Features scaled using StandardScaler.\")\n",
    "        print(\"\\nSample of scaled training features (X_train_scaled_df head):\")\n",
    "        print(X_train_scaled_df.head())\n",
    "        \n",
    "        # Save the scaler\n",
    "        scaler_path = MODEL_OUTPUT_DIR / \"feature_scaler_v1.joblib\"\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        logger.info(f\"Scaler saved to: {scaler_path}\")\n",
    "        \n",
    "        # Also save the list of feature columns used for training (in order)\n",
    "        # This is CRITICAL for the backtester to use the same features.\n",
    "        feature_columns_list_path = MODEL_OUTPUT_DIR / \"feature_columns_v1.json\"\n",
    "        with open(feature_columns_list_path, 'w') as f:\n",
    "            json.dump(feature_columns.tolist() if isinstance(feature_columns, pd.Index) else feature_columns, f)\n",
    "        logger.info(f\"List of feature columns saved to: {feature_columns_list_path}\")\n",
    "\n",
    "    else:\n",
    "        logger.error(\"df_model_data is empty after NaN handling. Cannot proceed to define X, y, or split.\")\n",
    "        # Initialize X, y, etc. as empty or None to prevent errors in later cells if run out of order\n",
    "        X, y, X_train, y_train, X_test, y_test, X_train_scaled_df, X_test_scaled_df = [pd.DataFrame()]*8 \n",
    "        scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Training and Evaluation\n",
    "\n",
    "if 'X_train_scaled_df' not in globals() or X_train_scaled_df.empty:\n",
    "    logger.error(\"Scaled training data (X_train_scaled_df) not found or is empty. Please ensure Cell 2 ran successfully.\")\n",
    "    # Optionally, raise an error or stop notebook execution\n",
    "    # For now, this cell will likely fail if data is missing.\n",
    "else:\n",
    "    logger.info(\"--- Starting Model Training ---\")\n",
    "\n",
    "    # --- 1. Initialize and Train Linear Regression Model ---\n",
    "    linear_model = LinearRegression()\n",
    "    logger.info(f\"Training LinearRegression model on {X_train_scaled_df.shape[0]} samples...\")\n",
    "    \n",
    "    # Ensure y_train is available\n",
    "    if 'y_train' not in globals() or y_train.empty:\n",
    "        logger.error(\"y_train is not available. Cannot train model.\")\n",
    "    else:\n",
    "        try:\n",
    "            linear_model.fit(X_train_scaled_df, y_train)\n",
    "            logger.info(\"LinearRegression model training complete.\")\n",
    "\n",
    "            # --- 2. Make Predictions on the Test Set ---\n",
    "            logger.info(f\"Making predictions on the test set ({X_test_scaled_df.shape[0]} samples)...\")\n",
    "            y_pred_test = linear_model.predict(X_test_scaled_df)\n",
    "\n",
    "            # --- 3. Evaluate Model Performance (Regression Metrics) ---\n",
    "            if 'y_test' not in globals() or y_test.empty:\n",
    "                logger.error(\"y_test is not available. Cannot evaluate model.\")\n",
    "            else:\n",
    "                mae = mean_absolute_error(y_test, y_pred_test)\n",
    "                mse = mean_squared_error(y_test, y_pred_test)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "                logger.info(\"\\n--- Regression Model Evaluation Metrics (Test Set) ---\")\n",
    "                logger.info(f\"  Mean Absolute Error (MAE):      {mae:.4f}\")\n",
    "                logger.info(f\"  Mean Squared Error (MSE):       {mse:.4f}\")\n",
    "                logger.info(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "                logger.info(f\"  R-squared (R2 Score):           {r2:.4f}\")\n",
    "\n",
    "                # --- 4. Custom Evaluation (Trading-Oriented) ---\n",
    "                # The target is BTC_price_at_resolution - kalshi_strike_price\n",
    "                # Prediction > 0 implies model thinks BTC will be above strike (favors Kalshi YES)\n",
    "                # Prediction < 0 implies model thinks BTC will be below strike (favors Kalshi NO)\n",
    "\n",
    "                # Actual outcome sign:\n",
    "                actual_outcome_sign = np.sign(y_test) # 1 if >0 (YES), -1 if <0 (NO), 0 if exactly on strike\n",
    "\n",
    "                # Predicted outcome sign:\n",
    "                predicted_outcome_sign = np.sign(y_pred_test)\n",
    "\n",
    "                # Accuracy of predicting the correct side of the strike\n",
    "                correct_side_predictions = np.sum(actual_outcome_sign == predicted_outcome_sign)\n",
    "                # Exclude cases where actual_outcome_sign is 0 (BTC landed exactly on strike) for a clearer accuracy\n",
    "                valid_outcomes_for_sign_accuracy = actual_outcome_sign[actual_outcome_sign != 0]\n",
    "                valid_predictions_for_sign_accuracy = predicted_outcome_sign[actual_outcome_sign != 0]\n",
    "                \n",
    "                if len(valid_outcomes_for_sign_accuracy) > 0:\n",
    "                    sign_accuracy = np.sum(valid_outcomes_for_sign_accuracy == valid_predictions_for_sign_accuracy) / len(valid_outcomes_for_sign_accuracy)\n",
    "                    logger.info(f\"  Accuracy (Predicting Side of Strike): {sign_accuracy:.4f} (on {len(valid_outcomes_for_sign_accuracy)} non-zero outcome samples)\")\n",
    "                else:\n",
    "                    logger.info(\"  Accuracy (Predicting Side of Strike): N/A (no non-zero actual outcomes)\")\n",
    "\n",
    "                # Create a DataFrame for easier analysis of predictions vs actuals\n",
    "                df_results = pd.DataFrame({\n",
    "                    'actual_target': y_test,\n",
    "                    'predicted_target': y_pred_test,\n",
    "                    'actual_sign': actual_outcome_sign,\n",
    "                    'predicted_sign': predicted_outcome_sign\n",
    "                })\n",
    "                print(\"\\n--- Sample of Test Set Predictions vs Actuals ---\")\n",
    "                print(df_results.head(10).to_string())\n",
    "\n",
    "                # --- 5. Inspect Model Coefficients ---\n",
    "                logger.info(\"\\n--- Model Coefficients ---\")\n",
    "                logger.info(f\"Intercept: {linear_model.intercept_:.4f}\")\n",
    "                \n",
    "                # Ensure feature_columns is available (should be from Cell 2, or loaded from JSON)\n",
    "                if 'feature_columns' not in globals():\n",
    "                    feature_columns_list_path = MODEL_OUTPUT_DIR / \"feature_columns_v1.json\"\n",
    "                    if feature_columns_list_path.exists():\n",
    "                        with open(feature_columns_list_path, 'r') as f:\n",
    "                            feature_columns = json.load(f)\n",
    "                        logger.info(f\"Loaded feature_columns list from {feature_columns_list_path}\")\n",
    "                    else:\n",
    "                        logger.warning(\"feature_columns list not found. Cannot display coefficient names.\")\n",
    "                        feature_columns = [f\"feature_{i}\" for i in range(len(linear_model.coef_))]\n",
    "\n",
    "                coefficients = pd.DataFrame({'feature': feature_columns, 'coefficient': linear_model.coef_})\n",
    "                coefficients['abs_coefficient'] = np.abs(coefficients['coefficient'])\n",
    "                coefficients.sort_values(by='abs_coefficient', ascending=False, inplace=True)\n",
    "                \n",
    "                print(\"\\nTop Coefficients (by absolute value):\")\n",
    "                print(coefficients.head(20).to_string()) # Print top N coefficients\n",
    "\n",
    "                # --- 6. Save the Trained Model ---\n",
    "                model_path = MODEL_OUTPUT_DIR / \"linear_regression_btc_predictor_v1.joblib\"\n",
    "                joblib.dump(linear_model, model_path)\n",
    "                logger.info(f\"Trained Linear Regression model saved to: {model_path}\")\n",
    "\n",
    "                # Save model parameters (intercept, coefficients) to a JSON file for backtest.py\n",
    "                # This is an alternative to loading the whole joblib model in backtest.py\n",
    "                # and can be simpler if linreg_strategy.py only needs these.\n",
    "                model_params_for_backtest = {\n",
    "                    'intercept': linear_model.intercept_,\n",
    "                    'coefficients': dict(zip(feature_columns, linear_model.coef_)),\n",
    "                    'feature_order': feature_columns # Store the order for consistent dot product\n",
    "                }\n",
    "                params_path = MODEL_OUTPUT_DIR / \"lr_model_params_v1.json\"\n",
    "                with open(params_path, 'w') as f:\n",
    "                    json.dump(model_params_for_backtest, f, indent=4)\n",
    "                logger.info(f\"Model parameters (intercept, coefs, feature_order) saved to: {params_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"An error occurred during model training or evaluation: {e}\")\n",
    "            if 'linear_model' in locals():\n",
    "                 logger.info(\"Model training might have partially completed or failed during evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
