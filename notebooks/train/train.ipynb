{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import joblib \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import json \n",
    "\n",
    "# --- Logging Setup ---\n",
    "logger = logging.getLogger(\"training_logreg_per_minute\") # Changed logger name slightly\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Directories ---\n",
    "BASE_PROJECT_DIR = Path(\"/Users/omarabul-hassan/Desktop/projects/kalshi\")\n",
    "NOTEBOOKS_DIR = BASE_PROJECT_DIR / \"notebooks\"\n",
    "FEATURES_DIR = NOTEBOOKS_DIR / \"features\"\n",
    "TRAINED_MODELS_OUTPUT_DIR = NOTEBOOKS_DIR / \"trained_models\" / \"logreg_per_minute\" # New subdir for this model type\n",
    "\n",
    "TRAINED_MODELS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Features expected from: {FEATURES_DIR}\")\n",
    "logger.info(f\"Trained models (per-minute) will be saved to: {TRAINED_MODELS_OUTPUT_DIR}\")\n",
    "\n",
    "# --- Constants for Train/Test Split (based on market RESOLUTION time) ---\n",
    "TRAIN_END_DATE_STR = \"2025-05-08\" \n",
    "TEST_START_DATE_STR = \"2025-05-09\" \n",
    "TEST_END_DATE_STR = \"2025-05-15\"   \n",
    "\n",
    "TRAIN_UPTO_TS = int(dt.datetime.strptime(TRAIN_END_DATE_STR + \" 23:59:59\", \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc).timestamp())\n",
    "TEST_FROM_TS = int(dt.datetime.strptime(TEST_START_DATE_STR + \" 00:00:00\", \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc).timestamp())\n",
    "TEST_UPTO_TS = int(dt.datetime.strptime(TEST_END_DATE_STR + \" 23:59:59\", \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "logger.info(f\"Training data: Market resolutions up to {TRAIN_END_DATE_STR} (ts: {TRAIN_UPTO_TS})\")\n",
    "logger.info(f\"Testing data: Market resolutions from {TEST_START_DATE_STR} (ts: {TEST_FROM_TS}) to {TEST_END_DATE_STR} (ts: {TEST_UPTO_TS})\")\n",
    "\n",
    "logger.info(\"Cell 1: Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Features (MODIFIED)\n",
    "\n",
    "# Find the latest \"per-minute decision features\" CSV file\n",
    "# This uses the new filename pattern from the revised feature_engineering.ipynb\n",
    "list_of_feature_files = sorted(\n",
    "    glob.glob(str(FEATURES_DIR / \"kalshi_per_minute_decision_features_*.csv\")), # MODIFIED pattern\n",
    "    key=os.path.getctime,\n",
    "    reverse=True \n",
    ")\n",
    "\n",
    "if not list_of_feature_files:\n",
    "    logger.critical(f\"CRITICAL: No 'per_minute_decision_features' CSV files found in {FEATURES_DIR}.\")\n",
    "    raise FileNotFoundError(f\"No 'per_minute_decision_features' CSV files found. Ensure new feature_engineering.ipynb has run.\")\n",
    "else:\n",
    "    # Use the specific file you mentioned if you want to lock it, otherwise use latest\n",
    "    # LATEST_FEATURES_CSV_PATH = Path(\"/Users/omarabul-hassan/Desktop/projects/kalshi/notebooks/features/kalshi_per_minute_decision_features_20250522_172347.csv\")\n",
    "    LATEST_FEATURES_CSV_PATH = Path(list_of_feature_files[0]) # Or hardcode if preferred for reproducibility\n",
    "    \n",
    "    logger.info(f\"Loading PER-MINUTE features from: {LATEST_FEATURES_CSV_PATH}\")\n",
    "    try:\n",
    "        features_df = pd.read_csv(LATEST_FEATURES_CSV_PATH, low_memory=False)\n",
    "        logger.info(f\"Loaded PER-MINUTE features DataFrame with {features_df.shape[0]} rows and {features_df.shape[1]} columns.\")\n",
    "        \n",
    "        # Essential columns for this new structure\n",
    "        essential_cols = ['market_ticker', 'decision_timestamp_s', 'resolution_time_ts', 'target']\n",
    "        if any(col not in features_df.columns for col in essential_cols):\n",
    "            missing = [col for col in essential_cols if col not in features_df.columns]\n",
    "            logger.critical(f\"Essential columns ({missing}) not found in features DataFrame.\")\n",
    "            raise ValueError(f\"Features DataFrame missing essential columns: {missing}\")\n",
    "        \n",
    "        # Optional: Display info or head for a quick check\n",
    "        # logger.info(features_df.info())\n",
    "        # display(features_df.head())\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error loading features CSV {LATEST_FEATURES_CSV_PATH}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "logger.info(\"Cell 2: Per-minute feature loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Preprocessing and Feature Selection (SIGNIFICANTLY MODIFIED)\n",
    "\n",
    "if 'features_df' not in locals() or features_df.empty: \n",
    "    logger.error(\"Features DataFrame is empty or not loaded. Cannot proceed.\")\n",
    "    X, y, processed_df, MODEL_FEATURE_NAMES = pd.DataFrame(), pd.Series(dtype='float64'), pd.DataFrame(), [] \n",
    "else:\n",
    "    # --- 1. Define Feature Columns for the Per-Minute Model ---\n",
    "    # These are the new features generated by the revised feature_engineering.ipynb\n",
    "    # Exclude identifiers (market_ticker, decision_timestamp_s, resolution_time_ts) and the target.\n",
    "    \n",
    "    MODEL_FEATURE_NAMES = [\n",
    "        'strike_price',\n",
    "        'time_to_resolution_minutes',\n",
    "        'current_btc_price',\n",
    "        'current_dist_strike_abs',\n",
    "        'current_dist_strike_pct',\n",
    "        # Add lag features\n",
    "        'btc_price_change_pct_1m', 'btc_price_change_pct_3m', 'btc_price_change_pct_5m',\n",
    "        'btc_price_change_pct_10m', 'btc_price_change_pct_15m', 'btc_price_change_pct_30m',\n",
    "        # Add volatility features\n",
    "        'btc_volatility_5m', 'btc_volatility_15m', 'btc_volatility_30m',\n",
    "        # Add current Kalshi market state features\n",
    "        'current_kalshi_yes_bid', 'current_kalshi_yes_ask', 'current_kalshi_mid_price',\n",
    "        'current_kalshi_spread_abs', 'current_kalshi_spread_pct',\n",
    "        'current_kalshi_volume', 'current_kalshi_oi'\n",
    "    ]\n",
    "    \n",
    "    # Verify that all selected feature names actually exist in the loaded DataFrame\n",
    "    actual_columns_in_df = features_df.columns.tolist()\n",
    "    missing_model_features = [name for name in MODEL_FEATURE_NAMES if name not in actual_columns_in_df]\n",
    "    if missing_model_features:\n",
    "        logger.critical(f\"CRITICAL: The following selected MODEL_FEATURE_NAMES are MISSING from the loaded features_df: {missing_model_features}\")\n",
    "        logger.critical(f\"Available columns in features_df: {actual_columns_in_df}\")\n",
    "        raise ValueError(\"Selected model features are not present in the loaded data.\")\n",
    "    \n",
    "    logger.info(f\"Selected {len(MODEL_FEATURE_NAMES)} features for the per-minute model.\")\n",
    "    logger.info(f\"Selected features: {MODEL_FEATURE_NAMES}\")\n",
    "\n",
    "    # --- 2. Handle Missing Values (NaNs) for SELECTED FEATURES ---\n",
    "    processed_df = features_df.copy() # Work on a copy\n",
    "    \n",
    "    # Impute NaNs ONLY in the columns we've selected for MODEL_FEATURE_NAMES\n",
    "    for col in MODEL_FEATURE_NAMES:\n",
    "        if processed_df[col].isnull().any(): \n",
    "            median_val = processed_df[col].median()\n",
    "            if pd.isna(median_val): \n",
    "                logger.warning(f\"Median for selected feature '{col}' is NaN. Filling with 0 as a fallback.\")\n",
    "                processed_df[col].fillna(0, inplace=True) \n",
    "            else:\n",
    "                processed_df[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    # --- Define X (features) and y (target) ---\n",
    "    X = processed_df[MODEL_FEATURE_NAMES]\n",
    "    y = processed_df['target']\n",
    "\n",
    "    # Final NaN check on X\n",
    "    if X.isnull().sum().sum() > 0:\n",
    "        logger.warning(f\"NaNs still present in X after imputation. This is unexpected for median imputation unless medians were also NaN and not handled by fallback.\")\n",
    "        logger.warning(f\"Columns with NaNs: \\n{X.isnull().sum()[X.isnull().sum() > 0]}\")\n",
    "        logger.info(\"Dropping rows with any remaining NaNs in X for robust training.\")\n",
    "        nan_rows_mask = X.isnull().any(axis=1)\n",
    "        X = X[~nan_rows_mask]\n",
    "        y = y[~nan_rows_mask] # Crucial: Align y with X\n",
    "        logger.info(f\"New X shape after dropping NaN rows: {X.shape}\")\n",
    "    else:\n",
    "        logger.info(\"Median imputation for selected features complete. No NaNs remaining in X.\")\n",
    "\n",
    "    logger.info(f\"Shape of X (features): {X.shape}\")\n",
    "    logger.info(f\"Shape of y (target): {y.shape}\")\n",
    "    if not y.empty:\n",
    "        logger.info(f\"Target value counts:\\n{y.value_counts(normalize=True)}\")\n",
    "    \n",
    "    if not X.empty:\n",
    "        display(X.head())\n",
    "    else:\n",
    "        logger.warning(\"Feature set X is empty after selection/preprocessing.\")\n",
    "\n",
    "    # We also need `processed_df` to contain `MODEL_FEATURE_NAMES` and `target` for the split,\n",
    "    # and `resolution_time_ts` for filtering. So, ensure `processed_df` also reflects dropped rows if `X.dropna` was used.\n",
    "    # If X was modified by dropping rows, processed_df needs to be re-aligned.\n",
    "    # Assuming X and y are now aligned:\n",
    "    # processed_df = processed_df.loc[X.index] # Re-align processed_df to match X and y\n",
    "    # This is tricky if X.dropna() was used. Simpler: ensure processed_df passed to split has clean features.\n",
    "    # The split cell will use `processed_df_for_split` which is a copy of the `processed_df` from *this cell*.\n",
    "    # If X had rows dropped, `processed_df` here needs to reflect that for the split mask to align.\n",
    "    # Let's ensure 'processed_df' contains the final X and y indices for the split\n",
    "    if X.shape[0] < len(features_df): # if rows were dropped from X\n",
    "         processed_df = processed_df.loc[X.index] # Align processed_df with the cleaned X\n",
    "         logger.info(f\"Aligned 'processed_df' with cleaned X. New processed_df shape: {processed_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Time-Based Train-Test Split (MODIFIED)\n",
    "\n",
    "if 'processed_df' not in locals() or processed_df.empty or 'X' not in locals() or X.empty:\n",
    "    logger.error(\"processed_df or X is not available or empty from Cell 3. Cannot split.\")\n",
    "    X_train, y_train, X_test, y_test, test_identifiers_df = pd.DataFrame(), pd.Series(dtype='float64'), pd.DataFrame(), pd.Series(dtype='float64'), pd.DataFrame()\n",
    "else:\n",
    "    # `processed_df` from Cell 3 already has MODEL_FEATURE_NAMES (imputed) and 'target'.\n",
    "    # It also has 'resolution_time_ts', 'market_ticker', 'decision_timestamp_s'.\n",
    "    \n",
    "    # Ensure 'resolution_time_ts' is numeric for filtering\n",
    "    if 'resolution_time_ts' not in processed_df.columns:\n",
    "        raise ValueError(\"'resolution_time_ts' column is missing from processed_df for splitting.\")\n",
    "    processed_df['resolution_time_ts'] = pd.to_numeric(processed_df['resolution_time_ts'], errors='coerce')\n",
    "    \n",
    "    # Drop rows where resolution_time_ts could not be parsed (should be rare if data is clean)\n",
    "    initial_rows_before_ts_dropna = len(processed_df)\n",
    "    processed_df.dropna(subset=['resolution_time_ts'], inplace=True)\n",
    "    if len(processed_df) < initial_rows_before_ts_dropna:\n",
    "        logger.warning(f\"Dropped {initial_rows_before_ts_dropna - len(processed_df)} rows due to NaN in 'resolution_time_ts' for splitting.\")\n",
    "        # If rows were dropped here, X and y also need to be re-aligned from this new processed_df\n",
    "        X = processed_df[MODEL_FEATURE_NAMES]\n",
    "        y = processed_df['target']\n",
    "        logger.info(f\"Re-aligned X and y after 'resolution_time_ts' NaN drop. New X shape: {X.shape}\")\n",
    "\n",
    "\n",
    "    # Training set: all decision points for markets resolving up to TRAIN_UPTO_TS\n",
    "    train_mask = (processed_df['resolution_time_ts'] <= TRAIN_UPTO_TS)\n",
    "    \n",
    "    # Testing set: all decision points for markets resolving between TEST_FROM_TS and TEST_UPTO_TS\n",
    "    test_mask = (processed_df['resolution_time_ts'] >= TEST_FROM_TS) & \\\n",
    "                  (processed_df['resolution_time_ts'] <= TEST_UPTO_TS)\n",
    "\n",
    "    # Apply mask to the feature set X and target y\n",
    "    X_train = X[train_mask]\n",
    "    y_train = y[train_mask]\n",
    "    \n",
    "    X_test = X[test_mask]\n",
    "    y_test = y[test_mask]\n",
    "\n",
    "    # Store test set identifiers, now including decision_timestamp_s\n",
    "    test_identifiers_df = processed_df[test_mask][['market_ticker', 'decision_timestamp_s', 'resolution_time_ts', 'strike_price']].copy()\n",
    "\n",
    "    logger.info(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    logger.info(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    if X_train.empty or X_test.empty:\n",
    "        logger.critical(\"Training or testing set is empty after time-based split! Check date ranges and data volume.\")\n",
    "        min_res_ts = processed_df['resolution_time_ts'].min()\n",
    "        max_res_ts = processed_df['resolution_time_ts'].max()\n",
    "        logger.info(f\"Min resolution_time_ts in data: {dt.datetime.fromtimestamp(min_res_ts, tz=timezone.utc) if pd.notna(min_res_ts) else 'N/A'}\")\n",
    "        logger.info(f\"Max resolution_time_ts in data: {dt.datetime.fromtimestamp(max_res_ts, tz=timezone.utc) if pd.notna(max_res_ts) else 'N/A'}\")\n",
    "        logger.info(f\"TRAIN_UPTO_TS: {dt.datetime.fromtimestamp(TRAIN_UPTO_TS, tz=timezone.utc)}\")\n",
    "        logger.info(f\"TEST_FROM_TS: {dt.datetime.fromtimestamp(TEST_FROM_TS, tz=timezone.utc)}\")\n",
    "        logger.info(f\"TEST_UPTO_TS: {dt.datetime.fromtimestamp(TEST_UPTO_TS, tz=timezone.utc)}\")\n",
    "    else:\n",
    "        logger.info(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "        logger.info(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        \n",
    "        if X_train.isnull().sum().sum() > 0: logger.warning(f\"NaNs found in X_train after split: {X_train.isnull().sum().sum()}\")\n",
    "        if X_test.isnull().sum().sum() > 0: logger.warning(f\"NaNs found in X_test after split: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Model Training (Logistic Regression)\n",
    "\n",
    "if 'X_train' not in locals() or X_train.empty:\n",
    "    logger.error(\"X_train is not available or empty. Cannot train model.\")\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    if not X_test.empty:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_test_scaled = np.array([]) \n",
    "        logger.warning(\"X_test is empty, X_test_scaled will be empty. Evaluation might not be possible.\")\n",
    "    \n",
    "    logger.info(\"Numeric features scaled using StandardScaler.\")\n",
    "\n",
    "    logreg_model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced', C=1.0, max_iter=1000) # Added max_iter\n",
    "    \n",
    "    logger.info(f\"Training Logistic Regression model on {X_train_scaled.shape[0]} samples...\")\n",
    "    logreg_model.fit(X_train_scaled, y_train)\n",
    "    logger.info(\"Model training complete.\")\n",
    "\n",
    "    scaler_filename = \"logreg_per_minute_scaler.joblib\" # New naming\n",
    "    model_filename = \"logreg_per_minute_model.joblib\"   # New naming\n",
    "    feature_names_filename = \"logreg_per_minute_feature_names.json\" \n",
    "\n",
    "    try:\n",
    "        joblib.dump(scaler, TRAINED_MODELS_OUTPUT_DIR / scaler_filename)\n",
    "        logger.info(f\"Scaler saved to {TRAINED_MODELS_OUTPUT_DIR / scaler_filename}\")\n",
    "        \n",
    "        joblib.dump(logreg_model, TRAINED_MODELS_OUTPUT_DIR / model_filename)\n",
    "        logger.info(f\"Model saved to {TRAINED_MODELS_OUTPUT_DIR / model_filename}\")\n",
    "\n",
    "        if 'MODEL_FEATURE_NAMES' in locals() and MODEL_FEATURE_NAMES:\n",
    "            with open(TRAINED_MODELS_OUTPUT_DIR / feature_names_filename, 'w') as f:\n",
    "                json.dump(MODEL_FEATURE_NAMES, f)\n",
    "            logger.info(f\"Feature names used for model saved to {TRAINED_MODELS_OUTPUT_DIR / feature_names_filename}\")\n",
    "        else:\n",
    "            logger.warning(\"MODEL_FEATURE_NAMES not defined or empty. Not saving feature names list.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving model or scaler: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Evaluation (MODIFIED)\n",
    "\n",
    "model_ready = 'logreg_model' in locals()\n",
    "test_data_available = ('X_test_scaled' in locals() and isinstance(X_test_scaled, np.ndarray) and X_test_scaled.size > 0 and \\\n",
    "                       'y_test' in locals() and not y_test.empty and \\\n",
    "                       'test_identifiers_df' in locals() and not test_identifiers_df.empty)\n",
    "\n",
    "\n",
    "if not model_ready or not test_data_available:\n",
    "    logger.error(\"Model not trained or test data (X_test_scaled, y_test, test_identifiers_df) not available/empty. Cannot evaluate.\")\n",
    "else:\n",
    "    logger.info(\"Evaluating model on the test set (per-minute decision points)...\")\n",
    "    \n",
    "    y_pred_test = logreg_model.predict(X_test_scaled)\n",
    "    y_pred_proba_test = logreg_model.predict_proba(X_test_scaled)[:, 1] \n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "        logger.info(f\"Test Set ROC AUC Score: {roc_auc:.4f}\")\n",
    "    except ValueError as e:\n",
    "        logger.warning(f\"Could not calculate ROC AUC: {e}. This can happen if only one class is present in y_test.\")\n",
    "        roc_auc = np.nan # Or handle as needed\n",
    "    \n",
    "    logger.info(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    logger.info(\"\\nTest Set Classification Report:\\n\" + classification_report(y_test, y_pred_test, zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred No', 'Pred Yes'], yticklabels=['Actual No', 'Actual Yes'])\n",
    "    plt.title('Test Set Confusion Matrix (Per-Minute Decisions)')\n",
    "    plt.ylabel('Actual Market Outcome')\n",
    "    plt.xlabel('Predicted Outcome at Decision Point')\n",
    "    plt.show()\n",
    "\n",
    "    # Store predictions with identifiers for backtesting analysis\n",
    "    # test_identifiers_df already has 'market_ticker', 'decision_timestamp_s', 'resolution_time_ts', 'strike_price'\n",
    "    # Ensure its index aligns with y_test for correct assignment if y_test was modified\n",
    "    test_predictions_df = test_identifiers_df.copy()\n",
    "    \n",
    "    # Defensive check: if y_test or X_test (and thus test_identifiers_df) had rows dropped, \n",
    "    # their indices might not be continuous from 0.\n",
    "    # It's safer to assign based on shared index if possible, or ensure order is maintained.\n",
    "    # Assuming test_identifiers_df.index matches y_test.index (as they come from the same mask on processed_df)\n",
    "    test_predictions_df['actual_target'] = y_test.values # .values to ignore index if y_test index is tricky\n",
    "    test_predictions_df['predicted_target_logreg'] = y_pred_test\n",
    "    test_predictions_df['predicted_proba_yes_logreg'] = y_pred_proba_test\n",
    "    \n",
    "    # Add time_to_resolution for context in predictions file\n",
    "    if 'time_to_resolution_minutes' in X_test.columns:\n",
    "         # X_test index should align with test_predictions_df index if created correctly\n",
    "        test_predictions_df['time_to_resolution_at_pred'] = X_test['time_to_resolution_minutes'].values\n",
    "    \n",
    "    predictions_filename = f\"logreg_per_minute_test_predictions_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    predictions_filepath = TRAINED_MODELS_OUTPUT_DIR / predictions_filename\n",
    "    test_predictions_df.to_csv(predictions_filepath, index=False)\n",
    "    logger.info(f\"Test set (per-minute) predictions saved to: {predictions_filepath}\")\n",
    "    display(test_predictions_df.head())\n",
    "\n",
    "logger.info(\"Cell 6: Model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
