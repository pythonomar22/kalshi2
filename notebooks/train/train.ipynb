{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Load Data for Classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "import logging\n",
    "import json # For saving feature_columns_list and imputation_values\n",
    "import joblib # For saving the model and scaler\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.linear_model import LogisticRegression # Changed from RandomForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV # For probability calibration\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# Using a timestamp in logger name to differentiate runs\n",
    "run_dt_str_log = dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "logger_name = f\"model_training_logreg_{run_dt_str_log}\" # Changed logger name\n",
    "logger = logging.getLogger(logger_name)\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration ---\n",
    "current_notebook_dir = Path.cwd() # Assumes notebook is in notebooks/train/\n",
    "FEATURES_DIR = current_notebook_dir.parent / \"features\"\n",
    "logger.info(f\"Attempting to find feature files in: {FEATURES_DIR.resolve()}\")\n",
    "\n",
    "try:\n",
    "    if not FEATURES_DIR.exists():\n",
    "        alt_features_dir = Path.cwd().parent.parent / \"features\" # Assuming project_root/features\n",
    "        if alt_features_dir.exists():\n",
    "            FEATURES_DIR = alt_features_dir\n",
    "            logger.info(f\"Primary FEATURES_DIR not found, using alternative: {FEATURES_DIR.resolve()}\")\n",
    "        else:\n",
    "            pass # Let glob catch it\n",
    "\n",
    "    # We'll use the same V2 features as before\n",
    "    feature_glob_pattern = \"kalshi_btc_features_target_v2_filtered_15m_*.csv\"\n",
    "    feature_files = sorted(FEATURES_DIR.glob(feature_glob_pattern), key=os.path.getctime, reverse=True)\n",
    "\n",
    "    if not feature_files:\n",
    "        logger.warning(f\"No '{feature_glob_pattern}' files found in {FEATURES_DIR.resolve()}. \"\n",
    "                       f\"Falling back to 'kalshi_btc_features_target_v1_filtered_*.csv' (older version)...\")\n",
    "        feature_glob_pattern_v1_filt = \"kalshi_btc_features_target_v1_filtered_*.csv\"\n",
    "        feature_files = sorted(FEATURES_DIR.glob(feature_glob_pattern_v1_filt), key=os.path.getctime, reverse=True)\n",
    "        if not feature_files:\n",
    "            raise FileNotFoundError(f\"No feature CSV files found in {FEATURES_DIR.resolve()} matching EITHER \"\n",
    "                                    f\"'{feature_glob_pattern}' OR '{feature_glob_pattern_v1_filt}'\")\n",
    "\n",
    "    FEATURES_CSV_PATH = feature_files[0]\n",
    "    logger.info(f\"Using features CSV: {FEATURES_CSV_PATH.resolve()}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.critical(str(e))\n",
    "    FEATURES_CSV_PATH = None\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error finding features CSV: {e}\", exc_info=True)\n",
    "    FEATURES_CSV_PATH = None\n",
    "\n",
    "# --- UPDATED MODEL_OUTPUT_DIR for Logistic Regression ---\n",
    "MODEL_OUTPUT_DIR = Path(\"/Users/omarabul-hassan/Desktop/projects/kalshi/notebooks/trained_models/logreg\")\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Trained Logistic Regression models and artifacts will be saved in: {MODEL_OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "# --- Load the Features DataFrame ---\n",
    "df_model_data = pd.DataFrame()\n",
    "if FEATURES_CSV_PATH and FEATURES_CSV_PATH.exists():\n",
    "    try:\n",
    "        df_model_data = pd.read_csv(FEATURES_CSV_PATH)\n",
    "        logger.info(f\"Successfully loaded features data from: {FEATURES_CSV_PATH.resolve()}\")\n",
    "        logger.info(f\"Shape of loaded data: {df_model_data.shape}\")\n",
    "        print(\"--- Data Head (Raw from CSV) ---\")\n",
    "        with pd.option_context('display.max_columns', None): print(df_model_data.head())\n",
    "        print(\"\\n--- Data Info (Raw from CSV) ---\")\n",
    "        df_model_data.info(verbose=True, show_counts=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error loading features CSV {FEATURES_CSV_PATH.resolve()}: {e}\")\n",
    "else:\n",
    "    if FEATURES_CSV_PATH: logger.critical(f\"Features CSV file not found: {FEATURES_CSV_PATH.resolve()}\")\n",
    "    else: logger.critical(\"FEATURES_CSV_PATH was not set. Cannot load data.\")\n",
    "\n",
    "if df_model_data.empty: logger.warning(\"DataFrame df_model_data is empty. Subsequent cells might fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preprocessing, Target Transformation, Feature Selection, and Splitting\n",
    "# This cell uses the SAME robust imputation logic as developed for RF. Artifact names will be updated for LogReg.\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.error(\"df_model_data is empty. Cannot proceed. Ensure Cell 1 ran correctly.\")\n",
    "    X, y, X_train, y_train, X_test, y_test, X_train_scaled_df, X_test_scaled_df = [pd.DataFrame()]*8\n",
    "    scaler = None\n",
    "    imputation_values = {}\n",
    "else:\n",
    "    logger.info(f\"Starting preprocessing for df_model_data with shape: {df_model_data.shape}\")\n",
    "\n",
    "    df_model_data.sort_values(by='decision_point_ts_utc', inplace=True)\n",
    "    df_model_data.reset_index(drop=True, inplace=True)\n",
    "    logger.info(\"Data sorted by 'decision_point_ts_utc'.\")\n",
    "\n",
    "    original_target_col = 'TARGET_btc_diff_from_strike'\n",
    "    classification_target_col = 'TARGET_market_resolves_yes'\n",
    "    if original_target_col not in df_model_data.columns:\n",
    "        raise ValueError(f\"Missing required column for target: {original_target_col}\")\n",
    "    df_model_data[classification_target_col] = (df_model_data[original_target_col] > 0).astype(int)\n",
    "    logger.info(f\"Created binary classification target '{classification_target_col}'.\")\n",
    "    logger.info(f\"Value counts for '{classification_target_col}':\\n{df_model_data[classification_target_col].value_counts(normalize=True)}\")\n",
    "\n",
    "    identifier_cols = ['kalshi_market_ticker', 'decision_point_ts_utc', 'kalshi_strike_price']\n",
    "    feature_columns = [\n",
    "        col for col in df_model_data.columns\n",
    "        if col not in identifier_cols + [original_target_col, classification_target_col]\n",
    "    ]\n",
    "    logger.info(f\"Identified {len(feature_columns)} feature columns.\")\n",
    "\n",
    "    original_len_before_target_nan_drop = len(df_model_data)\n",
    "    df_model_data.dropna(subset=[classification_target_col], inplace=True)\n",
    "    logger.info(f\"Dropped {original_len_before_target_nan_drop - len(df_model_data)} rows due to NaN in target '{classification_target_col}'.\")\n",
    "    \n",
    "    if df_model_data.empty:\n",
    "        logger.error(\"df_model_data is empty after dropping NaNs in target. Cannot proceed.\")\n",
    "        X, y, X_train, y_train, X_test, y_test, X_train_scaled_df, X_test_scaled_df = [pd.DataFrame()]*8\n",
    "        scaler = None\n",
    "        imputation_values = {}\n",
    "    else:\n",
    "        X = df_model_data[feature_columns].copy()\n",
    "        y = df_model_data[classification_target_col].copy()\n",
    "\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(X) * split_ratio)\n",
    "        X_train = X.iloc[:split_index].copy(); y_train = y.iloc[:split_index].copy()\n",
    "        X_test = X.iloc[split_index:].copy(); y_test = y.iloc[split_index:].copy()\n",
    "        logger.info(f\"Data split chronologically: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
    "\n",
    "        imputation_values = {}\n",
    "        logger.info(\"Calculating imputation values from X_train and applying to X_train & X_test...\")\n",
    "        def apply_imputation_value(df, col_name, value_to_fill):\n",
    "            if col_name in df.columns: df[col_name].fillna(value_to_fill, inplace=True)\n",
    "\n",
    "        if 'kalshi_yes_bid' in feature_columns:\n",
    "            val = 0.0; imputation_values['kalshi_yes_bid'] = val\n",
    "            apply_imputation_value(X_train, 'kalshi_yes_bid', val); apply_imputation_value(X_test, 'kalshi_yes_bid', val)\n",
    "        if 'kalshi_yes_ask' in feature_columns:\n",
    "            val = 100.0; imputation_values['kalshi_yes_ask'] = val\n",
    "            apply_imputation_value(X_train, 'kalshi_yes_ask', val); apply_imputation_value(X_test, 'kalshi_yes_ask', val)\n",
    "\n",
    "        if 'kalshi_spread' in feature_columns and 'kalshi_yes_ask' in X_train.columns and 'kalshi_yes_bid' in X_train.columns:\n",
    "            X_train['kalshi_spread'] = X_train['kalshi_yes_ask'] - X_train['kalshi_yes_bid']\n",
    "            X_test['kalshi_spread'] = X_test['kalshi_yes_ask'] - X_test['kalshi_yes_bid']\n",
    "        if 'kalshi_mid_price' in feature_columns and 'kalshi_yes_ask' in X_train.columns and 'kalshi_yes_bid' in X_train.columns:\n",
    "            X_train['kalshi_mid_price'] = (X_train['kalshi_yes_bid'] + X_train['kalshi_yes_ask']) / 2.0\n",
    "            X_test['kalshi_mid_price'] = (X_test['kalshi_yes_bid'] + X_test['kalshi_yes_ask']) / 2.0\n",
    "\n",
    "        for col in feature_columns:\n",
    "            if col in imputation_values: continue\n",
    "            fill_value_for_col = np.nan\n",
    "            if X_train[col].isnull().any():\n",
    "                if 'btc_mom' in col or 'kalshi_mid_chg' in col: fill_value_for_col = 0.0\n",
    "                elif 'btc_vol' in col or 'kalshi_mid_vol' in col: fill_value_for_col = X_train[col].median()\n",
    "                elif 'btc_sma' in col or 'btc_ema' in col: fill_value_for_col = X_train[col].median()\n",
    "                elif 'btc_price_vs_sma' in col or 'btc_price_vs_ema' in col: fill_value_for_col = 1.0\n",
    "                elif 'btc_rsi' in col: fill_value_for_col = 50.0\n",
    "                elif 'btc_atr' in col: fill_value_for_col = X_train[col].median()\n",
    "                elif col == 'distance_to_strike_norm_atr': fill_value_for_col = 0.0\n",
    "                elif col == 'kalshi_vs_btc_implied_spread': fill_value_for_col = 0.0\n",
    "                elif col == 'kalshi_spread': fill_value_for_col = X_train[col].median() \n",
    "                elif col == 'kalshi_mid_price': fill_value_for_col = X_train[col].median()\n",
    "                elif 'kalshi_volume_t_minus_1' in col or 'kalshi_open_interest_t_minus_1' in col: fill_value_for_col = 0.0\n",
    "                else: fill_value_for_col = X_train[col].median()\n",
    "                if pd.isna(fill_value_for_col): fill_value_for_col = 0.0\n",
    "            else: # No NaNs in X_train[col]\n",
    "                if 'btc_mom' in col or 'kalshi_mid_chg' in col: fill_value_for_col = 0.0\n",
    "                elif 'btc_price_vs_sma' in col or 'btc_price_vs_ema' in col: fill_value_for_col = 1.0\n",
    "                elif 'btc_rsi' in col: fill_value_for_col = 50.0\n",
    "                elif col == 'distance_to_strike_norm_atr': fill_value_for_col = 0.0\n",
    "                elif col == 'kalshi_vs_btc_implied_spread': fill_value_for_col = 0.0\n",
    "                elif col == 'kalshi_spread': fill_value_for_col = 100.0\n",
    "                elif col == 'kalshi_mid_price': fill_value_for_col = 50.0\n",
    "                elif 'kalshi_volume_t_minus_1' in col or 'kalshi_open_interest_t_minus_1' in col: fill_value_for_col = 0.0\n",
    "                else: fill_value_for_col = X_train[col].median() if not X_train[col].empty else 0.0\n",
    "                if pd.isna(fill_value_for_col): fill_value_for_col = 0.0\n",
    "            imputation_values[col] = fill_value_for_col\n",
    "            apply_imputation_value(X_train, col, fill_value_for_col); apply_imputation_value(X_test, col, fill_value_for_col)\n",
    "        logger.info(f\"Imputation values map created with {len(imputation_values)} entries.\")\n",
    "\n",
    "        if X_train.isnull().any().any():\n",
    "            nan_cols_train = X_train.columns[X_train.isnull().any()].tolist()\n",
    "            logger.warning(f\"X_train still has NaNs after imputation in columns: {nan_cols_train}. Dropping affected rows.\")\n",
    "            X_train.dropna(inplace=True); y_train = y_train.loc[X_train.index]\n",
    "        if X_test.isnull().any().any():\n",
    "            nan_cols_test = X_test.columns[X_test.isnull().any()].tolist()\n",
    "            logger.warning(f\"X_test still has NaNs after imputation in columns: {nan_cols_test}. Dropping affected rows.\")\n",
    "            X_test.dropna(inplace=True); y_test = y_test.loc[X_test.index]\n",
    "        logger.info(f\"Shapes after imputation: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
    "\n",
    "        if X_train.empty or X_test.empty:\n",
    "            logger.error(\"X_train or X_test is empty after NaN handling. Cannot proceed.\")\n",
    "            X_train_scaled_df, X_test_scaled_df = pd.DataFrame(), pd.DataFrame(); scaler = None\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "            X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "            logger.info(\"Features scaled using StandardScaler.\")\n",
    "\n",
    "            # --- ARTIFACT FILENAMES FOR LOGISTIC REGRESSION ---\n",
    "            # Using \"logreg_v2\" to indicate it uses the V2 feature set.\n",
    "            scaler_path_logreg_v2 = MODEL_OUTPUT_DIR / \"feature_scaler_logreg_v2.joblib\"\n",
    "            joblib.dump(scaler, scaler_path_logreg_v2)\n",
    "            logger.info(f\"Scaler for LogReg (V2 features) saved to: {scaler_path_logreg_v2}\")\n",
    "\n",
    "            feature_columns_list_path_logreg_v2 = MODEL_OUTPUT_DIR / \"feature_columns_logreg_v2.json\"\n",
    "            with open(feature_columns_list_path_logreg_v2, 'w') as f: json.dump(feature_columns, f)\n",
    "            logger.info(f\"List of V2 feature columns for LogReg saved to: {feature_columns_list_path_logreg_v2}\")\n",
    "\n",
    "            imputation_values_path_logreg_v2 = MODEL_OUTPUT_DIR / \"imputation_values_logreg_v2.json\"\n",
    "            serializable_imputation_values = {k: (v.item() if isinstance(v, np.generic) else v) for k, v in imputation_values.items()}\n",
    "            with open(imputation_values_path_logreg_v2, 'w') as f: json.dump(serializable_imputation_values, f, indent=4)\n",
    "            logger.info(f\"Imputation values for LogReg (V2 features) saved to: {imputation_values_path_logreg_v2}\")\n",
    "            \n",
    "            if not X_train.empty and 'decision_point_ts_utc' in df_model_data.columns:\n",
    "                try:\n",
    "                    train_start_ts_iso = dt.datetime.fromtimestamp(df_model_data.loc[X_train.index[0], 'decision_point_ts_utc'], tz=timezone.utc).isoformat()\n",
    "                    train_end_ts_iso = dt.datetime.fromtimestamp(df_model_data.loc[X_train.index[-1], 'decision_point_ts_utc'], tz=timezone.utc).isoformat()\n",
    "                    test_start_ts_iso = dt.datetime.fromtimestamp(df_model_data.loc[X_test.index[0], 'decision_point_ts_utc'], tz=timezone.utc).isoformat()\n",
    "                    test_end_ts_iso = dt.datetime.fromtimestamp(df_model_data.loc[X_test.index[-1], 'decision_point_ts_utc'], tz=timezone.utc).isoformat()\n",
    "                    logger.info(f\"  Training data from: {train_start_ts_iso} to {train_end_ts_iso}\")\n",
    "                    logger.info(f\"  Test data from:     {test_start_ts_iso} to {test_end_ts_iso}\")\n",
    "                except Exception as e: logger.warning(f\"Could not log train/test date ranges: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Logistic Regression Model Training and Evaluation\n",
    "\n",
    "if 'X_train_scaled_df' not in globals() or X_train_scaled_df.empty:\n",
    "    logger.error(\"Scaled training data (X_train_scaled_df) not found or is empty. Please ensure Cell 2 ran successfully.\")\n",
    "else:\n",
    "    logger.info(\"--- Starting Logistic Regression Model Training (with Calibration) ---\")\n",
    "\n",
    "    # --- 1. Initialize Base LogisticRegression Model ---\n",
    "    # Standard parameters for Logistic Regression. L2 penalty is common.\n",
    "    # 'liblinear' solver is good for smaller datasets, 'saga' or 'lbfgs' for larger.\n",
    "    # saga supports elasticnet, l1, l2. lbfgs supports l2 or none.\n",
    "    base_logreg_model = LogisticRegression(\n",
    "        solver='liblinear', # Good default, handles L1 and L2\n",
    "        penalty='l2',       # L2 regularization by default\n",
    "        C=1.0,              # Inverse of regularization strength; smaller values specify stronger regularization.\n",
    "        class_weight='balanced', # Adjusts weights inversely proportional to class frequencies\n",
    "        random_state=42,\n",
    "        max_iter=1000       # Increased max_iter for convergence\n",
    "    )\n",
    "    \n",
    "    # Model name for artifacts\n",
    "    model_name = \"CalibratedLogReg_classifier_v2\" # Using V2 features\n",
    "\n",
    "    # --- 2. Initialize and Train CalibratedClassifierCV ---\n",
    "    # We'll calibrate the Logistic Regression model as well.\n",
    "    logger.info(f\"Training {model_name} (Calibrated Logistic Regression with sigmoid) on {X_train_scaled_df.shape[0]} samples...\")\n",
    "    \n",
    "    calibrated_logreg_model = CalibratedClassifierCV(\n",
    "        base_logreg_model, # Pass the configured Logistic Regression\n",
    "        method='sigmoid',  # Platt scaling\n",
    "        cv=3               # Number of folds for calibration (3 or 5 is common)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Model parameters for CalibratedClassifierCV: {calibrated_logreg_model.get_params()}\")\n",
    "    \n",
    "    if 'y_train' not in globals() or y_train.empty:\n",
    "        logger.error(\"y_train (binary target) is not available. Cannot train calibrated model.\")\n",
    "    else:\n",
    "        try:\n",
    "            calibrated_logreg_model.fit(X_train_scaled_df, y_train) # Fit on scaled data\n",
    "            logger.info(f\"{model_name} model training complete.\")\n",
    "\n",
    "            # --- 3. Make Predictions on the Test Set ---\n",
    "            if X_test_scaled_df.empty:\n",
    "                logger.warning(\"Test set (X_test_scaled_df) is empty. Skipping predictions and evaluation.\")\n",
    "            else:\n",
    "                logger.info(f\"Making predictions with {model_name} on the test set ({X_test_scaled_df.shape[0]} samples)...\\n\")\n",
    "                y_pred_test_class = calibrated_logreg_model.predict(X_test_scaled_df)\n",
    "                y_pred_test_proba = calibrated_logreg_model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "\n",
    "                # --- 4. Evaluate Model Performance ---\n",
    "                if 'y_test' not in globals() or y_test.empty:\n",
    "                    logger.error(\"y_test (binary target) is not available. Cannot evaluate model.\")\n",
    "                else:\n",
    "                    accuracy = accuracy_score(y_test, y_pred_test_class)\n",
    "                    precision = precision_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                    recall = recall_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                    f1 = f1_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                    try: roc_auc = roc_auc_score(y_test, y_pred_test_proba)\n",
    "                    except ValueError as e: logger.warning(f\"Could not calculate ROC AUC: {e}\"); roc_auc = np.nan\n",
    "                    logloss = log_loss(y_test, y_pred_test_proba)\n",
    "\n",
    "                    logger.info(f\"--- {model_name} Evaluation Metrics (Test Set) ---\")\n",
    "                    logger.info(f\"  Accuracy:          {accuracy:.4f}\")\n",
    "                    logger.info(f\"  Precision:         {precision:.4f}\")\n",
    "                    logger.info(f\"  Recall (TPR):      {recall:.4f}\")\n",
    "                    logger.info(f\"  F1-Score:          {f1:.4f}\")\n",
    "                    logger.info(f\"  ROC AUC:           {roc_auc:.4f}\")\n",
    "                    logger.info(f\"  Log Loss:          {logloss:.4f}\")\n",
    "\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    plt.figure(figsize=(10,6))\n",
    "                    plt.hist(y_pred_test_proba, bins=50, alpha=0.7, label=f'P(YES) - {model_name}')\n",
    "                    plt.title(f'Histogram of Predicted Probabilities (P(YES)) on Test Set - {model_name}')\n",
    "                    plt.xlabel('Predicted Probability of YES')\n",
    "                    plt.ylabel('Frequency')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    logger.info(f\"Description of P(YES) for {model_name} on test set:\\n{pd.Series(y_pred_test_proba).describe().to_string()}\")\n",
    "\n",
    "                    logger.info(f\"\\n--- Confusion Matrix (Test Set) - {model_name} ---\")\n",
    "                    cm = confusion_matrix(y_test, y_pred_test_class)\n",
    "                    logger.info(f\"\\n{cm}\")\n",
    "                    try:\n",
    "                        tn, fp, fn, tp = cm.ravel()\n",
    "                        logger.info(f\"  True Negatives (TN):  {tn}\")\n",
    "                        logger.info(f\"  False Positives (FP): {fp} (Type I Error)\")\n",
    "                        logger.info(f\"  False Negatives (FN): {fn} (Type II Error)\")\n",
    "                        logger.info(f\"  True Positives (TP):  {tp}\")\n",
    "                    except ValueError: logger.warning(\"Could not unpack full confusion matrix.\")\n",
    "\n",
    "                    # --- 5. Inspect Coefficients (for Logistic Regression) ---\n",
    "                    # Coefficients are from the *base* logistic regression model within CalibratedClassifierCV.\n",
    "                    # CalibratedClassifierCV fits multiple base models if cv > 1.\n",
    "                    # We'll look at coefficients from the first calibrated estimator's base model.\n",
    "                    logger.info(f\"\\n\\n--- Coefficients (from base model of {model_name}) ---\")\n",
    "                    try:\n",
    "                        # Access the base estimator from one of the calibrated classifiers\n",
    "                        # (e.g., the first one, assuming they are similar)\n",
    "                        if calibrated_logreg_model.calibrated_classifiers_ and \\\n",
    "                           hasattr(calibrated_logreg_model.calibrated_classifiers_[0].estimator, 'coef_'):\n",
    "                            \n",
    "                            base_estimator_for_coef = calibrated_logreg_model.calibrated_classifiers_[0].estimator\n",
    "                            coefficients = base_estimator_for_coef.coef_[0] # Coef_ is 2D for binary, get first row\n",
    "\n",
    "                            if 'feature_columns' in globals() and len(feature_columns) == len(coefficients):\n",
    "                                feature_names_for_coef = feature_columns\n",
    "                            else: # Fallback if feature_columns global isn't perfect match\n",
    "                                feature_names_for_coef = [f\"feature_{i}\" for i in range(len(coefficients))]\n",
    "                                logger.warning(\"Using generic feature names for coefficients as 'feature_columns' global mismatch.\")\n",
    "\n",
    "                            coef_df = pd.DataFrame({'feature': feature_names_for_coef, 'coefficient': coefficients})\n",
    "                            coef_df['abs_coefficient'] = np.abs(coef_df['coefficient'])\n",
    "                            coef_df.sort_values(by='abs_coefficient', ascending=False, inplace=True)\n",
    "                            \n",
    "                            print(\"Top Feature Coefficients (by absolute value):\")\n",
    "                            print(coef_df[['feature', 'coefficient']].head(30).to_string())\n",
    "                        else:\n",
    "                            logger.warning(f\"Could not retrieve coefficients from the base LogReg model for {model_name}.\")\n",
    "                    except Exception as e_coef:\n",
    "                        logger.error(f\"Error retrieving coefficients: {e_coef}\")\n",
    "\n",
    "\n",
    "                    # --- 6. Save the Trained CALIBRATED Model ---\n",
    "                    model_path = MODEL_OUTPUT_DIR / f\"{model_name}.joblib\"\n",
    "                    joblib.dump(calibrated_logreg_model, model_path)\n",
    "                    logger.info(f\"Trained {model_name} model saved to: {model_path}\")\n",
    "\n",
    "                    model_hyperparams = calibrated_logreg_model.get_params(deep=True)\n",
    "                    params_path = MODEL_OUTPUT_DIR / f\"{model_name}_hyperparams.json\"\n",
    "                    with open(params_path, 'w') as f:\n",
    "                        serializable_params = {}\n",
    "                        for k_param, v_param in model_hyperparams.items():\n",
    "                            if isinstance(v_param, np.ndarray): serializable_params[k_param] = v_param.tolist()\n",
    "                            elif isinstance(v_param, (np.bool_, np.integer, np.floating)): serializable_params[k_param] = v_param.item()\n",
    "                            elif isinstance(v_param, (BaseException)): serializable_params[k_param] = str(v_param) # e.g. for base_estimator\n",
    "                            else: serializable_params[k_param] = v_param\n",
    "                        json.dump(serializable_params, f, indent=4, default=lambda o: '<not serializable>')\n",
    "                    logger.info(f\"{model_name} hyperparameters saved to: {params_path}\")\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"An error occurred during {model_name} model training or evaluation: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
