{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import joblib \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import json \n",
    "\n",
    "# --- Logging Setup ---\n",
    "logger = logging.getLogger(\"training_logreg_per_minute_v2\") # Changed logger name\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    # Clear existing handlers if re-running cell\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    logger.setLevel(logging.INFO) # Ensure level is set after clearing\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "\n",
    "# --- Directories ---\n",
    "BASE_PROJECT_DIR = Path(\"/Users/omarabul-hassan/Desktop/projects/kalshi\")\n",
    "NOTEBOOKS_DIR = BASE_PROJECT_DIR / \"notebooks\"\n",
    "FEATURES_DIR = NOTEBOOKS_DIR / \"features\" # Using historical features for this retraining\n",
    "\n",
    "# *** MODIFIED: New output directory for this version of the model ***\n",
    "MODEL_VERSION_SUFFIX = \"no_vol_oi\" \n",
    "TRAINED_MODELS_OUTPUT_DIR = NOTEBOOKS_DIR / \"trained_models\" / f\"logreg_per_minute_{MODEL_VERSION_SUFFIX}\"\n",
    "\n",
    "TRAINED_MODELS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Features expected from: {FEATURES_DIR}\")\n",
    "logger.info(f\"Trained models (per-minute, {MODEL_VERSION_SUFFIX}) will be saved to: {TRAINED_MODELS_OUTPUT_DIR}\")\n",
    "\n",
    "# --- Constants for Train/Test Split (based on market RESOLUTION time) ---\n",
    "# These remain the same as you're using your historical data for retraining\n",
    "TRAIN_END_DATE_STR = \"2025-05-08\" \n",
    "TEST_START_DATE_STR = \"2025-05-09\" \n",
    "TEST_END_DATE_STR = \"2025-05-15\"   \n",
    "\n",
    "TRAIN_UPTO_TS = int(dt.datetime.strptime(TRAIN_END_DATE_STR + \" 23:59:59\", \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc).timestamp())\n",
    "TEST_FROM_TS = int(dt.datetime.strptime(TEST_START_DATE_STR + \" 00:00:00\", \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc).timestamp())\n",
    "TEST_UPTO_TS = int(dt.datetime.strptime(TEST_END_DATE_STR + \" 23:59:59\", \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "logger.info(f\"Training data: Market resolutions up to {TRAIN_END_DATE_STR} (ts: {TRAIN_UPTO_TS})\")\n",
    "logger.info(f\"Testing data: Market resolutions from {TEST_START_DATE_STR} (ts: {TEST_FROM_TS}) to {TEST_END_DATE_STR} (ts: {TEST_UPTO_TS})\")\n",
    "\n",
    "logger.info(\"Cell 1: Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Features \n",
    "\n",
    "# Find the latest \"per-minute decision features\" CSV file\n",
    "# This uses the filename pattern from the original (historical) feature_engineering.ipynb\n",
    "list_of_feature_files = sorted(\n",
    "    glob.glob(str(FEATURES_DIR / \"kalshi_per_minute_decision_features_*.csv\")), \n",
    "    key=os.path.getctime,\n",
    "    reverse=True \n",
    ")\n",
    "\n",
    "if not list_of_feature_files:\n",
    "    logger.critical(f\"CRITICAL: No 'kalshi_per_minute_decision_features_*.csv' files found in {FEATURES_DIR}.\")\n",
    "    raise FileNotFoundError(f\"No historical 'per_minute_decision_features' CSV files found. Ensure historical feature_engineering.ipynb has run.\")\n",
    "else:\n",
    "    LATEST_FEATURES_CSV_PATH = Path(list_of_feature_files[0]) \n",
    "    \n",
    "    logger.info(f\"Loading HISTORICAL PER-MINUTE features from: {LATEST_FEATURES_CSV_PATH} for retraining.\")\n",
    "    try:\n",
    "        features_df = pd.read_csv(LATEST_FEATURES_CSV_PATH, low_memory=False)\n",
    "        logger.info(f\"Loaded HISTORICAL PER-MINUTE features DataFrame with {features_df.shape[0]} rows and {features_df.shape[1]} columns.\")\n",
    "        \n",
    "        essential_cols = ['market_ticker', 'decision_timestamp_s', 'resolution_time_ts', 'target']\n",
    "        if any(col not in features_df.columns for col in essential_cols):\n",
    "            missing = [col for col in essential_cols if col not in features_df.columns]\n",
    "            logger.critical(f\"Essential columns ({missing}) not found in features DataFrame.\")\n",
    "            raise ValueError(f\"Features DataFrame missing essential columns: {missing}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error loading features CSV {LATEST_FEATURES_CSV_PATH}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "logger.info(\"Cell 2: Historical per-minute feature loading complete for retraining.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Preprocessing and Feature Selection (MODIFIED for no_vol_oi model)\n",
    "\n",
    "if 'features_df' not in locals() or features_df.empty: \n",
    "    logger.error(\"Features DataFrame is empty or not loaded. Cannot proceed.\")\n",
    "    X, y, processed_df, MODEL_FEATURE_NAMES = pd.DataFrame(), pd.Series(dtype='float64'), pd.DataFrame(), [] \n",
    "else:\n",
    "    # --- 1. Define Feature Columns for the Per-Minute Model (NO VOLUME/OI) ---\n",
    "    \n",
    "    MODEL_FEATURE_NAMES = [\n",
    "        'strike_price',\n",
    "        'time_to_resolution_minutes',\n",
    "        'current_btc_price',\n",
    "        'current_dist_strike_abs',\n",
    "        'current_dist_strike_pct',\n",
    "        # Add lag features\n",
    "        'btc_price_change_pct_1m', 'btc_price_change_pct_3m', 'btc_price_change_pct_5m',\n",
    "        'btc_price_change_pct_10m', 'btc_price_change_pct_15m', 'btc_price_change_pct_30m',\n",
    "        # Add volatility features\n",
    "        'btc_volatility_5m', 'btc_volatility_15m', 'btc_volatility_30m',\n",
    "        # Add current Kalshi market state features\n",
    "        'current_kalshi_yes_bid', 'current_kalshi_yes_ask', 'current_kalshi_mid_price',\n",
    "        'current_kalshi_spread_abs', 'current_kalshi_spread_pct'\n",
    "        # 'current_kalshi_volume', 'current_kalshi_oi'  <--- REMOVED THESE\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"MODEL_VERSION_SUFFIX: {MODEL_VERSION_SUFFIX}\")\n",
    "    logger.info(f\"Selected {len(MODEL_FEATURE_NAMES)} features for the '{MODEL_VERSION_SUFFIX}' model.\")\n",
    "    logger.info(f\"Selected features: {MODEL_FEATURE_NAMES}\")\n",
    "\n",
    "    # Verify that all selected feature names actually exist in the loaded DataFrame\n",
    "    actual_columns_in_df = features_df.columns.tolist()\n",
    "    missing_model_features = [name for name in MODEL_FEATURE_NAMES if name not in actual_columns_in_df]\n",
    "    if missing_model_features:\n",
    "        logger.critical(f\"CRITICAL: The following selected MODEL_FEATURE_NAMES are MISSING from the loaded features_df: {missing_model_features}\")\n",
    "        logger.critical(f\"Available columns in features_df: {actual_columns_in_df}\")\n",
    "        raise ValueError(\"Selected model features are not present in the loaded data.\")\n",
    "    \n",
    "    # --- 2. Handle Missing Values (NaNs) for SELECTED FEATURES ---\n",
    "    # Your historical features *should* have values for volume and OI.\n",
    "    # If you run this cell as-is, the imputation will still run on the original features_df\n",
    "    # which includes current_kalshi_volume and current_kalshi_oi.\n",
    "    # However, X will only be selected from the new MODEL_FEATURE_NAMES.\n",
    "    \n",
    "    processed_df = features_df.copy() \n",
    "\n",
    "    # Impute NaNs in ALL original features_df columns that might be selected or related,\n",
    "    # just to be safe, even if some are dropped from MODEL_FEATURE_NAMES later.\n",
    "    # The historical features for volume/OI might have some NaNs, so impute them.\n",
    "    potential_features_to_impute = [\n",
    "        'strike_price', 'time_to_resolution_minutes', 'current_btc_price',\n",
    "        'current_dist_strike_abs', 'current_dist_strike_pct',\n",
    "        'btc_price_change_pct_1m', 'btc_price_change_pct_3m', 'btc_price_change_pct_5m',\n",
    "        'btc_price_change_pct_10m', 'btc_price_change_pct_15m', 'btc_price_change_pct_30m',\n",
    "        'btc_volatility_5m', 'btc_volatility_15m', 'btc_volatility_30m',\n",
    "        'current_kalshi_yes_bid', 'current_kalshi_yes_ask', 'current_kalshi_mid_price',\n",
    "        'current_kalshi_spread_abs', 'current_kalshi_spread_pct',\n",
    "        'current_kalshi_volume', 'current_kalshi_oi' # Impute these too, even if not used by this model version\n",
    "    ]\n",
    "\n",
    "    for col in potential_features_to_impute:\n",
    "        if col in processed_df.columns and processed_df[col].isnull().any(): \n",
    "            median_val = processed_df[col].median()\n",
    "            if pd.isna(median_val): \n",
    "                # This can happen if a column is ALL NaN, or if a non-numeric column slips through.\n",
    "                # The historical features should generally have valid medians for numeric cols.\n",
    "                logger.warning(f\"Median for feature '{col}' is NaN. Filling with 0 as a fallback.\")\n",
    "                processed_df[col].fillna(0, inplace=True) \n",
    "            else:\n",
    "                processed_df[col].fillna(median_val, inplace=True)\n",
    "                # logger.debug(f\"Imputed NaNs in '{col}' with median {median_val_temp}.\")\n",
    "    \n",
    "    # --- Define X (features) and y (target) using the NEW MODEL_FEATURE_NAMES ---\n",
    "    X = processed_df[MODEL_FEATURE_NAMES] # This uses the list without volume/OI\n",
    "    y = processed_df['target']\n",
    "\n",
    "    # Final NaN check on X (selected features)\n",
    "    if X.isnull().sum().sum() > 0:\n",
    "        logger.warning(f\"NaNs still present in X (final selected features) after imputation. This is unexpected.\")\n",
    "        logger.warning(f\"Columns with NaNs in X: \\n{X.isnull().sum()[X.isnull().sum() > 0]}\")\n",
    "        logger.info(\"Dropping rows with any remaining NaNs in X for robust training.\")\n",
    "        nan_rows_mask = X.isnull().any(axis=1)\n",
    "        X = X[~nan_rows_mask]\n",
    "        y = y[~nan_rows_mask] \n",
    "        logger.info(f\"New X shape after dropping NaN rows: {X.shape}\")\n",
    "    else:\n",
    "        logger.info(\"Imputation for relevant features complete. No NaNs remaining in the final selected X.\")\n",
    "\n",
    "    logger.info(f\"Shape of X (features for {MODEL_VERSION_SUFFIX} model): {X.shape}\")\n",
    "    logger.info(f\"Shape of y (target): {y.shape}\")\n",
    "    if not y.empty:\n",
    "        logger.info(f\"Target value counts:\\n{y.value_counts(normalize=True)}\")\n",
    "    \n",
    "    if not X.empty:\n",
    "        print(f\"Head of X for {MODEL_VERSION_SUFFIX} model:\")\n",
    "        display(X.head())\n",
    "    else:\n",
    "        logger.warning(\"Feature set X is empty after selection/preprocessing.\")\n",
    "\n",
    "    if X.shape[0] < len(features_df): \n",
    "         processed_df = processed_df.loc[X.index] \n",
    "         logger.info(f\"Aligned 'processed_df' with cleaned X. New processed_df shape: {processed_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Time-Based Train-Test Split \n",
    "\n",
    "if 'processed_df' not in locals() or processed_df.empty or 'X' not in locals() or X.empty:\n",
    "    logger.error(\"processed_df or X is not available or empty from Cell 3. Cannot split.\")\n",
    "    X_train, y_train, X_test, y_test, test_identifiers_df = pd.DataFrame(), pd.Series(dtype='float64'), pd.DataFrame(), pd.Series(dtype='float64'), pd.DataFrame()\n",
    "else:\n",
    "    if 'resolution_time_ts' not in processed_df.columns:\n",
    "        raise ValueError(\"'resolution_time_ts' column is missing from processed_df for splitting.\")\n",
    "    processed_df['resolution_time_ts'] = pd.to_numeric(processed_df['resolution_time_ts'], errors='coerce')\n",
    "    \n",
    "    initial_rows_before_ts_dropna = len(processed_df)\n",
    "    processed_df.dropna(subset=['resolution_time_ts'], inplace=True)\n",
    "    if len(processed_df) < initial_rows_before_ts_dropna:\n",
    "        logger.warning(f\"Dropped {initial_rows_before_ts_dropna - len(processed_df)} rows due to NaN in 'resolution_time_ts' for splitting.\")\n",
    "        # Re-align X and y if rows were dropped from processed_df\n",
    "        X = processed_df[MODEL_FEATURE_NAMES] # MODEL_FEATURE_NAMES is now the shorter list\n",
    "        y = processed_df['target']\n",
    "        logger.info(f\"Re-aligned X and y after 'resolution_time_ts' NaN drop. New X shape: {X.shape}\")\n",
    "\n",
    "    train_mask = (processed_df['resolution_time_ts'] <= TRAIN_UPTO_TS)\n",
    "    test_mask = (processed_df['resolution_time_ts'] >= TEST_FROM_TS) & \\\n",
    "                  (processed_df['resolution_time_ts'] <= TEST_UPTO_TS)\n",
    "\n",
    "    X_train = X[train_mask]\n",
    "    y_train = y[train_mask]\n",
    "    \n",
    "    X_test = X[test_mask]\n",
    "    y_test = y[test_mask]\n",
    "\n",
    "    test_identifiers_df = processed_df[test_mask][['market_ticker', 'decision_timestamp_s', 'resolution_time_ts', 'strike_price']].copy()\n",
    "\n",
    "    logger.info(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    logger.info(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    if X_train.empty or X_test.empty:\n",
    "        logger.critical(\"Training or testing set is empty after time-based split! Check date ranges and data volume.\")\n",
    "        min_res_ts = processed_df['resolution_time_ts'].min()\n",
    "        max_res_ts = processed_df['resolution_time_ts'].max()\n",
    "        logger.info(f\"Min resolution_time_ts in data: {dt.datetime.fromtimestamp(min_res_ts, tz=timezone.utc) if pd.notna(min_res_ts) else 'N/A'}\")\n",
    "        logger.info(f\"Max resolution_time_ts in data: {dt.datetime.fromtimestamp(max_res_ts, tz=timezone.utc) if pd.notna(max_res_ts) else 'N/A'}\")\n",
    "        logger.info(f\"TRAIN_UPTO_TS: {dt.datetime.fromtimestamp(TRAIN_UPTO_TS, tz=timezone.utc)}\")\n",
    "        logger.info(f\"TEST_FROM_TS: {dt.datetime.fromtimestamp(TEST_FROM_TS, tz=timezone.utc)}\")\n",
    "        logger.info(f\"TEST_UPTO_TS: {dt.datetime.fromtimestamp(TEST_UPTO_TS, tz=timezone.utc)}\")\n",
    "    else:\n",
    "        logger.info(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "        logger.info(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        \n",
    "        if X_train.isnull().sum().sum() > 0: logger.warning(f\"NaNs found in X_train after split: {X_train.isnull().sum().sum()}\")\n",
    "        if X_test.isnull().sum().sum() > 0: logger.warning(f\"NaNs found in X_test after split: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Model Training (Logistic Regression)\n",
    "\n",
    "if 'X_train' not in locals() or X_train.empty:\n",
    "    logger.error(\"X_train is not available or empty. Cannot train model.\")\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train) # X_train now has fewer features\n",
    "    \n",
    "    if not X_test.empty:\n",
    "        X_test_scaled = scaler.transform(X_test) # X_test also has fewer features\n",
    "    else:\n",
    "        X_test_scaled = np.array([]) \n",
    "        logger.warning(\"X_test is empty, X_test_scaled will be empty. Evaluation might not be possible.\")\n",
    "    \n",
    "    logger.info(\"Numeric features scaled using StandardScaler.\")\n",
    "\n",
    "    logreg_model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced', C=1.0, max_iter=1000)\n",
    "    \n",
    "    logger.info(f\"Training Logistic Regression model ({MODEL_VERSION_SUFFIX} version) on {X_train_scaled.shape[0]} samples...\")\n",
    "    logreg_model.fit(X_train_scaled, y_train)\n",
    "    logger.info(f\"Model training complete ({MODEL_VERSION_SUFFIX} version).\")\n",
    "\n",
    "    # *** MODIFIED: Filenames to reflect the new model version ***\n",
    "    base_filename = f\"logreg_per_minute_{MODEL_VERSION_SUFFIX}\"\n",
    "    scaler_filename = f\"{base_filename}_scaler.joblib\"\n",
    "    model_filename = f\"{base_filename}_model.joblib\"\n",
    "    feature_names_filename = f\"{base_filename}_feature_names.json\" \n",
    "\n",
    "    try:\n",
    "        joblib.dump(scaler, TRAINED_MODELS_OUTPUT_DIR / scaler_filename)\n",
    "        logger.info(f\"Scaler saved to {TRAINED_MODELS_OUTPUT_DIR / scaler_filename}\")\n",
    "        \n",
    "        joblib.dump(logreg_model, TRAINED_MODELS_OUTPUT_DIR / model_filename)\n",
    "        logger.info(f\"Model saved to {TRAINED_MODELS_OUTPUT_DIR / model_filename}\")\n",
    "\n",
    "        if 'MODEL_FEATURE_NAMES' in locals() and MODEL_FEATURE_NAMES: # MODEL_FEATURE_NAMES is now the shorter list\n",
    "            with open(TRAINED_MODELS_OUTPUT_DIR / feature_names_filename, 'w') as f:\n",
    "                json.dump(MODEL_FEATURE_NAMES, f)\n",
    "            logger.info(f\"Feature names used for model saved to {TRAINED_MODELS_OUTPUT_DIR / feature_names_filename}\")\n",
    "        else:\n",
    "            logger.warning(\"MODEL_FEATURE_NAMES not defined or empty. Not saving feature names list.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving model or scaler: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Evaluation\n",
    "\n",
    "model_ready = 'logreg_model' in locals()\n",
    "test_data_available = ('X_test_scaled' in locals() and isinstance(X_test_scaled, np.ndarray) and X_test_scaled.size > 0 and \\\n",
    "                       'y_test' in locals() and not y_test.empty and \\\n",
    "                       'test_identifiers_df' in locals() and not test_identifiers_df.empty)\n",
    "\n",
    "\n",
    "if not model_ready or not test_data_available:\n",
    "    logger.error(f\"Model ({MODEL_VERSION_SUFFIX}) not trained or test data not available/empty. Cannot evaluate.\")\n",
    "else:\n",
    "    logger.info(f\"Evaluating model ({MODEL_VERSION_SUFFIX} version) on the test set (per-minute decision points)...\")\n",
    "    \n",
    "    y_pred_test = logreg_model.predict(X_test_scaled)\n",
    "    y_pred_proba_test = logreg_model.predict_proba(X_test_scaled)[:, 1] \n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "        logger.info(f\"Test Set ROC AUC Score: {roc_auc:.4f}\")\n",
    "    except ValueError as e:\n",
    "        logger.warning(f\"Could not calculate ROC AUC: {e}. This can happen if only one class is present in y_test.\")\n",
    "        roc_auc = np.nan \n",
    "    \n",
    "    logger.info(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    logger.info(\"\\nTest Set Classification Report:\\n\" + classification_report(y_test, y_pred_test, zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred No', 'Pred Yes'], yticklabels=['Actual No', 'Actual Yes'])\n",
    "    plt.title(f'Test Set Confusion Matrix ({MODEL_VERSION_SUFFIX} Decisions)')\n",
    "    plt.ylabel('Actual Market Outcome')\n",
    "    plt.xlabel('Predicted Outcome at Decision Point')\n",
    "    plt.show()\n",
    "\n",
    "    test_predictions_df = test_identifiers_df.copy()\n",
    "    test_predictions_df['actual_target'] = y_test.values \n",
    "    test_predictions_df['predicted_target_logreg'] = y_pred_test\n",
    "    test_predictions_df['predicted_proba_yes_logreg'] = y_pred_proba_test\n",
    "    \n",
    "    if 'time_to_resolution_minutes' in X_test.columns:\n",
    "        test_predictions_df['time_to_resolution_at_pred'] = X_test['time_to_resolution_minutes'].values\n",
    "    \n",
    "    # *** MODIFIED: Prediction filename to reflect the new model version ***\n",
    "    base_pred_filename = f\"logreg_per_minute_{MODEL_VERSION_SUFFIX}\"\n",
    "    predictions_filename = f\"{base_pred_filename}_test_predictions_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    predictions_filepath = TRAINED_MODELS_OUTPUT_DIR / predictions_filename\n",
    "    test_predictions_df.to_csv(predictions_filepath, index=False)\n",
    "    logger.info(f\"Test set (per-minute, {MODEL_VERSION_SUFFIX} model) predictions saved to: {predictions_filepath}\")\n",
    "    print(f\"Head of test predictions for {MODEL_VERSION_SUFFIX} model:\")\n",
    "    display(test_predictions_df.head())\n",
    "\n",
    "logger.info(f\"Cell 6: Model evaluation complete ({MODEL_VERSION_SUFFIX} version).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
