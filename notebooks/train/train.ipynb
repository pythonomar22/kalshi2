{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Load Data for Classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "import logging\n",
    "import json # For saving feature_columns_list\n",
    "import joblib # For saving the model and scaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split # We'll do a chronological split manually\n",
    "# RandomForest will be imported in Cell 3\n",
    "from sklearn.preprocessing import StandardScaler # For feature scaling\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, confusion_matrix \n",
    "\n",
    "# --- Logging Setup ---\n",
    "logger_name = f\"model_training_classifier_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\" \n",
    "logger = logging.getLogger(logger_name)\n",
    "if not logger.handlers: \n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration ---\n",
    "current_notebook_dir = Path.cwd() # Assumes notebook is in notebooks/train/\n",
    "\n",
    "# Correctly point to where feature_engineering.ipynb saved the CSV: project_root/notebooks/features/\n",
    "# If train.ipynb is in project_root/notebooks/train/\n",
    "# then current_notebook_dir.parent is project_root/notebooks/\n",
    "FEATURES_DIR = current_notebook_dir.parent / \"features\" \n",
    "\n",
    "logger.info(f\"Attempting to find feature files in: {FEATURES_DIR.resolve()}\")\n",
    "\n",
    "try:\n",
    "    if not FEATURES_DIR.exists():\n",
    "        # Fallback if FEATURES_DIR was not found as expected (e.g. if notebook structure changed)\n",
    "        alt_features_dir = Path.cwd().parent.parent / \"features\" # Assuming project_root/features\n",
    "        if alt_features_dir.exists():\n",
    "            FEATURES_DIR = alt_features_dir\n",
    "            logger.info(f\"Primary FEATURES_DIR not found, using alternative: {FEATURES_DIR.resolve()}\")\n",
    "        else:\n",
    "            # Last attempt, perhaps features are in the same dir as the 'train' notebook's parent ('notebooks/')\n",
    "            # This would match if feature_engineering.ipynb was run from ./notebooks and saved there.\n",
    "            # However, your log shows it saved to project_root/notebooks/features/\n",
    "            # So the first FEATURES_DIR should be correct.\n",
    "            # This is more of a safety net.\n",
    "            # raise FileNotFoundError(f\"The directory {FEATURES_DIR.resolve()} (and {alt_features_dir.resolve()}) does not exist. Please check the path.\")\n",
    "            # For your specific case, the path should be:\n",
    "            # /Users/omarabul-hassan/Desktop/projects/kalshi/notebooks/features/\n",
    "            # If train.ipynb is at /Users/omarabul-hassan/Desktop/projects/kalshi/notebooks/train/\n",
    "            # current_notebook_dir.parent = /Users/omarabul-hassan/Desktop/projects/kalshi/notebooks/\n",
    "            # FEATURES_DIR = current_notebook_dir.parent / \"features\" is correct.\n",
    "            pass # Let the error be caught by glob if FEATURES_DIR is truly wrong.\n",
    "\n",
    "\n",
    "    # *** MODIFIED: Update glob pattern for v2_filtered_15m files ***\n",
    "    # The filename includes the offset, e.g., \"_15m_\"\n",
    "    # We can make the glob more general if the offset might change, or specific if we know it.\n",
    "    # For now, let's be specific to ensure we pick up the 15m offset files.\n",
    "    # If you use other offsets like 20m, 25m, you'll need to adjust this or use a broader pattern like \"v2_filtered_*m_\"\n",
    "    feature_glob_pattern = \"kalshi_btc_features_target_v2_filtered_15m_*.csv\"\n",
    "    feature_files = sorted(FEATURES_DIR.glob(feature_glob_pattern), key=os.path.getctime, reverse=True)\n",
    "    \n",
    "    if not feature_files:\n",
    "        logger.warning(f\"No '{feature_glob_pattern}' files found in {FEATURES_DIR.resolve()}. \"\n",
    "                       f\"Falling back to 'kalshi_btc_features_target_v1_filtered_*.csv'...\")\n",
    "        feature_glob_pattern_v1_filt = \"kalshi_btc_features_target_v1_filtered_*.csv\"\n",
    "        feature_files = sorted(FEATURES_DIR.glob(feature_glob_pattern_v1_filt), key=os.path.getctime, reverse=True)\n",
    "        if not feature_files:\n",
    "            raise FileNotFoundError(f\"No feature CSV files found in {FEATURES_DIR.resolve()} matching EITHER \"\n",
    "                                    f\"'{feature_glob_pattern}' OR '{feature_glob_pattern_v1_filt}'\")\n",
    "    \n",
    "    FEATURES_CSV_PATH = feature_files[0]\n",
    "    logger.info(f\"Using features CSV: {FEATURES_CSV_PATH.resolve()}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.critical(str(e))\n",
    "    FEATURES_CSV_PATH = None\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error finding features CSV: {e}\", exc_info=True)\n",
    "    FEATURES_CSV_PATH = None\n",
    "\n",
    "# Output directory for trained classifier models\n",
    "MODEL_OUTPUT_DIR = current_notebook_dir.parent / \"trained_models/rf\" \n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Trained classifier models will be saved in: {MODEL_OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "# --- Load the Features DataFrame ---\n",
    "df_model_data = pd.DataFrame()\n",
    "\n",
    "if FEATURES_CSV_PATH and FEATURES_CSV_PATH.exists():\n",
    "    try:\n",
    "        df_model_data = pd.read_csv(FEATURES_CSV_PATH)\n",
    "        logger.info(f\"Successfully loaded features data from: {FEATURES_CSV_PATH.resolve()}\")\n",
    "        logger.info(f\"Shape of loaded data: {df_model_data.shape}\")\n",
    "        \n",
    "        print(\"--- Data Head (Raw from CSV) ---\")\n",
    "        with pd.option_context('display.max_columns', None): # Show all columns for head\n",
    "            print(df_model_data.head())\n",
    "        print(\"\\n--- Data Info (Raw from CSV) ---\")\n",
    "        df_model_data.info(verbose=True, show_counts=True) # More detailed info\n",
    "        # print(\"\\n--- Data Description (Numerical, Raw from CSV) ---\")\n",
    "        # print(df_model_data.describe().to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error loading features CSV {FEATURES_CSV_PATH.resolve()}: {e}\")\n",
    "else:\n",
    "    if FEATURES_CSV_PATH:\n",
    "         logger.critical(f\"Features CSV file not found at the specified path: {FEATURES_CSV_PATH.resolve()}\")\n",
    "    else:\n",
    "         logger.critical(\"FEATURES_CSV_PATH was not set (likely due to an error finding the file). Cannot load data.\")\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.warning(\"DataFrame df_model_data is empty. Subsequent cells might fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preprocessing, Target Transformation, Feature Selection, and Splitting\n",
    "\n",
    "if df_model_data.empty:\n",
    "    logger.error(\"df_model_data is empty. Cannot proceed with preprocessing and splitting. Please ensure Cell 1 ran correctly and loaded data.\")\n",
    "else:\n",
    "    logger.info(f\"Starting preprocessing for df_model_data with shape: {df_model_data.shape}\")\n",
    "\n",
    "    # --- 1. Ensure Chronological Order ---\n",
    "    df_model_data.sort_values(by='decision_point_ts_utc', inplace=True)\n",
    "    df_model_data.reset_index(drop=True, inplace=True)\n",
    "    logger.info(\"Data sorted by 'decision_point_ts_utc'.\")\n",
    "\n",
    "    # --- 2. Define NEW Target Variable for Classification ---\n",
    "    original_target_col = 'TARGET_btc_diff_from_strike'\n",
    "    classification_target_col = 'TARGET_market_resolves_yes' \n",
    "\n",
    "    if original_target_col not in df_model_data.columns:\n",
    "        logger.critical(f\"Original target column '{original_target_col}' not found. Cannot create classification target.\")\n",
    "        raise ValueError(f\"Missing required column: {original_target_col}\")\n",
    "    \n",
    "    df_model_data[classification_target_col] = (df_model_data[original_target_col] > 0).astype(int)\n",
    "    logger.info(f\"Created binary classification target '{classification_target_col}'.\")\n",
    "    logger.info(f\"Value counts for '{classification_target_col}':\\n{df_model_data[classification_target_col].value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "    # --- 3. Handle Missing Values (NaNs) in Features ---\n",
    "    identifier_cols = ['kalshi_market_ticker', 'decision_point_ts_utc', 'kalshi_strike_price']\n",
    "    # This feature_columns list will now contain all new and old features\n",
    "    feature_columns = [\n",
    "        col for col in df_model_data.columns \n",
    "        if col not in identifier_cols + [original_target_col, classification_target_col]\n",
    "    ]\n",
    "    logger.info(f\"Full feature columns list ({len(feature_columns)}):\")\n",
    "    for i in range(0, len(feature_columns), 5): # Print in chunks for readability\n",
    "        logger.info(f\"  {feature_columns[i:i+5]}\")\n",
    "\n",
    "\n",
    "    nan_summary_before = df_model_data[feature_columns].isnull().sum()\n",
    "    nan_summary_before = nan_summary_before[nan_summary_before > 0].sort_values(ascending=False)\n",
    "    if not nan_summary_before.empty:\n",
    "        logger.warning(f\"NaN values found in feature columns BEFORE IMPUTATION:\\n{nan_summary_before}\")\n",
    "    else:\n",
    "        logger.info(\"No NaNs found in feature columns before imputation.\")\n",
    "\n",
    "    # --- Imputation Strategy (as implemented in your previous version) ---\n",
    "    for col in feature_columns:\n",
    "        if df_model_data[col].isnull().any(): \n",
    "            if 'btc_mom' in col or 'kalshi_mid_chg' in col: df_model_data[col].fillna(0, inplace=True)\n",
    "            elif 'btc_vol' in col or 'kalshi_mid_vol' in col: df_model_data[col].fillna(df_model_data[col].median(), inplace=True)\n",
    "            elif 'btc_sma' in col or 'btc_ema' in col: df_model_data[col].fillna(df_model_data[col].median(), inplace=True)\n",
    "            elif 'btc_price_vs_sma' in col or 'btc_price_vs_ema' in col: df_model_data[col].fillna(1.0, inplace=True)\n",
    "            elif 'btc_rsi' in col: df_model_data[col].fillna(50.0, inplace=True)\n",
    "            elif 'btc_atr' in col: df_model_data[col].fillna(df_model_data[col].median(), inplace=True)\n",
    "            elif col == 'distance_to_strike_norm_atr': df_model_data[col].fillna(0, inplace=True)\n",
    "            elif col == 'kalshi_vs_btc_implied_spread': df_model_data[col].fillna(0, inplace=True)\n",
    "            elif col == 'kalshi_yes_bid': df_model_data[col].fillna(0, inplace=True)\n",
    "            elif col == 'kalshi_yes_ask': df_model_data[col].fillna(100, inplace=True)\n",
    "            elif col == 'kalshi_spread': \n",
    "                df_model_data[col] = df_model_data['kalshi_yes_ask'].fillna(100) - df_model_data['kalshi_yes_bid'].fillna(0)\n",
    "                df_model_data[col].fillna(100.0, inplace=True) \n",
    "            elif col == 'kalshi_mid_price': \n",
    "                df_model_data[col] = (df_model_data['kalshi_yes_bid'].fillna(0) + df_model_data['kalshi_yes_ask'].fillna(100)) / 2\n",
    "                df_model_data[col].fillna(50.0, inplace=True)\n",
    "            elif 'kalshi_volume_t_minus_1' in col or 'kalshi_open_interest_t_minus_1' in col: df_model_data[col].fillna(0, inplace=True)\n",
    "            else: \n",
    "                if df_model_data[col].isnull().any(): \n",
    "                    logger.warning(f\"Feature '{col}' has NaNs, using general fillna(0).\")\n",
    "                    df_model_data[col].fillna(0, inplace=True)\n",
    "\n",
    "    if 'kalshi_yes_bid' in feature_columns and 'kalshi_yes_ask' in feature_columns:\n",
    "        if 'kalshi_spread' in feature_columns:\n",
    "            df_model_data['kalshi_spread'] = df_model_data['kalshi_yes_ask'] - df_model_data['kalshi_yes_bid']\n",
    "        if 'kalshi_mid_price' in feature_columns:\n",
    "            df_model_data['kalshi_mid_price'] = (df_model_data['kalshi_yes_bid'] + df_model_data['kalshi_yes_ask']) / 2\n",
    "    \n",
    "    nan_summary_after = df_model_data[feature_columns].isnull().sum()\n",
    "    nan_summary_after = nan_summary_after[nan_summary_after > 0]\n",
    "    if not nan_summary_after.empty:\n",
    "        logger.error(f\"STILL HAVE NaNs after targeted imputation! Columns:\\n{nan_summary_after}\")\n",
    "        logger.info(\"Proceeding with dropna for remaining NaNs in features.\")\n",
    "        df_model_data.dropna(subset=feature_columns, inplace=True) # Drop rows if any NaNs persist\n",
    "    else:\n",
    "        logger.info(\"Successfully handled/imputed NaNs in feature columns.\")\n",
    "        \n",
    "    original_row_count = len(df_model_data)\n",
    "    df_model_data.dropna(subset=[classification_target_col], inplace=True)\n",
    "    # Final check: ensure no NaNs in the feature columns that will be used for X\n",
    "    df_model_data.dropna(subset=feature_columns, inplace=True)\n",
    "    logger.info(f\"Dropped {original_row_count - len(df_model_data)} rows due to any remaining NaNs in features or target.\")\n",
    "        \n",
    "    if not df_model_data.empty:\n",
    "        X = df_model_data[feature_columns].copy()\n",
    "        y = df_model_data[classification_target_col].copy()\n",
    "        logger.info(f\"Defined X (features) with shape: {X.shape}\") # This should now be (N, 40)\n",
    "        logger.info(f\"Defined y (binary target) with shape: {y.shape}\")\n",
    "        logger.info(f\"Target y value counts:\\n{y.value_counts(normalize=True)}\")\n",
    "\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(X) * split_ratio)\n",
    "        X_train = X.iloc[:split_index]; y_train = y.iloc[:split_index] \n",
    "        X_test = X.iloc[split_index:]; y_test = y.iloc[split_index:]   \n",
    "\n",
    "        logger.info(f\"Data split chronologically:\")\n",
    "        logger.info(f\"  X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        logger.info(f\"  X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        if not X_train.empty and not X_test.empty :\n",
    "            train_start_ts = df_model_data['decision_point_ts_utc'].iloc[X_train.index[0]]\n",
    "            train_end_ts = df_model_data['decision_point_ts_utc'].iloc[X_train.index[-1]]\n",
    "            test_start_ts = df_model_data['decision_point_ts_utc'].iloc[X_test.index[0]]\n",
    "            test_end_ts = df_model_data['decision_point_ts_utc'].iloc[X_test.index[-1]]\n",
    "            logger.info(f\"  Training data from: {dt.datetime.fromtimestamp(train_start_ts, tz=timezone.utc).isoformat()} to {dt.datetime.fromtimestamp(train_end_ts, tz=timezone.utc).isoformat()}\")\n",
    "            logger.info(f\"  Test data from:     {dt.datetime.fromtimestamp(test_start_ts, tz=timezone.utc).isoformat()} to {dt.datetime.fromtimestamp(test_end_ts, tz=timezone.utc).isoformat()}\")\n",
    "        else: logger.warning(\"Train or Test set is empty after split.\")\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train) # Scaler is fit on the 40 features\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        logger.info(\"Features scaled using StandardScaler.\")\n",
    "        \n",
    "        # *** MODIFIED: Save scaler and feature list with _v2 suffix ***\n",
    "        scaler_path_v2 = MODEL_OUTPUT_DIR / \"feature_scaler_classifier_v2.joblib\"\n",
    "        joblib.dump(scaler, scaler_path_v2)\n",
    "        logger.info(f\"Scaler for V2 features saved to: {scaler_path_v2}\")\n",
    "        \n",
    "        feature_columns_list_path_v2 = MODEL_OUTPUT_DIR / \"feature_columns_classifier_v2.json\"\n",
    "        with open(feature_columns_list_path_v2, 'w') as f:\n",
    "            json.dump(feature_columns, f) # Save the full list of 40 feature names\n",
    "        logger.info(f\"List of V2 feature columns ({len(feature_columns)} features) saved to: {feature_columns_list_path_v2}\")\n",
    "\n",
    "    else:\n",
    "        logger.error(\"df_model_data is empty after NaN handling. Cannot proceed.\")\n",
    "        X, y, X_train, y_train, X_test, y_test, X_train_scaled_df, X_test_scaled_df = [pd.DataFrame()]*8 \n",
    "        scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Classification Model Training and Evaluation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV # For probability calibration\n",
    "\n",
    "if 'X_train_scaled_df' not in globals() or X_train_scaled_df.empty:\n",
    "    logger.error(\"Scaled training data (X_train_scaled_df) not found or is empty. Please ensure Cell 2 ran successfully.\")\n",
    "else:\n",
    "    logger.info(\"--- Starting Classification Model Training (Calibrated Random Forest) ---\")\n",
    "\n",
    "    # --- 1. Initialize Base RandomForestClassifier Model ---\n",
    "    # Consider adjusting max_depth or min_samples_leaf/split if AUC was still too high\n",
    "    # on the 15m filtered data, to make the base model slightly less complex.\n",
    "    base_rf_model = RandomForestClassifier(\n",
    "        n_estimators=200,        \n",
    "        max_depth=12,             # Example: Reduced max_depth slightly\n",
    "        min_samples_split=20,     # Example: Increased min_samples_split\n",
    "        min_samples_leaf=10,      # Example: Increased min_samples_leaf\n",
    "        class_weight='balanced_subsample', \n",
    "        random_state=42,\n",
    "        n_jobs=-1,                \n",
    "        oob_score=True            # OOB score for the base model\n",
    "    )\n",
    "    \n",
    "    model_name = \"CalibratedRandomForest_classifier_v2\" # Updated model name for artifacts\n",
    "    \n",
    "    # Optionally, fit the base model to check its OOB score before calibration\n",
    "    logger.info(f\"Fitting base RandomForest model to check OOB score on {X_train_scaled_df.shape[0]} samples...\")\n",
    "    if 'y_train' not in globals() or y_train.empty:\n",
    "        logger.error(\"y_train (binary target) is not available. Cannot train base model.\")\n",
    "    else:\n",
    "        try:\n",
    "            base_rf_model.fit(X_train_scaled_df, y_train) # Fit on scaled data\n",
    "            logger.info(f\"Base RandomForest model training complete for OOB check.\")\n",
    "            if hasattr(base_rf_model, 'oob_score_') and base_rf_model.oob_score_: # Check if oob_score was calculated\n",
    "                 logger.info(f\"  Base Model Out-of-Bag (OOB) Score: {base_rf_model.oob_score_:.4f}\")\n",
    "            else:\n",
    "                logger.warning(\"  Base Model OOB Score not available (oob_score=True might not have run or failed).\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fitting base RF model for OOB check: {e}\", exc_info=True)\n",
    "    \n",
    "    # --- 2. Initialize and Train CalibratedClassifierCV ---\n",
    "    # 'sigmoid' is generally more robust than 'isotonic', especially if the probability curve isn't strictly monotonic.\n",
    "    # cv=3 or cv=5 is common.\n",
    "    logger.info(f\"Training {model_name} (Calibrated RF with sigmoid) on {X_train_scaled_df.shape[0]} samples...\")\n",
    "    # The CalibratedClassifierCV will re-fit the base_rf_model internally during its cross-validation.\n",
    "    # So, the base_rf_model instance above is primarily for checking OOB if desired.\n",
    "    # For the final calibrated model, a new base estimator is effectively used by CalibratedClassifierCV.\n",
    "    # To pass the same params, we can re-instantiate or pass the fitted one (it gets cloned).\n",
    "    # It's often cleaner to pass an unfitted estimator to CalibratedClassifierCV.\n",
    "    calibrated_classifier_model = CalibratedClassifierCV(\n",
    "        RandomForestClassifier( # Pass a new instance with the same params\n",
    "            n_estimators=200, max_depth=12, min_samples_split=20, min_samples_leaf=10,\n",
    "            class_weight='balanced_subsample', random_state=42, n_jobs=-1, oob_score=False # OOB not used by CalibratedCV directly\n",
    "        ),\n",
    "        method='sigmoid', \n",
    "        cv=3 # Number of folds for calibration\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Model parameters for CalibratedClassifierCV: {calibrated_classifier_model.get_params()}\")\n",
    "    \n",
    "    if 'y_train' not in globals() or y_train.empty:\n",
    "        logger.error(\"y_train (binary target) is not available. Cannot train calibrated model.\")\n",
    "    else:\n",
    "        try:\n",
    "            calibrated_classifier_model.fit(X_train_scaled_df, y_train) # Fit on scaled data\n",
    "            logger.info(f\"{model_name} model training complete.\")\n",
    "            # CalibratedClassifierCV itself does not have an oob_score_ attribute.\n",
    "\n",
    "            # --- 3. Make Predictions on the Test Set ---\n",
    "            logger.info(f\"Making predictions with {model_name} on the test set ({X_test_scaled_df.shape[0]} samples)...\")\n",
    "            y_pred_test_class = calibrated_classifier_model.predict(X_test_scaled_df)\n",
    "            y_pred_test_proba = calibrated_classifier_model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "\n",
    "            # --- 4. Evaluate Model Performance (Classification Metrics) ---\n",
    "            # (This section remains largely the same, just uses calibrated_classifier_model and model_name)\n",
    "            if 'y_test' not in globals() or y_test.empty:\n",
    "                logger.error(\"y_test (binary target) is not available. Cannot evaluate model.\")\n",
    "            else:\n",
    "                accuracy = accuracy_score(y_test, y_pred_test_class)\n",
    "                precision = precision_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                recall = recall_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                f1 = f1_score(y_test, y_pred_test_class, zero_division=0)\n",
    "                try: roc_auc = roc_auc_score(y_test, y_pred_test_proba)\n",
    "                except ValueError as e: logger.warning(f\"Could not calculate ROC AUC: {e}\"); roc_auc = np.nan\n",
    "                logloss = log_loss(y_test, y_pred_test_proba)\n",
    "\n",
    "                logger.info(f\"\\n--- {model_name} Evaluation Metrics (Test Set) ---\")\n",
    "                logger.info(f\"  Accuracy:          {accuracy:.4f}\")\n",
    "                logger.info(f\"  Precision:         {precision:.4f}\")\n",
    "                logger.info(f\"  Recall (TPR):      {recall:.4f}\")\n",
    "                logger.info(f\"  F1-Score:          {f1:.4f}\")\n",
    "                logger.info(f\"  ROC AUC:           {roc_auc:.4f}\")\n",
    "                logger.info(f\"  Log Loss:          {logloss:.4f}\") # Log loss is important for calibrated probabilities\n",
    "\n",
    "                # Plot histogram of probabilities\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure(figsize=(10,6))\n",
    "                plt.hist(y_pred_test_proba, bins=50, alpha=0.7, label=f'P(YES) - {model_name}')\n",
    "                plt.title(f'Histogram of Predicted Probabilities (P(YES)) on Test Set - {model_name}')\n",
    "                plt.xlabel('Predicted Probability of YES')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                logger.info(f\"Description of P(YES) for {model_name} on test set:\\n{pd.Series(y_pred_test_proba).describe().to_string()}\")\n",
    "\n",
    "\n",
    "                logger.info(f\"\\n--- Confusion Matrix (Test Set) - {model_name} ---\")\n",
    "                cm = confusion_matrix(y_test, y_pred_test_class)\n",
    "                logger.info(f\"\\n{cm}\")\n",
    "                try:\n",
    "                    tn, fp, fn, tp = cm.ravel()\n",
    "                    logger.info(f\"  True Negatives (TN):  {tn}\")\n",
    "                    logger.info(f\"  False Positives (FP): {fp} (Type I Error)\")\n",
    "                    logger.info(f\"  False Negatives (FN): {fn} (Type II Error)\")\n",
    "                    logger.info(f\"  True Positives (TP):  {tp}\")\n",
    "                except ValueError: logger.warning(\"Could not unpack full confusion matrix.\")\n",
    "\n",
    "                df_results_class = pd.DataFrame({\n",
    "                    'actual_target_resolves_yes': y_test,\n",
    "                    'predicted_class_resolves_yes': y_pred_test_class,\n",
    "                    'predicted_proba_resolves_yes': y_pred_test_proba\n",
    "                })\n",
    "                # ... (rest of df_results_class population and printing as before) ...\n",
    "                if 'original_target_col' in globals() and original_target_col in df_model_data.columns and not X_test.empty : # Check if X_test has valid indices\n",
    "                    df_results_class['original_target_diff'] = df_model_data.loc[X_test.index, original_target_col].values\n",
    "                if 'kalshi_market_ticker' in df_model_data.columns and not X_test.empty:\n",
    "                    df_results_class['kalshi_market_ticker'] = df_model_data.loc[X_test.index, 'kalshi_market_ticker'].values\n",
    "                if 'decision_point_ts_utc' in df_model_data.columns and not X_test.empty:\n",
    "                    df_results_class['decision_point_ts_utc'] = df_model_data.loc[X_test.index, 'decision_point_ts_utc'].values\n",
    "                \n",
    "                print(f\"\\n--- Sample of Test Set Predictions vs Actuals ({model_name}) ---\")\n",
    "                print(df_results_class.head(10).to_string())\n",
    "\n",
    "                # --- Detailed Analysis by Actual Outcome (Test Set) ---\n",
    "                # ... (This detailed analysis section remains the same, just ensure it uses df_results_class correctly) ...\n",
    "\n",
    "\n",
    "                # --- 5. Inspect Feature Importances (from the BASE RandomForest model) ---\n",
    "                # CalibratedClassifierCV wraps the model, importances are from the underlying estimators.\n",
    "                # We can access the first calibrated estimator's base model for this.\n",
    "                logger.info(f\"\\n\\n--- Feature Importances (from base model of {model_name}) ---\")\n",
    "                # Get the base estimator from the first calibrated classifier\n",
    "                # Note: CalibratedClassifierCV fits multiple base models if cv > 1. \n",
    "                # We'll look at the importances from the first one as representative.\n",
    "                # Or, fit base_rf_model separately on full X_train_scaled_df to get \"overall\" importances.\n",
    "                # For simplicity, let's use the one already fitted for OOB check (base_rf_model)\n",
    "                \n",
    "                if hasattr(base_rf_model, 'feature_importances_'):\n",
    "                    # feature_columns should be defined from Cell 2\n",
    "                    if 'feature_columns' not in globals() or not feature_columns: \n",
    "                        logger.warning(\"feature_columns list not found or empty. Cannot display importance names.\")\n",
    "                        # Fallback: Try to load from JSON if path exists\n",
    "                        feature_columns_list_path = MODEL_OUTPUT_DIR / \"feature_columns_classifier_v1.json\"\n",
    "                        if feature_columns_list_path.exists():\n",
    "                            with open(feature_columns_list_path, 'r') as f: feature_columns = json.load(f)\n",
    "                            logger.info(f\"Loaded feature_columns from {feature_columns_list_path} for importances.\")\n",
    "                        else:\n",
    "                            feature_columns = [f\"feature_{i}\" for i in range(X_train_scaled_df.shape[1])]\n",
    "                    \n",
    "                    importances = pd.DataFrame({\n",
    "                        'feature': feature_columns, \n",
    "                        'importance': base_rf_model.feature_importances_\n",
    "                    })\n",
    "                    importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "                    \n",
    "                    print(\"\\nTop Feature Importances (from base RF model):\")\n",
    "                    print(importances.head(30).to_string()) # Print more features\n",
    "                else:\n",
    "                    logger.warning(f\"Could not retrieve feature importances from the base RF model for {model_name}.\")\n",
    "\n",
    "                # --- 6. Save the Trained CALIBRATED Model ---\n",
    "                # Use the new model_name for saving\n",
    "                model_path = MODEL_OUTPUT_DIR / f\"{model_name}.joblib\"\n",
    "                joblib.dump(calibrated_classifier_model, model_path)\n",
    "                logger.info(f\"Trained {model_name} model saved to: {model_path}\")\n",
    "\n",
    "                # Hyperparameters are for the CalibratedClassifierCV wrapper and its base estimator\n",
    "                model_hyperparams = calibrated_classifier_model.get_params(deep=True)\n",
    "                params_path = MODEL_OUTPUT_DIR / f\"{model_name}_hyperparams.json\"\n",
    "                with open(params_path, 'w') as f:\n",
    "                    serializable_params = {}\n",
    "                    for k, v in model_hyperparams.items():\n",
    "                        if isinstance(v, np.ndarray): serializable_params[k] = v.tolist()\n",
    "                        elif isinstance(v, (np.bool_, np.integer, np.floating)): serializable_params[k] = v.item()\n",
    "                        elif isinstance(v, (BaseException)): serializable_params[k] = str(v) # e.g. for estimator\n",
    "                        else: serializable_params[k] = v\n",
    "                    json.dump(serializable_params, f, indent=4, default=lambda o: '<not serializable>')\n",
    "\n",
    "                logger.info(f\"{model_name} hyperparameters saved to: {params_path}\")\n",
    "                \n",
    "                # The feature_scaler_classifier_v1.joblib and feature_columns_classifier_v1.json\n",
    "                # should still be named consistently if you want backtesters to find them easily.\n",
    "                # Or update backtester to look for v2 if you change their names.\n",
    "                # For now, let's assume they are overwritten with the latest versions.\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"An error occurred during {model_name} model training or evaluation: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
