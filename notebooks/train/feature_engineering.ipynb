{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logger = logging.getLogger(\"feature_engineering_per_minute\")\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO) \n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Base Directories ---\n",
    "BASE_PROJECT_DIR = Path(\"/Users/omarabul-hassan/Desktop/projects/kalshi\") \n",
    "NOTEBOOKS_DIR = BASE_PROJECT_DIR / \"notebooks\"\n",
    "DATA_DIR = NOTEBOOKS_DIR / \"data\"\n",
    "\n",
    "KALSHI_DATA_BASE_DIR = DATA_DIR / \"kalshi_data\"\n",
    "BINANCE_DATA_BASE_DIR = DATA_DIR / \"binance_data\"\n",
    "FEATURES_OUTPUT_DIR = NOTEBOOKS_DIR / \"features\"\n",
    "\n",
    "FEATURES_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Kalshi data expected at: {KALSHI_DATA_BASE_DIR}\")\n",
    "logger.info(f\"Binance data expected at: {BINANCE_DATA_BASE_DIR}\")\n",
    "logger.info(f\"Per-minute decision features will be saved to: {FEATURES_OUTPUT_DIR}\")\n",
    "\n",
    "# --- Constants ---\n",
    "MIN_MINUTES_BEFORE_RESOLUTION_FOR_DECISION = 1\n",
    "LAG_WINDOWS_MINUTES = [1, 3, 5, 10, 15, 30] \n",
    "ROLLING_WINDOWS_MINUTES = [5, 15, 30]\n",
    "\n",
    "# For detailed debugging of BTC price fetching for the first N original markets AND their first few decision points\n",
    "DEBUG_FIRST_N_ORIG_MARKETS = 1 # Set >0 for targeted debug logs\n",
    "DEBUG_FIRST_N_DECISION_POINTS_PER_MARKET = 3 # How many decision points to log for for the debugged markets\n",
    "debug_orig_market_count = 0 # Counter for markets being debugged\n",
    "\n",
    "logger.info(f\"Decision points will be generated up to T-{MIN_MINUTES_BEFORE_RESOLUTION_FOR_DECISION}m before market resolution.\")\n",
    "logger.info(f\"Using lag windows: {LAG_WINDOWS_MINUTES} minutes.\")\n",
    "logger.info(f\"Using rolling windows: {ROLLING_WINDOWS_MINUTES} minutes.\")\n",
    "logger.info(f\"Debug logging for first {DEBUG_FIRST_N_ORIG_MARKETS} markets, first {DEBUG_FIRST_N_DECISION_POINTS_PER_MARKET} decision points each.\")\n",
    "logger.info(\"Cell 1: Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Utility Functions (Data Loading & Parsing)\n",
    "\n",
    "_binance_day_data_cache = {}\n",
    "_kalshi_market_data_cache = {} \n",
    "\n",
    "def clear_all_caches():\n",
    "    global _binance_day_data_cache, _kalshi_market_data_cache, debug_orig_market_count # Use new counter\n",
    "    _binance_day_data_cache = {}\n",
    "    _kalshi_market_data_cache = {}\n",
    "    debug_orig_market_count = 0 # Reset this counter\n",
    "    logger.info(\"Cleared Binance, Kalshi caches and debug_orig_market_count.\")\n",
    "\n",
    "def load_binance_day_data(date_str_yyyy_mm_dd: str) -> pd.DataFrame | None:\n",
    "    global _binance_day_data_cache\n",
    "    # Return cache if available\n",
    "    if date_str_yyyy_mm_dd in _binance_day_data_cache:\n",
    "        return _binance_day_data_cache[date_str_yyyy_mm_dd]\n",
    "\n",
    "    filename_base = f\"BTCUSDT-1m-{date_str_yyyy_mm_dd}\"\n",
    "    filepath = BINANCE_DATA_BASE_DIR / f\"{filename_base}.csv\"\n",
    "    if not filepath.exists():\n",
    "        _binance_day_data_cache[date_str_yyyy_mm_dd] = None\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # --- 1. Read CSV ---\n",
    "        column_names = [\n",
    "            \"open_time_raw\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "            \"close_time_ms_raw\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "            \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    "        ]\n",
    "        df = pd.read_csv(filepath, header=None, names=column_names, low_memory=False)\n",
    "        if df.empty:\n",
    "            logger.warning(f\"[LOAD_BINANCE] File is empty: {filepath}\")\n",
    "            _binance_day_data_cache[date_str_yyyy_mm_dd] = None\n",
    "            return None\n",
    "\n",
    "        # --- 2. Detect units of open_time_raw and convert to seconds ---\n",
    "        first_raw = df[\"open_time_raw\"].iloc[0]\n",
    "        # MICROSECONDS (≈1e15), MILLISECONDS (≈1e12), or SECONDS (≈1e9)\n",
    "        if first_raw > 1e14:\n",
    "            logger.info(f\"[LOAD_BINANCE DEBUG {filepath.name}] Detected MICROSECONDS. Dividing by 1,000,000.\")\n",
    "            df[\"timestamp_s\"] = df[\"open_time_raw\"] // 1_000_000\n",
    "        elif 1e12 < first_raw <= 1e14:\n",
    "            logger.info(f\"[LOAD_BINANCE DEBUG {filepath.name}] Detected MILLISECONDS. Dividing by 1,000.\")\n",
    "            df[\"timestamp_s\"] = df[\"open_time_raw\"] // 1_000\n",
    "        elif 1e9 < first_raw <= 1e10:\n",
    "            logger.info(f\"[LOAD_BINANCE DEBUG {filepath.name}] Detected SECONDS. Using as is.\")\n",
    "            df[\"timestamp_s\"] = df[\"open_time_raw\"]\n",
    "        else:\n",
    "            logger.warning(f\"[LOAD_BINANCE WARNING {filepath.name}] Unusual timestamp magnitude: {first_raw}. Attempting to use as is.\")\n",
    "            df[\"timestamp_s\"] = df[\"open_time_raw\"]\n",
    "\n",
    "        # --- 3. Set index, sort, and coerce numerics ---\n",
    "        df.set_index(\"timestamp_s\", inplace=True)\n",
    "        if not df.index.is_monotonic_increasing:\n",
    "            df.sort_index(inplace=True)\n",
    "\n",
    "        for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # --- 4. Cache and return ---\n",
    "        _binance_day_data_cache[date_str_yyyy_mm_dd] = df\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[LOAD_BINANCE] Error loading Binance data from {filepath}: {e}\", exc_info=True)\n",
    "        _binance_day_data_cache[date_str_yyyy_mm_dd] = None\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_btc_kline_at_or_before_ts(target_timestamp_s: int, current_market_ticker_for_debug: str = None, decision_point_count_for_debug: int = 0) -> pd.Series | None:\n",
    "    global debug_orig_market_count \n",
    "    target_dt_utc = dt.datetime.fromtimestamp(target_timestamp_s, tz=timezone.utc); date_str_needed = target_dt_utc.strftime(\"%Y-%m-%d\")\n",
    "    perform_debug_logging = (DEBUG_FIRST_N_ORIG_MARKETS > 0 and current_market_ticker_for_debug is not None and \n",
    "                             debug_orig_market_count < DEBUG_FIRST_N_ORIG_MARKETS and\n",
    "                             decision_point_count_for_debug < DEBUG_FIRST_N_DECISION_POINTS_PER_MARKET)\n",
    "    if perform_debug_logging: logger.info(f\"[DEBUG BTC KLINE Market: {current_market_ticker_for_debug} (Overall #{debug_orig_market_count}), DecisionPt#{decision_point_count_for_debug}] Request kline for ts: {target_timestamp_s} ({target_dt_utc.isoformat()})\")\n",
    "    binance_df = load_binance_day_data(date_str_needed)\n",
    "    if binance_df is None or binance_df.empty:\n",
    "        if target_dt_utc.hour == 0 and target_dt_utc.minute < 5: \n",
    "            prev_date_dt_utc = target_dt_utc - timedelta(days=1); prev_date_str = prev_date_dt_utc.strftime(\"%Y-%m-%d\")\n",
    "            binance_df_prev = load_binance_day_data(prev_date_str)\n",
    "            if binance_df_prev is not None and not binance_df_prev.empty:\n",
    "                idx_pos_prev = binance_df_prev.index.searchsorted(target_timestamp_s, side='right')\n",
    "                if idx_pos_prev > 0:\n",
    "                    kline_data = binance_df_prev.iloc[idx_pos_prev - 1]\n",
    "                    if kline_data.name <= target_timestamp_s: return kline_data # Check against lookahead\n",
    "        return None\n",
    "    try:\n",
    "        idx_pos = binance_df.index.searchsorted(target_timestamp_s, side='right')\n",
    "        if idx_pos == 0: return None\n",
    "        kline_data = binance_df.iloc[idx_pos - 1]\n",
    "        if kline_data.name > target_timestamp_s: logger.error(f\"LOOKAHEAD (get_btc_kline)! Kline ts {kline_data.name} > target {target_timestamp_s}\"); return None \n",
    "        if perform_debug_logging: logger.info(f\"[DEBUG BTC KLINE Market: {current_market_ticker_for_debug}] Found kline ending ts {kline_data.name} for target {target_timestamp_s}\")\n",
    "        return kline_data\n",
    "    except Exception: return None\n",
    "\n",
    "def get_kalshi_candle_at_or_before_ts(market_df: pd.DataFrame, target_timestamp_s: int) -> pd.Series | None:\n",
    "    if market_df is None or market_df.empty: return None\n",
    "    try:\n",
    "        idx_pos = market_df.index.searchsorted(target_timestamp_s, side='right')\n",
    "        if idx_pos == 0: return None\n",
    "        candle_data = market_df.iloc[idx_pos - 1]\n",
    "        if candle_data.name > target_timestamp_s: logger.error(f\"LOOKAHEAD (get_kalshi_candle)! Candle ts {candle_data.name} > target {target_timestamp_s}\"); return None\n",
    "        if target_timestamp_s - candle_data.name > (3 * 60): return None \n",
    "        return candle_data\n",
    "    except Exception: return None\n",
    "\n",
    "def get_event_details_from_ticker(ticker_string: str | None) -> dict | None:\n",
    "    if not ticker_string: return None\n",
    "    m = re.match(r\"^(.*?)-(\\d{2}[A-Z]{3}\\d{2})(\\d{2})(?:-(T(\\d+\\.?\\d*)))?$\", ticker_string) or \\\n",
    "        re.match(r\"^(.*?)-(\\d{2}[A-Z]{3}\\d{2})(\\d{2})$\", ticker_string)\n",
    "    if not m: return None\n",
    "    g = m.groups(); strike = float(g[4]) if len(g) >=5 and g[4] else None\n",
    "    return {\"series\":g[0],\"date_str_yymmmdd\":g[1],\"hour_str_edt\":g[2],\"strike_price_from_ticker\":strike}\n",
    "\n",
    "def parse_iso_to_unix_timestamp(ds: str|None) -> int|None:\n",
    "    if not ds: return None\n",
    "    try:\n",
    "        dt_obj = dt.datetime.fromisoformat(ds.replace('Z','+00:00')) if ds.endswith('Z') else dt.datetime.fromisoformat(ds)\n",
    "        return int((dt_obj.replace(tzinfo=timezone.utc) if dt_obj.tzinfo is None else dt_obj).timestamp())\n",
    "    except Exception: return None\n",
    "\n",
    "def load_kalshi_market_data(market_ticker: str) -> pd.DataFrame | None:\n",
    "    global _kalshi_market_data_cache\n",
    "    if market_ticker in _kalshi_market_data_cache: return _kalshi_market_data_cache[market_ticker]\n",
    "    details = get_event_details_from_ticker(market_ticker)\n",
    "    if not details: return None\n",
    "    fp = KALSHI_DATA_BASE_DIR/details['date_str_yymmmdd']/(details['hour_str_edt'].zfill(2))/f\"{market_ticker}.csv\"\n",
    "    if not fp.exists(): return None\n",
    "    try:\n",
    "        df = pd.read_csv(fp, low_memory=False); _kalshi_market_data_cache[market_ticker] = df\n",
    "        if df.empty: return None\n",
    "        df['timestamp_s'] = pd.to_numeric(df['timestamp_s'], errors='coerce').astype('Int64'); df.dropna(subset=['timestamp_s'], inplace=True) \n",
    "        df.set_index('timestamp_s', inplace=True); \n",
    "        if not df.index.is_monotonic_increasing: df.sort_index(inplace=True)\n",
    "        cols = [c for c in df.columns if 'cents' in c]; df[cols] = df[cols].apply(pd.to_numeric, errors='coerce') / 100.0\n",
    "        for v_col in ['volume', 'open_interest']: \n",
    "            if v_col in df.columns: df[v_col] = pd.to_numeric(df[v_col], errors='coerce')\n",
    "        return df\n",
    "    except Exception as e: logger.error(f\"Err load Kalshi {fp}: {e}\", exc_info=True); return None\n",
    "\n",
    "logger.info(\"Cell 2: Utility functions defined/updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load NTM Outcomes Manifest\n",
    "# (Same as your previous version that successfully loaded 9192 markets)\n",
    "\n",
    "list_of_outcome_files = sorted(\n",
    "    glob.glob(str(KALSHI_DATA_BASE_DIR / \"kalshi_btc_hourly_NTM_filtered_market_outcomes_*.csv\")),\n",
    "    key=os.path.getctime,\n",
    "    reverse=True \n",
    ")\n",
    "ntm_outcomes_df = pd.DataFrame() \n",
    "if not list_of_outcome_files:\n",
    "    logger.critical(f\"CRITICAL: No NTM outcome CSV files found in {KALSHI_DATA_BASE_DIR}.\")\n",
    "else:\n",
    "    LATEST_NTM_OUTCOMES_CSV_PATH = Path(list_of_outcome_files[0])\n",
    "    logger.info(f\"Using NTM outcomes manifest from: {LATEST_NTM_OUTCOMES_CSV_PATH}\")\n",
    "    try:\n",
    "        ntm_outcomes_df = pd.read_csv(LATEST_NTM_OUTCOMES_CSV_PATH, low_memory=False)\n",
    "        logger.info(f\"Loaded NTM outcomes manifest with {len(ntm_outcomes_df)} markets initially.\")\n",
    "        required_cols = ['market_ticker', 'result', 'event_resolution_time_iso', 'kalshi_strike_price', 'market_open_time_iso', 'market_close_time_iso']\n",
    "        if any(col not in ntm_outcomes_df.columns for col in required_cols):\n",
    "            logger.critical(f\"NTM outcomes CSV is missing required columns.\"); ntm_outcomes_df = pd.DataFrame() \n",
    "        if not ntm_outcomes_df.empty:\n",
    "            ntm_outcomes_df['target'] = ntm_outcomes_df['result'].astype(str).str.upper().apply(lambda x: 1 if x=='YES' else (0 if x=='NO' else np.nan))\n",
    "            ntm_outcomes_df.dropna(subset=['target'], inplace=True)\n",
    "            if not ntm_outcomes_df.empty:\n",
    "                ntm_outcomes_df['target'] = ntm_outcomes_df['target'].astype(int)\n",
    "                for col, func_col_name in {'resolution_time_ts':'event_resolution_time_iso', 'market_open_ts':'market_open_time_iso', 'market_close_ts':'market_close_time_iso'}.items():\n",
    "                    ntm_outcomes_df[col] = ntm_outcomes_df[func_col_name].apply(parse_iso_to_unix_timestamp)\n",
    "                ntm_outcomes_df['kalshi_strike_price'] = pd.to_numeric(ntm_outcomes_df['kalshi_strike_price'], errors='coerce')\n",
    "                ntm_outcomes_df.dropna(subset=['market_ticker', 'resolution_time_ts', 'market_open_ts', 'market_close_ts', 'kalshi_strike_price', 'target'], how='any', inplace=True)\n",
    "                logger.info(f\"Processed NTM outcomes. {len(ntm_outcomes_df)} markets remain for feature engineering.\")\n",
    "                if not ntm_outcomes_df.empty: display(ntm_outcomes_df.head())\n",
    "    except Exception as e: logger.critical(f\"Error loading NTM outcomes CSV: {e}\", exc_info=True); ntm_outcomes_df = pd.DataFrame() \n",
    "if ntm_outcomes_df.empty: logger.warning(\"No NTM markets loaded. Feature engineering will not proceed.\")\n",
    "logger.info(\"Cell 3: NTM Outcomes Manifest loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Per-Minute Feature Engineering Loop (REFINED BTC STATS)\n",
    "\n",
    "all_decision_point_features_list = []\n",
    "\n",
    "if 'ntm_outcomes_df' not in locals() or ntm_outcomes_df.empty:\n",
    "    logger.warning(\"Skipping per-minute feature engineering: NTM outcomes manifest is empty.\")\n",
    "else:\n",
    "    logger.info(f\"Starting PER-MINUTE feature engineering for {len(ntm_outcomes_df)} NTM markets...\")\n",
    "    \n",
    "    clear_all_caches() \n",
    "\n",
    "    for index, ntm_market_row in tqdm(ntm_outcomes_df.iterrows(), total=ntm_outcomes_df.shape[0], desc=\"Processing NTM Markets\"):\n",
    "        market_ticker = ntm_market_row['market_ticker']\n",
    "        kalshi_strike_price = ntm_market_row['kalshi_strike_price']\n",
    "        resolution_time_ts = int(ntm_market_row['resolution_time_ts'])\n",
    "        market_open_ts = int(ntm_market_row['market_open_ts'])\n",
    "        target_outcome = ntm_market_row['target']\n",
    "\n",
    "        # Debug control for the outer loop (per NTM market)\n",
    "        is_market_being_debugged = (DEBUG_FIRST_N_ORIG_MARKETS > 0 and \n",
    "                                    debug_orig_market_count < DEBUG_FIRST_N_ORIG_MARKETS)\n",
    "        if is_market_being_debugged:\n",
    "            logger.info(f\"--- Debugging Market #{debug_orig_market_count}: {market_ticker} ---\")\n",
    "            \n",
    "        kalshi_market_df = load_kalshi_market_data(market_ticker)\n",
    "\n",
    "        first_possible_decision_ts = market_open_ts + 60 \n",
    "        last_possible_decision_ts = resolution_time_ts - (MIN_MINUTES_BEFORE_RESOLUTION_FOR_DECISION * 60)\n",
    "\n",
    "        if first_possible_decision_ts > last_possible_decision_ts:\n",
    "            if is_market_being_debugged: debug_orig_market_count += 1 # Count it as debugged even if skipped\n",
    "            continue\n",
    "        \n",
    "        decision_point_counter_for_this_market = 0 # For debugging first N decision points\n",
    "\n",
    "        for decision_minute_ts in range(first_possible_decision_ts, last_possible_decision_ts + 1, 60):\n",
    "            # Debug control for inner loop (per decision point of a debugged market)\n",
    "            should_log_this_decision_point = (is_market_being_debugged and \n",
    "                                              decision_point_counter_for_this_market < DEBUG_FIRST_N_DECISION_POINTS_PER_MARKET)\n",
    "\n",
    "            features = {'market_ticker': market_ticker, 'decision_timestamp_s': decision_minute_ts,\n",
    "                        'resolution_time_ts': resolution_time_ts, 'strike_price': kalshi_strike_price,\n",
    "                        'target': target_outcome,\n",
    "                        'time_to_resolution_minutes': round((resolution_time_ts - decision_minute_ts) / 60.0, 2)}\n",
    "\n",
    "            current_btc_kline = get_btc_kline_at_or_before_ts(decision_minute_ts, \n",
    "                                                              current_market_ticker_for_debug=market_ticker if is_market_being_debugged else None,\n",
    "                                                              decision_point_count_for_debug=decision_point_counter_for_this_market if is_market_being_debugged else -1)\n",
    "            \n",
    "            if current_btc_kline is not None and pd.notna(current_btc_kline['close']):\n",
    "                features['current_btc_price'] = float(current_btc_kline['close'])\n",
    "                features['current_dist_strike_abs'] = features['current_btc_price'] - kalshi_strike_price\n",
    "                features['current_dist_strike_pct'] = (features['current_dist_strike_abs'] / kalshi_strike_price) if kalshi_strike_price != 0 else np.nan\n",
    "                \n",
    "                # --- REFINED: Build BTC Price History for Lags/Rolling ---\n",
    "                # Determine the earliest timestamp needed for any stat based on current_btc_kline.name\n",
    "                max_lookback_seconds = (max(LAG_WINDOWS_MINUTES + ROLLING_WINDOWS_MINUTES) + 5) * 60 # Add buffer\n",
    "                history_needed_start_ts = current_btc_kline.name - max_lookback_seconds\n",
    "\n",
    "                # Efficiently gather historical klines up to current_btc_kline.name\n",
    "                # This part requires a helper or careful iteration if crossing many day boundaries.\n",
    "                # For simplicity, let's assume a helper function `get_btc_history_series` could do this.\n",
    "                # For now, we will adapt the previous multi-day loading logic.\n",
    "                \n",
    "                btc_price_series_for_stats = pd.Series(dtype=float)\n",
    "                relevant_day_dfs_data = []\n",
    "\n",
    "                # Iterate backwards from current_btc_kline's day until history_needed_start_ts is covered\n",
    "                # or we run out of data. Start with current kline's day.\n",
    "                current_eval_day_ts = current_btc_kline.name\n",
    "                num_days_to_check = (current_btc_kline.name - history_needed_start_ts) // (24*60*60) + 2 # Estimate days needed\n",
    "\n",
    "                for i in range(num_days_to_check):\n",
    "                    day_str_to_load = (dt.datetime.fromtimestamp(current_eval_day_ts, tz=timezone.utc) - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "                    daily_df = load_binance_day_data(day_str_to_load)\n",
    "                    if daily_df is not None and not daily_df.empty:\n",
    "                        # Slice relevant part of this day's data\n",
    "                        day_slice = daily_df.loc[\n",
    "                            (daily_df.index >= history_needed_start_ts) & # Data must be after needed start\n",
    "                            (daily_df.index <= current_btc_kline.name)   # And not after current kline\n",
    "                        ]['close']\n",
    "                        if not day_slice.empty:\n",
    "                            relevant_day_dfs_data.append(day_slice)\n",
    "                        # If the earliest data loaded already covers history_needed_start_ts, we can stop for this day's df\n",
    "                        if daily_df.index.min() <= history_needed_start_ts:\n",
    "                            break \n",
    "                    elif i == 0 and daily_df is None : # Current day data missing, something is wrong\n",
    "                        if should_log_this_decision_point: logger.warning(f\"  [DEBUG] Current day Binance data missing for {day_str_to_load}\")\n",
    "                        break # Can't get current price or history\n",
    "                \n",
    "                if relevant_day_dfs_data:\n",
    "                    btc_price_series_for_stats = pd.concat(relevant_day_dfs_data)\n",
    "                    if not btc_price_series_for_stats.empty:\n",
    "                        btc_price_series_for_stats = btc_price_series_for_stats[\n",
    "                            ~btc_price_series_for_stats.index.duplicated(keep='last')\n",
    "                        ].sort_index()\n",
    "                \n",
    "                if should_log_this_decision_point and (DEBUG_FIRST_N_ORIG_MARKETS > 0):\n",
    "                    logger.info(f\"  [DEBUG] BTC History for {market_ticker} @ decision {decision_minute_ts} (kline_ts {current_btc_kline.name}):\")\n",
    "                    logger.info(f\"  [DEBUG]   Needed from: {dt.datetime.fromtimestamp(history_needed_start_ts, tz=timezone.utc).isoformat()}\")\n",
    "                    logger.info(f\"  [DEBUG]   Series len: {len(btc_price_series_for_stats)}, min_ts: {dt.datetime.fromtimestamp(btc_price_series_for_stats.index.min(), tz=timezone.utc).isoformat() if not btc_price_series_for_stats.empty else 'N/A'}, max_ts: {dt.datetime.fromtimestamp(btc_price_series_for_stats.index.max(), tz=timezone.utc).isoformat() if not btc_price_series_for_stats.empty else 'N/A'}\")\n",
    "\n",
    "                if not btc_price_series_for_stats.empty:\n",
    "                    # Convert index to DatetimeIndex for asof\n",
    "                    temp_series_for_asof = pd.Series(btc_price_series_for_stats.values, \n",
    "                                                     index=pd.to_datetime(btc_price_series_for_stats.index, unit='s', utc=True))\n",
    "\n",
    "                    for lag in LAG_WINDOWS_MINUTES:\n",
    "                        # target_lag_ts is the exact point in the past we're looking for data at/before\n",
    "                        target_lag_ts = current_btc_kline.name - (lag * 60)\n",
    "                        past_price = temp_series_for_asof.asof(pd.Timestamp(target_lag_ts, unit='s', tz='utc'))\n",
    "                        \n",
    "                        if pd.notna(past_price) and pd.notna(features.get('current_btc_price')):\n",
    "                            features[f'btc_price_change_pct_{lag}m'] = (features['current_btc_price'] - past_price) / past_price if past_price != 0 else np.nan\n",
    "                        else:\n",
    "                            features[f'btc_price_change_pct_{lag}m'] = np.nan\n",
    "                        if should_log_this_decision_point and (DEBUG_FIRST_N_ORIG_MARKETS > 0): logger.info(f\"  [DEBUG]   Lag {lag}m: target_ts={dt.datetime.fromtimestamp(target_lag_ts, tz=timezone.utc).isoformat()}, past_price={past_price}, calc_pct={features[f'btc_price_change_pct_{lag}m']}\")\n",
    "                    \n",
    "                    # Rolling window calculations using the same series, index already DatetimeIndex\n",
    "                    for window in ROLLING_WINDOWS_MINUTES:\n",
    "                        # We need 'window' number of 1-minute klines.\n",
    "                        # The series `temp_series_for_asof` contains history up to `current_btc_kline.name`.\n",
    "                        # We want the rolling std of the last `window` points of this series.\n",
    "                        if len(temp_series_for_asof) >= window:\n",
    "                            # .std() will be calculated on the values of the last 'window' elements.\n",
    "                            std_val = temp_series_for_asof.iloc[-window:].std() \n",
    "                        elif len(temp_series_for_asof) >= 2: # Fallback: std of available points if fewer than window but at least 2\n",
    "                            std_val = temp_series_for_asof.std()\n",
    "                        else: # Not enough data for any std calculation\n",
    "                            std_val = np.nan\n",
    "                        features[f'btc_volatility_{window}m'] = std_val # Already NaN if std_val is NaN\n",
    "                        if should_log_this_decision_point and (DEBUG_FIRST_N_ORIG_MARKETS > 0): logger.info(f\"  [DEBUG]   Roll {window}m: std_val={std_val}, assigned_feature={features[f'btc_volatility_{window}m']}\")\n",
    "                else: \n",
    "                    if should_log_this_decision_point and (DEBUG_FIRST_N_ORIG_MARKETS > 0): logger.info(f\"  [DEBUG]   BTC price series FOR STATS was EMPTY for decision_ts {decision_minute_ts}.\")\n",
    "                    for lag in LAG_WINDOWS_MINUTES: features[f'btc_price_change_pct_{lag}m'] = np.nan\n",
    "                    for window in ROLLING_WINDOWS_MINUTES: features[f'btc_volatility_{window}m'] = np.nan\n",
    "            else: # current_btc_kline is None\n",
    "                features.update({f:np.nan for f in ['current_btc_price','current_dist_strike_abs','current_dist_strike_pct']})\n",
    "                for lag in LAG_WINDOWS_MINUTES: features[f'btc_price_change_pct_{lag}m'] = np.nan\n",
    "                for window in ROLLING_WINDOWS_MINUTES: features[f'btc_volatility_{window}m'] = np.nan\n",
    "\n",
    "            # --- Kalshi Market Features ---\n",
    "            # (This part remains the same as your previous version)\n",
    "            if kalshi_market_df is not None:\n",
    "                current_kalshi_candle = get_kalshi_candle_at_or_before_ts(kalshi_market_df, decision_minute_ts)\n",
    "                if current_kalshi_candle is not None:\n",
    "                    features['current_kalshi_yes_bid'] = current_kalshi_candle.get('yes_bid_close_cents', np.nan)\n",
    "                    features['current_kalshi_yes_ask'] = current_kalshi_candle.get('yes_ask_close_cents', np.nan)\n",
    "                    features['current_kalshi_volume'] = current_kalshi_candle.get('volume', np.nan)\n",
    "                    features['current_kalshi_oi'] = current_kalshi_candle.get('open_interest', np.nan)\n",
    "                    if pd.notna(features['current_kalshi_yes_bid']) and pd.notna(features['current_kalshi_yes_ask']):\n",
    "                        features['current_kalshi_mid_price']=(features['current_kalshi_yes_bid']+features['current_kalshi_yes_ask'])/2.0\n",
    "                        features['current_kalshi_spread_abs']=features['current_kalshi_yes_ask']-features['current_kalshi_yes_bid']\n",
    "                        features['current_kalshi_spread_pct']=(features['current_kalshi_spread_abs']/features['current_kalshi_mid_price']) if features['current_kalshi_mid_price']!=0 else np.nan\n",
    "                    else: features.update({f:np.nan for f in ['current_kalshi_mid_price','current_kalshi_spread_abs','current_kalshi_spread_pct']})\n",
    "                else: features.update({f:np.nan for f in ['current_kalshi_yes_bid','current_kalshi_yes_ask','current_kalshi_mid_price','current_kalshi_spread_abs','current_kalshi_spread_pct','current_kalshi_volume','current_kalshi_oi']})\n",
    "            else: features.update({f:np.nan for f in ['current_kalshi_yes_bid','current_kalshi_yes_ask','current_kalshi_mid_price','current_kalshi_spread_abs','current_kalshi_spread_pct','current_kalshi_volume','current_kalshi_oi']})\n",
    "            \n",
    "            all_decision_point_features_list.append(features)\n",
    "            if is_market_being_debugged: decision_point_counter_for_this_market +=1\n",
    "        \n",
    "        if is_market_being_debugged: \n",
    "            debug_orig_market_count += 1 # Increment after all decision points for this market\n",
    "\n",
    "    if all_decision_point_features_list:\n",
    "        output_features_df = pd.DataFrame(all_decision_point_features_list)\n",
    "        logger.info(f\"Successfully engineered features for {len(output_features_df)} (market, decision_minute) points.\")\n",
    "    else:\n",
    "        output_features_df = pd.DataFrame(); logger.warning(\"No (market, decision_minute) features generated.\")\n",
    "logger.info(\"Cell 4: Per-Minute Feature engineering loop complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save Features\n",
    "\n",
    "if 'output_features_df' in locals() and not output_features_df.empty:\n",
    "    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    features_filename = f\"kalshi_per_minute_decision_features_{timestamp_str}.csv\" \n",
    "    features_filepath = FEATURES_OUTPUT_DIR / features_filename\n",
    "    try:\n",
    "        output_features_df.to_csv(features_filepath, index=False)\n",
    "        logger.info(f\"Successfully saved per-minute decision features for {len(output_features_df)} points to: {features_filepath}\")\n",
    "        print(f\"Features saved to: {features_filepath}\")\n",
    "    except Exception as e: logger.error(f\"Error saving features: {e}\", exc_info=True)\n",
    "elif 'output_features_df' in locals() and output_features_df.empty: logger.warning(\"output_features_df empty. Nothing to save.\")\n",
    "else: logger.warning(\"output_features_df not defined. Nothing to save.\")\n",
    "logger.info(\"Cell 5: Feature saving process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Inspect Output CSV\n",
    "\n",
    "LATEST_PER_MINUTE_FEATURES_CSV_PATH = None\n",
    "if 'features_filepath' in locals() and Path(features_filepath).exists(): \n",
    "    LATEST_PER_MINUTE_FEATURES_CSV_PATH = features_filepath\n",
    "else: \n",
    "    list_of_feature_files = sorted(glob.glob(str(FEATURES_OUTPUT_DIR / \"kalshi_per_minute_decision_features_*.csv\")), key=os.path.getctime, reverse=True)\n",
    "    if list_of_feature_files: LATEST_PER_MINUTE_FEATURES_CSV_PATH = Path(list_of_feature_files[0])\n",
    "\n",
    "if LATEST_PER_MINUTE_FEATURES_CSV_PATH and LATEST_PER_MINUTE_FEATURES_CSV_PATH.exists():\n",
    "    logger.info(f\"Inspecting features from: {LATEST_PER_MINUTE_FEATURES_CSV_PATH}\")\n",
    "    df_inspect = pd.read_csv(LATEST_PER_MINUTE_FEATURES_CSV_PATH, nrows=10000) \n",
    "    logger.info(f\"Shape of loaded sample: {df_inspect.shape}\"); logger.info(\"\\nFirst 5 rows:\"); display(df_inspect.head())\n",
    "    logger.info(\"\\nBasic Info:\"); df_inspect.info()\n",
    "    logger.info(\"\\nNaN Percentage per column (for the loaded sample):\")\n",
    "    nan_summary_inspect = ((df_inspect.isnull().sum() / len(df_inspect)) * 100)[lambda x: x > 0].sort_values(ascending=False)\n",
    "    if not nan_summary_inspect.empty: print(nan_summary_inspect.to_string())\n",
    "    else: logger.info(\"No NaNs found in the loaded sample of feature columns.\")\n",
    "    if 'time_to_resolution_minutes' in df_inspect.columns:\n",
    "        logger.info(\"\\nValue counts for 'time_to_resolution_minutes' (sample):\")\n",
    "        display(df_inspect['time_to_resolution_minutes'].value_counts().sort_index().head(10))\n",
    "        display(df_inspect['time_to_resolution_minutes'].value_counts().sort_index().tail(10))\n",
    "    if 'market_ticker' in df_inspect.columns:\n",
    "        logger.info(\"\\nNumber of decision points per market (sample of first few markets):\")\n",
    "        display(df_inspect['market_ticker'].value_counts().head(10))\n",
    "    numeric_cols_to_describe = [c for c in ['strike_price', 'current_btc_price', 'current_dist_strike_pct', 'time_to_resolution_minutes', 'current_kalshi_mid_price', 'current_kalshi_spread_pct'] if c in df_inspect.columns]\n",
    "    if numeric_cols_to_describe: logger.info(\"\\nDescriptive statistics for key numeric features (sample):\"); display(df_inspect[numeric_cols_to_describe].describe())\n",
    "else: logger.warning(\"No per-minute features CSV file found for inspection.\")\n",
    "logger.info(\"Cell 6: Inspection complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
