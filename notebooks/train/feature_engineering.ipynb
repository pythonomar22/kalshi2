{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "# For TA features (optional, install if needed: pip install ta)\n",
    "# import ta\n",
    "\n",
    "# --- Logging Setup ---\\n\",\n",
    "logger_name = f\"feature_engineering_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "logger = logging.getLogger(logger_name)\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s:%(lineno)d - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration ---\\n\",\n",
    "# Path.cwd() will be /Users/omarabul-hassan/Desktop/projects/kalshi/notebooks/train/\n",
    "current_notebook_dir = Path.cwd()\n",
    "\n",
    "# Navigate to the project's 'notebooks/data/' directory\n",
    "# current_notebook_dir.parent is 'notebooks/'\n",
    "# current_notebook_dir.parent.parent is 'kalshi/' (project root if notebooks is direct child)\n",
    "# More robust: current_notebook_dir.parent = 'notebooks', so we go one level up to 'notebooks', then into 'data'\n",
    "# Correction: if cwd is .../notebooks/train, then:\n",
    "# current_notebook_dir.parent is .../notebooks/\n",
    "# DATA_ROOT_DIR is .../notebooks/data/\n",
    "DATA_ROOT_DIR = current_notebook_dir.parent / \"data\"\n",
    "\n",
    "KALSHI_NTM_DATA_DIR = DATA_ROOT_DIR / \"kalshi_data\"\n",
    "BINANCE_FLAT_DATA_DIR = DATA_ROOT_DIR / \"binance_data\" # This was BINANCE_FLAT_DATA_DIR in your script for features\n",
    "\n",
    "logger.info(f\"Current working directory (notebook location): {current_notebook_dir.resolve()}\")\n",
    "logger.info(f\"KALSHI_NTM_DATA_DIR set to: {KALSHI_NTM_DATA_DIR.resolve()}\")\n",
    "logger.info(f\"BINANCE_FLAT_DATA_DIR set to: {BINANCE_FLAT_DATA_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "# Find the latest outcomes CSV\n",
    "KALSHI_OUTCOMES_CSV_PATH = None # Initialize\n",
    "try:\n",
    "    if not KALSHI_NTM_DATA_DIR.exists():\n",
    "        raise FileNotFoundError(f\"KALSHI_NTM_DATA_DIR does not exist: {KALSHI_NTM_DATA_DIR.resolve()}\")\n",
    "\n",
    "    outcomes_files = sorted(\n",
    "        list(KALSHI_NTM_DATA_DIR.glob(\"kalshi_btc_hourly_NTM_filtered_market_outcomes_*.csv\")),\n",
    "        key=os.path.getctime,\n",
    "        reverse=True\n",
    "    )\n",
    "    if not outcomes_files:\n",
    "        raise FileNotFoundError(f\"No NTM outcomes CSV (kalshi_btc_hourly_NTM_filtered_market_outcomes_*.csv) found in {KALSHI_NTM_DATA_DIR.resolve()}\")\n",
    "    KALSHI_OUTCOMES_CSV_PATH = outcomes_files[0]\n",
    "    logger.info(f\"Using Kalshi NTM outcomes CSV: {KALSHI_OUTCOMES_CSV_PATH.resolve()}\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.critical(str(e))\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error finding outcomes CSV: {e}\", exc_info=True)\n",
    "\n",
    "# --- Parameters for Feature Engineering ---\\n\",\n",
    "# For BTC features\n",
    "BTC_MOMENTUM_WINDOWS = [5, 10, 15, 30, 60] # Added 60 min momentum\n",
    "BTC_VOLATILITY_WINDOW = 15 # In minutes\n",
    "BTC_SMA_WINDOWS = [10, 30, 50] # Added 50 min SMA\n",
    "BTC_EMA_WINDOWS = [12, 26, 50] # Added 50 min EMA\n",
    "BTC_RSI_WINDOW = 14\n",
    "BTC_ATR_WINDOW = 14 # For Average True Range (new)\n",
    "\n",
    "# For Kalshi features\n",
    "KALSHI_PRICE_CHANGE_WINDOWS = [1, 3, 5, 10] # Added 10 min change\n",
    "KALSHI_VOLATILITY_WINDOWS = [5, 10] # For Kalshi mid-price volatility (new)\n",
    "\n",
    "# *** MODIFIED: More Aggressive Filtering ***\n",
    "# Start with 15. If train.ipynb AUC is still > 0.8, try 20 or 25.\n",
    "DECISION_OFFSET_MINUTES_BEFORE_CLOSE = 15 # Changed from 5\n",
    "logger.info(f\"Feature generation will stop generating records {DECISION_OFFSET_MINUTES_BEFORE_CLOSE} minutes before market close.\")\n",
    "\n",
    "KALSHI_MAX_STALENESS_SECONDS = 120\n",
    "\n",
    "logger.info(\"Feature Engineering Setup Complete.\")\n",
    "if not KALSHI_OUTCOMES_CSV_PATH:\n",
    "    logger.warning(\"KALSHI_OUTCOMES_CSV_PATH is not set or was not found. Data loading in subsequent cells will likely fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Kalshi Outcomes and Define Target Variable\n",
    "\n",
    "if KALSHI_OUTCOMES_CSV_PATH and KALSHI_OUTCOMES_CSV_PATH.exists():\n",
    "    try:\n",
    "        df_outcomes = pd.read_csv(KALSHI_OUTCOMES_CSV_PATH)\n",
    "        logger.info(f\"Loaded {len(df_outcomes)} NTM market outcomes from {KALSHI_OUTCOMES_CSV_PATH}\")\n",
    "        \n",
    "        # Convert relevant columns to correct types\n",
    "        df_outcomes['event_resolution_time_iso'] = pd.to_datetime(df_outcomes['event_resolution_time_iso'], errors='coerce', utc=True)\n",
    "        df_outcomes['market_open_time_iso'] = pd.to_datetime(df_outcomes['market_open_time_iso'], errors='coerce', utc=True)\n",
    "        df_outcomes['market_close_time_iso'] = pd.to_datetime(df_outcomes['market_close_time_iso'], errors='coerce', utc=True)\n",
    "        \n",
    "        # Drop rows where essential date conversions failed or key info is missing\n",
    "        df_outcomes.dropna(subset=['market_ticker', 'event_resolution_time_iso', \n",
    "                                   'market_open_time_iso', 'market_close_time_iso',\n",
    "                                   'kalshi_strike_price'], inplace=True)\n",
    "        \n",
    "        logger.info(f\"Outcomes DataFrame shape after initial cleaning: {df_outcomes.shape}\")\n",
    "        print(\"Outcomes DataFrame head:\")\n",
    "        print(df_outcomes.head())\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error loading or processing outcomes CSV {KALSHI_OUTCOMES_CSV_PATH}: {e}\")\n",
    "        df_outcomes = pd.DataFrame() # Empty df if load fails\n",
    "else:\n",
    "    logger.critical(\"Kalshi NTM outcomes CSV path not found or not set. Cannot proceed.\")\n",
    "    df_outcomes = pd.DataFrame()\n",
    "\n",
    "# --- Define Target Variable: BTC_price_at_resolution - kalshi_strike_price ---\n",
    "_binance_daily_data_cache_for_target = {}\n",
    "\n",
    "def get_btc_price_at_resolution(resolution_dt_utc: pd.Timestamp) -> float | None:\n",
    "    if pd.isna(resolution_dt_utc):\n",
    "        return None\n",
    "    global _binance_daily_data_cache_for_target\n",
    "    date_str = resolution_dt_utc.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    if date_str not in _binance_daily_data_cache_for_target:\n",
    "        filepath = BINANCE_FLAT_DATA_DIR / f\"BTCUSDT-1m-{date_str}.csv\"\n",
    "        if not filepath.exists():\n",
    "            logger.warning(f\"Target: Binance data file not found for {date_str} at {filepath}\")\n",
    "            _binance_daily_data_cache_for_target[date_str] = None\n",
    "            return None\n",
    "        try:\n",
    "            column_names = [\"open_time_raw\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "                            \"close_time_ms\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "                            \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"]\n",
    "            df_binance_day = pd.read_csv(filepath, header=None, names=column_names)\n",
    "            df_binance_day['timestamp_s'] = df_binance_day['open_time_raw'] // 1_000_000\n",
    "            df_binance_day.set_index('timestamp_s', inplace=True)\n",
    "            df_binance_day['close'] = pd.to_numeric(df_binance_day['close'])\n",
    "            _binance_daily_data_cache_for_target[date_str] = df_binance_day\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Target: Error loading Binance data from {filepath}: {e}\")\n",
    "            _binance_daily_data_cache_for_target[date_str] = None\n",
    "            return None\n",
    "            \n",
    "    df_binance_day_cached = _binance_daily_data_cache_for_target[date_str]\n",
    "    if df_binance_day_cached is None:\n",
    "        return None\n",
    "\n",
    "    resolution_timestamp_s = int(resolution_dt_utc.timestamp())\n",
    "    idx_pos = df_binance_day_cached.index.searchsorted(resolution_timestamp_s, side='right')\n",
    "    \n",
    "    if idx_pos == 0:\n",
    "        if resolution_dt_utc.time() < dt.time(0,1,0): \n",
    "             logger.warning(f\"Target: Resolution time {resolution_dt_utc.isoformat()} is too early in {date_str}, BTC price might be from previous day or ambiguous. Skipping.\")\n",
    "        return None \n",
    "        \n",
    "    btc_price_at_resolution = df_binance_day_cached.iloc[idx_pos - 1]['close']\n",
    "    return float(btc_price_at_resolution)\n",
    "\n",
    "if not df_outcomes.empty:\n",
    "    tqdm.pandas(desc=\"Fetching BTC price at resolution for target\")\n",
    "    df_outcomes['btc_price_at_resolution'] = df_outcomes['event_resolution_time_iso'].progress_apply(get_btc_price_at_resolution)\n",
    "    \n",
    "    df_outcomes['target_btc_diff_from_strike'] = df_outcomes['btc_price_at_resolution'] - df_outcomes['kalshi_strike_price']\n",
    "    \n",
    "    original_len = len(df_outcomes)\n",
    "    df_outcomes.dropna(subset=['btc_price_at_resolution', 'target_btc_diff_from_strike'], inplace=True)\n",
    "    logger.info(f\"Dropped {original_len - len(df_outcomes)} rows due to missing BTC price at resolution for target calculation.\")\n",
    "    \n",
    "    logger.info(f\"Target variable 'target_btc_diff_from_strike' calculated for {len(df_outcomes)} markets.\")\n",
    "    print(\"\\nOutcomes DataFrame with target variable (head):\")\n",
    "    print(df_outcomes[['market_ticker', 'kalshi_strike_price', 'event_resolution_time_iso', 'btc_price_at_resolution', 'target_btc_diff_from_strike']].head())\n",
    "    print(\"\\nTarget variable statistics:\")\n",
    "    print(df_outcomes['target_btc_diff_from_strike'].describe())\n",
    "else:\n",
    "    logger.warning(\"Outcomes DataFrame is empty, cannot calculate target variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Feature Generation Loop\n",
    "\n",
    "# --- Helper function to parse Kalshi tickers ---\n",
    "def get_event_resolution_details(ticker_string: str | None):\n",
    "    # ... (this function remains the same as your last version) ...\n",
    "    if not ticker_string: return None\n",
    "    event_match = re.match(r\"^(.*?)-(\\d{2}[A-Z]{3}\\d{2})(\\d{2})$\", ticker_string)\n",
    "    market_match = re.match(r\"^(.*?)-(\\d{2}[A-Z]{3}\\d{2})(\\d{2})-(T(\\d+\\.?\\d*))$\", ticker_string)\n",
    "    \n",
    "    match_to_use = market_match if market_match else event_match\n",
    "    if not match_to_use:\n",
    "        return None\n",
    "        \n",
    "    groups = match_to_use.groups()\n",
    "    series, date_str_yymmmdd, hour_str_edt = groups[0], groups[1], groups[2]\n",
    "    strike_price = float(groups[4]) if market_match and len(groups) > 4 and groups[4] else None\n",
    "    \n",
    "    try:\n",
    "        year_int = 2000 + int(date_str_yymmmdd[:2])\n",
    "        month_str = date_str_yymmmdd[2:5].upper()\n",
    "        day_int = int(date_str_yymmmdd[5:])\n",
    "        month_map = {'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,\n",
    "                     'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12}\n",
    "        month_int = month_map[month_str]\n",
    "        hour_edt_int = int(hour_str_edt)\n",
    "        \n",
    "        event_resolution_dt_naive_edt = dt.datetime(year_int, month_int, day_int, hour_edt_int, 0, 0)\n",
    "        utc_offset_hours = 4 \n",
    "        event_resolution_dt_utc_aware = event_resolution_dt_naive_edt.replace(tzinfo=timezone(timedelta(hours=-utc_offset_hours)))\n",
    "        event_resolution_dt_utc = event_resolution_dt_utc_aware.astimezone(timezone.utc)\n",
    "        \n",
    "        return {\n",
    "            \"series\": series,\n",
    "            \"date_str_yymmmdd\": date_str_yymmmdd,\n",
    "            \"hour_str_edt\": hour_str_edt,\n",
    "            \"strike_price\": strike_price,\n",
    "            \"event_resolution_dt_utc\": event_resolution_dt_utc\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing ticker '{ticker_string}' in get_event_resolution_details: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Binance Data Loading and Feature Calculation Helpers (Modified for ATR) ---\n",
    "_binance_daily_data_with_features_cache = {}\n",
    "\n",
    "def get_binance_data_with_features(date_str: str) -> pd.DataFrame | None:\n",
    "    global _binance_daily_data_with_features_cache\n",
    "    required_configs = [\n",
    "        'BINANCE_FLAT_DATA_DIR', 'BTC_MOMENTUM_WINDOWS', 'BTC_VOLATILITY_WINDOW',\n",
    "        'BTC_SMA_WINDOWS', 'BTC_EMA_WINDOWS', 'BTC_RSI_WINDOW', 'BTC_ATR_WINDOW' # Added ATR\n",
    "    ]\n",
    "    for config_var_name in required_configs:\n",
    "        if config_var_name not in globals():\n",
    "            logger.error(f\"Global configuration variable '{config_var_name}' not defined. Cannot calculate Binance features.\")\n",
    "            return None\n",
    "\n",
    "    if date_str in _binance_daily_data_with_features_cache:\n",
    "        cached_df = _binance_daily_data_with_features_cache[date_str]\n",
    "        return cached_df.copy() if cached_df is not None else None\n",
    "\n",
    "    filepath = BINANCE_FLAT_DATA_DIR / f\"BTCUSDT-1m-{date_str}.csv\"\n",
    "    if not filepath.exists():\n",
    "        _binance_daily_data_with_features_cache[date_str] = None\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        column_names = [\"open_time_raw\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "                        \"close_time_ms\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "                        \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"]\n",
    "        df = pd.read_csv(filepath, header=None, names=column_names)\n",
    "        df['timestamp_s'] = df['open_time_raw'] // 1_000_000\n",
    "        df.set_index('timestamp_s', inplace=True)\n",
    "        \n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=['close', 'high', 'low'], inplace=True) # Ensure H,L,C are present for ATR\n",
    "\n",
    "        for window in BTC_MOMENTUM_WINDOWS: \n",
    "            df[f'btc_mom_{window}m'] = df['close'].diff(periods=window)\n",
    "        df[f'btc_vol_{BTC_VOLATILITY_WINDOW}m'] = df['close'].rolling(window=BTC_VOLATILITY_WINDOW, min_periods=1).std()\n",
    "        for window in BTC_SMA_WINDOWS: \n",
    "            df[f'btc_sma_{window}m'] = df['close'].rolling(window=window, min_periods=1).mean()\n",
    "        for window in BTC_EMA_WINDOWS: \n",
    "            df[f'btc_ema_{window}m'] = df['close'].ewm(span=window, adjust=False, min_periods=1).mean()\n",
    "        \n",
    "        if BTC_RSI_WINDOW > 0:\n",
    "            delta = df['close'].diff(1)\n",
    "            gain = delta.where(delta > 0, 0)\n",
    "            loss = -delta.where(delta < 0, 0)\n",
    "            avg_gain = gain.rolling(window=BTC_RSI_WINDOW, min_periods=1).mean()\n",
    "            avg_loss = loss.rolling(window=BTC_RSI_WINDOW, min_periods=1).mean()\n",
    "            rs = avg_gain / avg_loss.replace(0, 1e-9) \n",
    "            df['btc_rsi'] = 100 - (100 / (1 + rs))\n",
    "            df['btc_rsi'].fillna(50, inplace=True)\n",
    "\n",
    "        # *** NEW: Calculate ATR ***\n",
    "        if BTC_ATR_WINDOW > 0:\n",
    "            high_low = df['high'] - df['low']\n",
    "            high_close_prev = np.abs(df['high'] - df['close'].shift(1))\n",
    "            low_close_prev = np.abs(df['low'] - df['close'].shift(1))\n",
    "            tr = pd.concat([high_low, high_close_prev, low_close_prev], axis=1).max(axis=1)\n",
    "            df[f'btc_atr_{BTC_ATR_WINDOW}'] = tr.ewm(alpha=1/BTC_ATR_WINDOW, adjust=False, min_periods=BTC_ATR_WINDOW).mean()\n",
    "            # df[f'btc_atr_{BTC_ATR_WINDOW}'] = tr.rolling(window=BTC_ATR_WINDOW, min_periods=1).mean() # Simpler rolling mean ATR\n",
    "            df[f'btc_atr_{BTC_ATR_WINDOW}'].fillna(method='bfill', inplace=True) # Backfill initial NaNs\n",
    "\n",
    "        _binance_daily_data_with_features_cache[date_str] = df\n",
    "        return df.copy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"FeatureGen: Error loading/processing Binance data from {filepath}: {e}\")\n",
    "        _binance_daily_data_with_features_cache[date_str] = None\n",
    "        return None\n",
    "\n",
    "# --- Kalshi Data Loading Helper ---\n",
    "_kalshi_market_data_cache = {}\n",
    "# ... (load_kalshi_market_data function remains the same as your last version) ...\n",
    "def load_kalshi_market_data(market_ticker: str, date_str_yymmmdd: str, hour_str_edt: str) -> pd.DataFrame | None:\n",
    "    global _kalshi_market_data_cache\n",
    "    if 'KALSHI_NTM_DATA_DIR' not in globals():\n",
    "        logger.error(\"KALSHI_NTM_DATA_DIR not defined globally. Cannot load Kalshi data.\")\n",
    "        return None\n",
    "\n",
    "    if market_ticker in _kalshi_market_data_cache:\n",
    "        cached_df = _kalshi_market_data_cache[market_ticker]\n",
    "        return cached_df.copy() if cached_df is not None else None\n",
    "    \n",
    "    filepath = KALSHI_NTM_DATA_DIR / date_str_yymmmdd / hour_str_edt.zfill(2) / f\"{market_ticker}.csv\"\n",
    "    if not filepath.exists():\n",
    "        _kalshi_market_data_cache[market_ticker] = None\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        if df.empty:\n",
    "            _kalshi_market_data_cache[market_ticker] = pd.DataFrame()\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        df['timestamp_s'] = pd.to_numeric(df['timestamp_s'])\n",
    "        df.set_index('timestamp_s', inplace=True)\n",
    "        # Add 'mid_price' calculation here for later use in Kalshi volatility\n",
    "        if 'yes_bid_close_cents' in df.columns and 'yes_ask_close_cents' in df.columns:\n",
    "            df['mid_price'] = (pd.to_numeric(df['yes_bid_close_cents'], errors='coerce') + \n",
    "                               pd.to_numeric(df['yes_ask_close_cents'], errors='coerce')) / 2\n",
    "            \n",
    "        for col in df.columns:\n",
    "            if 'cents' in col or 'volume' in col or 'interest' in col or 'mid_price' in col:\n",
    "                 if col in df.columns: # Check if column exists after potential creation (mid_price)\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        _kalshi_market_data_cache[market_ticker] = df\n",
    "        return df.copy()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FeatureGen: Error loading Kalshi market data from {filepath}: {e}\")\n",
    "        _kalshi_market_data_cache[market_ticker] = None\n",
    "        return None\n",
    "\n",
    "# --- Main Feature List ---\n",
    "all_feature_records = []\n",
    "\n",
    "if 'df_outcomes' not in globals() or df_outcomes.empty:\n",
    "    logger.error(\"df_outcomes is not defined or is empty. Please run Cell 2 first.\")\n",
    "else:\n",
    "    logger.info(f\"Starting feature generation for {len(df_outcomes)} Kalshi markets.\")\n",
    "    DECISION_OFFSET = globals().get('DECISION_OFFSET_MINUTES_BEFORE_CLOSE', 15) # Default to 15 if not set\n",
    "    logger.info(f\"Feature generation will use DECISION_OFFSET_MINUTES_BEFORE_CLOSE = {DECISION_OFFSET}\")\n",
    "\n",
    "    for idx, market_row in tqdm(df_outcomes.iterrows(), total=len(df_outcomes), desc=\"Processing Kalshi Markets\"):\n",
    "        kalshi_market_ticker = market_row['market_ticker']\n",
    "        kalshi_strike_price = market_row['kalshi_strike_price']\n",
    "        kalshi_market_open_dt = market_row['market_open_time_iso']\n",
    "        kalshi_market_close_dt = market_row['market_close_time_iso']\n",
    "        target_value = market_row['target_btc_diff_from_strike']\n",
    "\n",
    "        if pd.isna(kalshi_market_open_dt) or pd.isna(kalshi_market_close_dt):\n",
    "            continue\n",
    "            \n",
    "        parsed_ticker_info = get_event_resolution_details(kalshi_market_ticker)\n",
    "        if not parsed_ticker_info: continue\n",
    "        if not all(k in parsed_ticker_info for k in [\"date_str_yymmmdd\", \"hour_str_edt\"]): continue\n",
    "\n",
    "        date_dir_str = parsed_ticker_info[\"date_str_yymmmdd\"]\n",
    "        hour_dir_str = parsed_ticker_info[\"hour_str_edt\"]\n",
    "\n",
    "        df_kalshi_market = load_kalshi_market_data(kalshi_market_ticker, date_dir_str, hour_dir_str)\n",
    "        if df_kalshi_market is None or df_kalshi_market.empty:\n",
    "            continue\n",
    "\n",
    "        latest_permissible_decision_dt_for_features = kalshi_market_close_dt - timedelta(minutes=DECISION_OFFSET)\n",
    "        \n",
    "        current_minute_dt = kalshi_market_open_dt\n",
    "        while current_minute_dt < kalshi_market_close_dt :\n",
    "            if current_minute_dt >= latest_permissible_decision_dt_for_features:\n",
    "                current_minute_dt += timedelta(minutes=1)\n",
    "                continue \n",
    "\n",
    "            decision_point_dt_utc = current_minute_dt \n",
    "            decision_point_ts_utc = int(decision_point_dt_utc.timestamp())\n",
    "            signal_ts_utc = decision_point_ts_utc - 60\n",
    "            \n",
    "            signal_dt_utc_obj = dt.datetime.fromtimestamp(signal_ts_utc, tz=timezone.utc)\n",
    "            binance_day_str = signal_dt_utc_obj.strftime(\"%Y-%m-%d\")\n",
    "            df_binance_day_features = get_binance_data_with_features(binance_day_str)\n",
    "\n",
    "            btc_features = {}\n",
    "            # *** BTC FEATURES (including new ATR and relative price) ***\n",
    "            if df_binance_day_features is not None and not df_binance_day_features.empty:\n",
    "                if signal_ts_utc in df_binance_day_features.index:\n",
    "                    btc_row = df_binance_day_features.loc[signal_ts_utc]\n",
    "                    btc_price_t_minus_1 = btc_row.get('close')\n",
    "                    if pd.isna(btc_price_t_minus_1): # Critical check\n",
    "                        current_minute_dt += timedelta(minutes=1); continue\n",
    "                        \n",
    "                    btc_features['btc_price_t_minus_1'] = btc_price_t_minus_1\n",
    "                    for window in BTC_MOMENTUM_WINDOWS: btc_features[f'btc_mom_{window}m'] = btc_row.get(f'btc_mom_{window}m')\n",
    "                    btc_features[f'btc_vol_{BTC_VOLATILITY_WINDOW}m'] = btc_row.get(f'btc_vol_{BTC_VOLATILITY_WINDOW}m')\n",
    "                    for window in BTC_SMA_WINDOWS: \n",
    "                        sma_val = btc_row.get(f'btc_sma_{window}m')\n",
    "                        btc_features[f'btc_sma_{window}m'] = sma_val\n",
    "                        # NEW: BTC Price vs SMA\n",
    "                        if pd.notna(sma_val) and sma_val != 0:\n",
    "                             btc_features[f'btc_price_vs_sma_{window}m'] = btc_price_t_minus_1 / sma_val\n",
    "                        else:\n",
    "                             btc_features[f'btc_price_vs_sma_{window}m'] = 1.0 # Avoid div by zero, implies price is at SMA\n",
    "                    for window in BTC_EMA_WINDOWS: \n",
    "                        ema_val = btc_row.get(f'btc_ema_{window}m')\n",
    "                        btc_features[f'btc_ema_{window}m'] = ema_val\n",
    "                        # NEW: BTC Price vs EMA\n",
    "                        if pd.notna(ema_val) and ema_val != 0:\n",
    "                             btc_features[f'btc_price_vs_ema_{window}m'] = btc_price_t_minus_1 / ema_val\n",
    "                        else:\n",
    "                             btc_features[f'btc_price_vs_ema_{window}m'] = 1.0\n",
    "                    if BTC_RSI_WINDOW > 0: btc_features['btc_rsi'] = btc_row.get('btc_rsi')\n",
    "                    if BTC_ATR_WINDOW > 0: btc_features[f'btc_atr_{BTC_ATR_WINDOW}'] = btc_row.get(f'btc_atr_{BTC_ATR_WINDOW}')\n",
    "            \n",
    "            if 'btc_price_t_minus_1' not in btc_features: # Check again after potential NaN skip\n",
    "                current_minute_dt += timedelta(minutes=1)\n",
    "                continue\n",
    "            \n",
    "            # *** KALSHI FEATURES (including new volatility and relative features) ***\n",
    "            kalshi_features = {}\n",
    "            kalshi_mid_price_t_minus_1 = np.nan # Initialize\n",
    "            \n",
    "            relevant_kalshi_rows = df_kalshi_market[df_kalshi_market.index <= signal_ts_utc]\n",
    "            if not relevant_kalshi_rows.empty:\n",
    "                latest_kalshi_row = relevant_kalshi_rows.iloc[-1]\n",
    "                latest_kalshi_ts = latest_kalshi_row.name\n",
    "                STALENESS_LIMIT = globals().get('KALSHI_MAX_STALENESS_SECONDS', 120)\n",
    "                \n",
    "                if (signal_ts_utc - latest_kalshi_ts) <= STALENESS_LIMIT:\n",
    "                    kalshi_features['kalshi_yes_bid'] = latest_kalshi_row.get('yes_bid_close_cents')\n",
    "                    kalshi_features['kalshi_yes_ask'] = latest_kalshi_row.get('yes_ask_close_cents')\n",
    "                    \n",
    "                    if pd.notna(kalshi_features.get('kalshi_yes_bid')) and pd.notna(kalshi_features.get('kalshi_yes_ask')):\n",
    "                        kalshi_features['kalshi_spread'] = kalshi_features['kalshi_yes_ask'] - kalshi_features['kalshi_yes_bid']\n",
    "                        kalshi_mid_price_t_minus_1 = (kalshi_features['kalshi_yes_bid'] + kalshi_features['kalshi_yes_ask']) / 2.0\n",
    "                        kalshi_features['kalshi_mid_price'] = kalshi_mid_price_t_minus_1\n",
    "                    \n",
    "                    PRICE_CHANGE_WINDOWS = globals().get('KALSHI_PRICE_CHANGE_WINDOWS', [1,3,5,10])\n",
    "                    for window in PRICE_CHANGE_WINDOWS:\n",
    "                        prev_mid_price_ts = signal_ts_utc - (window * 60)\n",
    "                        prev_mid_rows = df_kalshi_market[df_kalshi_market.index <= prev_mid_price_ts]\n",
    "                        if not prev_mid_rows.empty and pd.notna(kalshi_mid_price_t_minus_1):\n",
    "                            prev_mid_latest_row = prev_mid_rows.iloc[-1]\n",
    "                            # Check if 'mid_price' column exists from load_kalshi_market_data\n",
    "                            prev_mid_val = prev_mid_latest_row.get('mid_price') \n",
    "                            if pd.notna(prev_mid_val):\n",
    "                                kalshi_features[f'kalshi_mid_chg_{window}m'] = kalshi_mid_price_t_minus_1 - prev_mid_val\n",
    "                    \n",
    "                    kalshi_features['kalshi_volume_t_minus_1'] = latest_kalshi_row.get('volume')\n",
    "                    kalshi_features['kalshi_open_interest_t_minus_1'] = latest_kalshi_row.get('open_interest')\n",
    "\n",
    "                    # NEW: Kalshi Mid Price Volatility\n",
    "                    # Need to ensure 'mid_price' was pre-calculated in df_kalshi_market by load_kalshi_market_data\n",
    "                    if 'mid_price' in df_kalshi_market.columns:\n",
    "                        kalshi_history_for_vol = df_kalshi_market[df_kalshi_market.index <= signal_ts_utc]['mid_price']\n",
    "                        for window in KALSHI_VOLATILITY_WINDOWS:\n",
    "                            if len(kalshi_history_for_vol) >= window :\n",
    "                                kalshi_features[f'kalshi_mid_vol_{window}m'] = kalshi_history_for_vol.tail(window).std()\n",
    "                            else:\n",
    "                                kalshi_features[f'kalshi_mid_vol_{window}m'] = np.nan # or 0 if preferred\n",
    "\n",
    "            # Original distance to strike\n",
    "            kalshi_features['distance_to_strike'] = btc_features['btc_price_t_minus_1'] - kalshi_strike_price\n",
    "            \n",
    "            # NEW: Distance to strike normalized by ATR\n",
    "            current_atr = btc_features.get(f'btc_atr_{BTC_ATR_WINDOW}')\n",
    "            if pd.notna(current_atr) and current_atr > 1e-6: # Avoid division by zero or tiny ATR\n",
    "                kalshi_features['distance_to_strike_norm_atr'] = kalshi_features['distance_to_strike'] / current_atr\n",
    "            else:\n",
    "                kalshi_features['distance_to_strike_norm_atr'] = kalshi_features['distance_to_strike'] # Fallback or set to a large number/NaN\n",
    "\n",
    "            # NEW: Kalshi implied prob vs BTC \"market prob\" (simplified)\n",
    "            # This is a very rough estimate, (BTC price - strike) / ATR can act as a Z-score\n",
    "            if pd.notna(kalshi_mid_price_t_minus_1) and pd.notna(kalshi_features.get('distance_to_strike_norm_atr')):\n",
    "                kalshi_implied_yes_prob = kalshi_mid_price_t_minus_1 / 100.0\n",
    "                # A simple way to map distance_to_strike_norm_atr to a 0-1 scale (e.g. using a sigmoid or erf, or just capping)\n",
    "                # For now, let's just make a diff. A positive diff means Kalshi implies higher prob than BTC's position\n",
    "                # This is very conceptual and needs refinement. Let's use a simpler 'kalshi_btc_price_spread_points'\n",
    "                # Representing the difference in cents if BTC price were the Kalshi mid.\n",
    "                # No, let's keep the original complex one commented out for now as it's too experimental.\n",
    "                # Let's calculate something like: kalshi_mid_price_t_minus_1 - (50 + (distance_to_strike / some_avg_move_range))\n",
    "                # This 'some_avg_move_range' could be related to ATR.\n",
    "                # For instance, if distance_to_strike is +1 ATR, maybe that's like +15c over 50. Very heuristic.\n",
    "                if pd.notna(current_atr) and current_atr > 1e-6:\n",
    "                     # If BTC is 1 ATR above strike, add 15 cents to 50. If 1 ATR below, subtract 15. Capped.\n",
    "                    btc_implied_value_offset = np.clip( (kalshi_features['distance_to_strike'] / current_atr) * 15, -45, 45)\n",
    "                    btc_equiv_kalshi_price = 50 + btc_implied_value_offset\n",
    "                    kalshi_features['kalshi_vs_btc_implied_spread'] = kalshi_mid_price_t_minus_1 - btc_equiv_kalshi_price\n",
    "                else:\n",
    "                    kalshi_features['kalshi_vs_btc_implied_spread'] = 0.0\n",
    "\n",
    "\n",
    "            time_features = {}\n",
    "            time_features['time_until_market_close_min'] = (kalshi_market_close_dt - decision_point_dt_utc).total_seconds() / 60\n",
    "            time_features['hour_of_day_utc'] = decision_point_dt_utc.hour \n",
    "            time_features['day_of_week_utc'] = decision_point_dt_utc.weekday()\n",
    "            decision_point_dt_edt = decision_point_dt_utc.astimezone(timezone(timedelta(hours=-4)))\n",
    "            time_features['hour_of_day_edt'] = decision_point_dt_edt.hour\n",
    "\n",
    "            current_record = {\n",
    "                'kalshi_market_ticker': kalshi_market_ticker,\n",
    "                'decision_point_ts_utc': decision_point_ts_utc,\n",
    "                'kalshi_strike_price': kalshi_strike_price,\n",
    "                **btc_features, **kalshi_features, **time_features,\n",
    "                'TARGET_btc_diff_from_strike': target_value\n",
    "            }\n",
    "            all_feature_records.append(current_record)\n",
    "            current_minute_dt += timedelta(minutes=1)\n",
    "\n",
    "        if kalshi_market_ticker in _kalshi_market_data_cache:\n",
    "            _kalshi_market_data_cache.pop(kalshi_market_ticker, None)\n",
    "    \n",
    "    _binance_daily_data_with_features_cache = {}\n",
    "\n",
    "df_features = pd.DataFrame(all_feature_records)\n",
    "logger.info(f\"Generated {len(df_features)} feature records in total after applying decision offset and adding new features.\")\n",
    "\n",
    "if not df_features.empty:\n",
    "    print(\"\\nSample of generated features (first 5 rows):\")\n",
    "    # Print more columns to see new features\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(df_features.head().to_string())\n",
    "    \n",
    "    save_dir = Path.cwd().parent / \"features\" # Save to project_root/features\n",
    "    save_dir.mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    features_csv_path = save_dir / f\"kalshi_btc_features_target_v2_filtered_{DECISION_OFFSET}m_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    try:\n",
    "        df_features.to_csv(features_csv_path, index=False)\n",
    "        logger.info(f\"Successfully saved FILTERED features (v2) and target to: {features_csv_path.resolve()}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving features DataFrame to CSV: {e}\")\n",
    "else:\n",
    "    logger.warning(\"No feature records were generated. Check DECISION_OFFSET and data availability.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
