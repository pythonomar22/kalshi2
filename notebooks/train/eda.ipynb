{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports, Load Backtest Logs, AND Load Training Input Features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json # For loading feature columns if needed\n",
    "\n",
    "# --- Configuration for Backtest Log Analysis ---\n",
    "# Assuming eda.ipynb is in /Users/omarabul-hassan/Desktop/projects/kalshi/notebooks/train/\n",
    "current_notebook_dir = Path.cwd() # This will be .../kalshi/notebooks/train\n",
    "# LOGS_PARENT_DIR should point to .../kalshi/notebooks/logs\n",
    "LOGS_PARENT_DIR = current_notebook_dir.parent.parent / \"notebooks\" / \"logs\" # Adjusted to go up two levels then into notebooks/logs\n",
    "# Or, if your project structure is flat within 'kalshi' and 'notebooks' is a direct child:\n",
    "# LOGS_PARENT_DIR = current_notebook_dir.parent.parent / \"logs\" # if 'train' is inside 'notebooks' which is inside 'kalshi'\n",
    "\n",
    "# Let's be more explicit based on your provided path for features:\n",
    "PROJECT_ROOT_DIR = Path(\"/Users/omarabul-hassan/Desktop/projects/kalshi\")\n",
    "LOGS_PARENT_DIR = PROJECT_ROOT_DIR / \"notebooks\" / \"logs\"\n",
    "FEATURES_INPUT_DIR = PROJECT_ROOT_DIR / \"notebooks\" / \"features\"\n",
    "TRAINED_MODELS_DIR = PROJECT_ROOT_DIR / \"notebooks\" / \"trained_models\" / \"logreg\"\n",
    "\n",
    "\n",
    "print(f\"PROJECT_ROOT_DIR: {PROJECT_ROOT_DIR.resolve()}\")\n",
    "print(f\"LOGS_PARENT_DIR: {LOGS_PARENT_DIR.resolve()}\")\n",
    "print(f\"FEATURES_INPUT_DIR: {FEATURES_INPUT_DIR.resolve()}\")\n",
    "print(f\"TRAINED_MODELS_DIR: {TRAINED_MODELS_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "MODEL_TYPE_LOG_SUBDIR_NAME = \"backtest_logreg_calibrated_v2_15m_offset\"\n",
    "print(f\"Expecting detailed logs in subdir: {MODEL_TYPE_LOG_SUBDIR_NAME}\")\n",
    "\n",
    "# --- STRATEGY PARAMETERS (Manually set to match the backtest run being analyzed) ---\n",
    "# These should match the values used in the backtest_classifier_v1.py run that produced the logs\n",
    "# From your previous EDA output, these were:\n",
    "STRATEGY_MIN_MODEL_PROB_FOR_CONSIDERATION = 0.60\n",
    "STRATEGY_EDGE_THRESHOLD_FOR_TRADE = 0.10\n",
    "# IF YOU RE-RAN THE BACKTEST WITH DIFFERENT PARAMS (e.g., P_model=0.85), UPDATE THESE!\n",
    "print(f\"Using for EDA (from backtest params): STRATEGY_MIN_MODEL_PROB_FOR_CONSIDERATION = {STRATEGY_MIN_MODEL_PROB_FOR_CONSIDERATION}\")\n",
    "print(f\"Using for EDA (from backtest params): STRATEGY_EDGE_THRESHOLD_FOR_TRADE = {STRATEGY_EDGE_THRESHOLD_FOR_TRADE}\")\n",
    "\n",
    "\n",
    "# --- Load Main Aggregated Trades File (from backtest) ---\n",
    "all_trades_files_pattern = \"ALL_TRADES_WITH_EVALS_*.csv\"\n",
    "# Ensure LOGS_PARENT_DIR is correct for this glob\n",
    "all_trades_files = sorted(LOGS_PARENT_DIR.glob(all_trades_files_pattern), key=os.path.getctime, reverse=True)\n",
    "\n",
    "df_all_decisions = pd.DataFrame()\n",
    "ALL_TRADES_CSV_PATH = None\n",
    "run_ts_from_filename = None # This will be the timestamp from the ALL_TRADES_WITH_EVALS filename\n",
    "\n",
    "if not all_trades_files:\n",
    "    print(f\"ERROR: No '{all_trades_files_pattern}' file found in {LOGS_PARENT_DIR.resolve()}\")\n",
    "else:\n",
    "    ALL_TRADES_CSV_PATH = all_trades_files[0]\n",
    "    if ALL_TRADES_CSV_PATH:\n",
    "        print(f\"Loading main decision log: {ALL_TRADES_CSV_PATH}\")\n",
    "        try:\n",
    "            df_all_decisions = pd.read_csv(ALL_TRADES_CSV_PATH)\n",
    "            df_all_decisions['decision_timestamp_utc'] = pd.to_datetime(df_all_decisions['decision_timestamp_utc'])\n",
    "            print(f\"Loaded {len(df_all_decisions)} total decision records from main log.\")\n",
    "\n",
    "            filename_stem = Path(ALL_TRADES_CSV_PATH).stem\n",
    "            match_ts = re.search(r'(\\d{8}_\\d{6})', filename_stem)\n",
    "            if match_ts:\n",
    "                run_ts_from_filename = match_ts.group(1)\n",
    "                print(f\"Extracted run_ts_from_filename: {run_ts_from_filename}\")\n",
    "            else:\n",
    "                parts = filename_stem.split('_')\n",
    "                if len(parts) > 1 and re.match(r'\\d{8}_\\d{6}', parts[-1]):\n",
    "                    run_ts_from_filename = parts[-1]\n",
    "                    print(f\"Extracted run_ts_from_filename (fallback): {run_ts_from_filename}\")\n",
    "                else:\n",
    "                    print(f\"WARNING: Could not robustly parse timestamp from '{filename_stem}'. Timestamp matching for detailed logs might be affected.\")\n",
    "                    # Example: run_ts_from_filename = \"20250521_235341\" # Manually set if needed AND if you are sure\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {ALL_TRADES_CSV_PATH}: {e}\")\n",
    "            df_all_decisions = pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"ERROR: Could not determine a suitable '{all_trades_files_pattern}' file.\")\n",
    "\n",
    "\n",
    "# --- Load and Merge Detailed Decision Evaluation Logs (from backtest) ---\n",
    "df_eval_details = pd.DataFrame()\n",
    "if not df_all_decisions.empty and run_ts_from_filename:\n",
    "    print(f\"Using run_timestamp '{run_ts_from_filename}' for finding decision_eval_logs.\")\n",
    "    decision_eval_log_specific_dir = LOGS_PARENT_DIR / MODEL_TYPE_LOG_SUBDIR_NAME\n",
    "    eval_log_pattern = f\"decision_eval_log_*_random_forest_{run_ts_from_filename}.csv\"\n",
    "    print(f\"Looking for decision_eval_log files in: {decision_eval_log_specific_dir.resolve()}\")\n",
    "    print(f\"Using pattern: {eval_log_pattern}\")\n",
    "    decision_eval_files = sorted(list(decision_eval_log_specific_dir.glob(eval_log_pattern)))\n",
    "\n",
    "    if not decision_eval_files:\n",
    "        print(f\"WARNING: No '{eval_log_pattern}' files found in {decision_eval_log_specific_dir.resolve()}\")\n",
    "    else:\n",
    "        print(f\"Found {len(decision_eval_files)} decision_eval_log files to merge.\")\n",
    "        df_list = []\n",
    "        for f_path in decision_eval_files:\n",
    "            try:\n",
    "                df_temp = pd.read_csv(f_path)\n",
    "                df_list.append(df_temp)\n",
    "            except Exception as e: print(f\"Error loading {f_path}: {e}\")\n",
    "        if df_list:\n",
    "            df_eval_details = pd.concat(df_list, ignore_index=True)\n",
    "            # Ensure 'decision_dt_utc' column is datetime\n",
    "            if 'decision_dt_utc' in df_eval_details.columns:\n",
    "                 df_eval_details['decision_dt_utc'] = pd.to_datetime(df_eval_details['decision_dt_utc'])\n",
    "            else:\n",
    "                print(\"WARNING: 'decision_dt_utc' missing in df_eval_details. Merge might fail or be incomplete.\")\n",
    "            \n",
    "            print(f\"Loaded and combined {len(df_eval_details)} records from decision_eval_logs.\")\n",
    "\n",
    "            eval_cols_to_merge = ['decision_dt_utc', 'market_ticker', 'implied_proba_yes_at_ask',\n",
    "                                  'edge_for_yes', 'predicted_proba_no', 'implied_proba_no_at_bid',\n",
    "                                  'edge_for_no', 'considered_action']\n",
    "            \n",
    "            # Check which of these columns actually exist in df_eval_details before trying to select them\n",
    "            existing_eval_cols_to_merge = [col for col in eval_cols_to_merge if col in df_eval_details.columns]\n",
    "            missing_cols = set(eval_cols_to_merge) - set(existing_eval_cols_to_merge)\n",
    "            if missing_cols:\n",
    "                print(f\"Warning: The following columns were expected but not found in df_eval_details and will not be merged: {missing_cols}\")\n",
    "\n",
    "            if 'decision_dt_utc' not in existing_eval_cols_to_merge and 'decision_dt_utc' in df_eval_details.columns: # Ensure merge keys are present\n",
    "                 existing_eval_cols_to_merge = ['decision_dt_utc', 'market_ticker'] + [c for c in existing_eval_cols_to_merge if c not in ['decision_dt_utc', 'market_ticker']]\n",
    "\n",
    "\n",
    "            if existing_eval_cols_to_merge and 'decision_dt_utc' in existing_eval_cols_to_merge and 'market_ticker' in existing_eval_cols_to_merge:\n",
    "                df_all_decisions = pd.merge(\n",
    "                    df_all_decisions,\n",
    "                    df_eval_details[existing_eval_cols_to_merge], # Only use existing columns\n",
    "                    left_on=['decision_timestamp_utc', 'market_ticker'],\n",
    "                    right_on=['decision_dt_utc', 'market_ticker'],\n",
    "                    how='left',\n",
    "                    suffixes=('', '_eval')\n",
    "                )\n",
    "                \n",
    "                # Cleanup after merge\n",
    "                if 'decision_dt_utc_eval' in df_all_decisions.columns:\n",
    "                     df_all_decisions.drop(columns=['decision_dt_utc_eval'], inplace=True)\n",
    "                elif 'decision_dt_utc' in df_all_decisions.columns and 'decision_dt_utc' in df_eval_details.columns and df_all_decisions.columns.tolist().count('decision_dt_utc') > 1:\n",
    "                     df_all_decisions = df_all_decisions.loc[:,~df_all_decisions.columns.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "                if 'considered_action_eval' in df_all_decisions.columns:\n",
    "                    df_all_decisions['considered_action'] = df_all_decisions['considered_action_eval'].fillna(df_all_decisions['considered_action'])\n",
    "                    df_all_decisions.drop(columns=['considered_action_eval'], inplace=True)\n",
    "                elif 'considered_action' not in df_all_decisions.columns and 'considered_action' in existing_eval_cols_to_merge:\n",
    "                     # This case should not happen if merge was successful and 'considered_action' was in eval_details\n",
    "                     pass\n",
    "\n",
    "\n",
    "                print(f\"Merged df_eval_details (edge info) into df_all_decisions. New shape: {df_all_decisions.shape}\")\n",
    "                newly_added_cols_from_eval_check = ['implied_proba_yes_at_ask', 'edge_for_yes', 'predicted_proba_no', 'implied_proba_no_at_bid', 'edge_for_no', 'considered_action']\n",
    "                for col in newly_added_cols_from_eval_check:\n",
    "                    if col in df_all_decisions.columns:\n",
    "                        print(f\"Column '{col}' present after merge. Non-NaN count: {df_all_decisions[col].notna().sum()}\")\n",
    "                    else:\n",
    "                         print(f\"INFO: Column '{col}' was not part of the merge or was dropped.\")\n",
    "            else:\n",
    "                print(\"Could not perform merge with df_eval_details due to missing key columns ('decision_dt_utc', 'market_ticker') in selected eval columns.\")\n",
    "        else:\n",
    "            print(\"No data loaded from decision_eval_logs, df_eval_details is empty.\")\n",
    "elif not df_all_decisions.empty:\n",
    "    print(f\"WARNING: run_ts_from_filename issue ('{run_ts_from_filename}'). Skipping merge of detailed eval logs.\")\n",
    "\n",
    "\n",
    "# --- Load and Merge Input Features from Training Data ---\n",
    "PATH_TO_TRAINING_FEATURES_CSV = FEATURES_INPUT_DIR / \"kalshi_btc_features_target_v2_filtered_15m_20250521_163917.csv\"\n",
    "\n",
    "df_input_features = pd.DataFrame()\n",
    "model_input_feature_names = [] # Define globally for later cells\n",
    "\n",
    "if PATH_TO_TRAINING_FEATURES_CSV.exists():\n",
    "    print(f\"Loading training input features from: {PATH_TO_TRAINING_FEATURES_CSV}\")\n",
    "    try:\n",
    "        df_input_features = pd.read_csv(PATH_TO_TRAINING_FEATURES_CSV)\n",
    "        if 'decision_point_ts_utc' in df_input_features.columns:\n",
    "            df_input_features['decision_point_ts_utc'] = pd.to_datetime(df_input_features['decision_point_ts_utc'], unit='s', utc=True)\n",
    "            print(f\"Loaded {len(df_input_features)} records with {len(df_input_features.columns)} columns from training features CSV.\")\n",
    "\n",
    "            # --- Merge input features into df_all_decisions ---\n",
    "            if not df_all_decisions.empty:\n",
    "                feature_cols_json_path = TRAINED_MODELS_DIR / \"feature_columns_classifier_v2.json\"\n",
    "                if feature_cols_json_path.exists():\n",
    "                    with open(feature_cols_json_path, 'r') as f:\n",
    "                        model_input_feature_names = json.load(f)\n",
    "                    print(f\"Loaded {len(model_input_feature_names)} model input feature names from JSON: {feature_cols_json_path}\")\n",
    "                else:\n",
    "                    print(f\"WARNING: {feature_cols_json_path} not found. Attempting to infer feature columns.\")\n",
    "                    # Infer feature columns (less robust)\n",
    "                    excluded_cols_infer = ['kalshi_market_ticker', 'decision_point_ts_utc', 'kalshi_strike_price', \n",
    "                                           'TARGET_btc_diff_from_strike', 'TARGET_market_resolves_yes']\n",
    "                    model_input_feature_names = [col for col in df_input_features.columns if col not in excluded_cols_infer]\n",
    "                    print(f\"Inferred {len(model_input_feature_names)} model input feature names.\")\n",
    "\n",
    "\n",
    "                cols_to_select_from_input_features = ['decision_point_ts_utc', 'kalshi_market_ticker'] + \\\n",
    "                                                     [col for col in model_input_feature_names if col in df_input_features.columns]\n",
    "                \n",
    "                # Ensure no duplicate columns in selection other than merge keys\n",
    "                unique_cols_to_select = []\n",
    "                for col in cols_to_select_from_input_features:\n",
    "                    if col not in unique_cols_to_select:\n",
    "                        unique_cols_to_select.append(col)\n",
    "                df_input_features_subset = df_input_features[unique_cols_to_select].copy()\n",
    "\n",
    "\n",
    "                original_decision_cols = df_all_decisions.columns.tolist()\n",
    "                \n",
    "                # Before merging, check for existing columns from model_input_feature_names in df_all_decisions\n",
    "                # This can happen if columns were somehow added from df_eval_details with same names\n",
    "                conflicting_cols = [col for col in model_input_feature_names if col in df_all_decisions.columns and col not in ['decision_timestamp_utc', 'market_ticker']]\n",
    "                if conflicting_cols:\n",
    "                    print(f\"Warning: The following input feature names already exist in df_all_decisions from previous merges: {conflicting_cols}. They will be overwritten or suffixed.\")\n",
    "                    # Optionally, rename them in df_all_decisions before merge:\n",
    "                    # df_all_decisions.rename(columns={c: c + '_from_eval' for c in conflicting_cols}, inplace=True)\n",
    "\n",
    "\n",
    "                df_all_decisions = pd.merge(\n",
    "                    df_all_decisions,\n",
    "                    df_input_features_subset,\n",
    "                    left_on=['decision_timestamp_utc', 'market_ticker'],\n",
    "                    right_on=['decision_point_ts_utc', 'kalshi_market_ticker'],\n",
    "                    how='left',\n",
    "                    suffixes=('', '_feat_input')\n",
    "                )\n",
    "                print(f\"Shape of df_all_decisions after merging input features: {df_all_decisions.shape}\")\n",
    "\n",
    "                # Clean up merge artifacts\n",
    "                if 'decision_point_ts_utc_feat_input' in df_all_decisions.columns: # Should not happen with current suffixes logic\n",
    "                    df_all_decisions.drop(columns=['decision_point_ts_utc_feat_input'], inplace=True)\n",
    "                if 'kalshi_market_ticker_feat_input' in df_all_decisions.columns:\n",
    "                    df_all_decisions.drop(columns=['kalshi_market_ticker_feat_input'], inplace=True)\n",
    "                # Drop the original decision_point_ts_utc from the right DF if it wasn't used as a suffix target\n",
    "                if 'decision_point_ts_utc' in df_all_decisions.columns and df_all_decisions.columns.tolist().count('decision_point_ts_utc') > 1:\n",
    "                     # Keep the one from the left table (df_all_decisions's original 'decision_timestamp_utc')\n",
    "                     # This can be tricky. A safer way is to ensure `decision_timestamp_utc` is primary.\n",
    "                     # The current suffixes=('', '_feat_input') means original cols from left are unsuffixed.\n",
    "                     # So if 'decision_point_ts_utc' came from right, it might be 'decision_point_ts_utc_feat_input' or just 'decision_point_ts_utc'\n",
    "                     # if there was no conflict.\n",
    "                     # Let's ensure `decision_timestamp_utc` is the one we use, and `decision_point_ts_utc` (if it's a separate col from right) is dropped.\n",
    "                     if 'decision_point_ts_utc' in df_all_decisions.columns and 'decision_timestamp_utc' in df_all_decisions.columns and 'decision_point_ts_utc' != 'decision_timestamp_utc':\n",
    "                         #This check is if 'decision_point_ts_utc' was NOT part of the original df_all_decisions\n",
    "                         cols_list = df_all_decisions.columns.tolist()\n",
    "                         if cols_list.count('decision_point_ts_utc') > 0 and cols_list.count('decision_timestamp_utc') > 0 :\n",
    "                             if 'decision_point_ts_utc_feat_input' not in cols_list : # if it wasn't suffixed\n",
    "                                 # find the index of the second 'decision_point_ts_utc' if not suffixed\n",
    "                                 # This part is tricky and depends on exact pre-merge columns.\n",
    "                                 # A simpler way is to drop 'decision_point_ts_utc' if it's an extra from the merge.\n",
    "                                 pass # Rely on general duplicate removal below if needed.\n",
    "\n",
    "\n",
    "                # General duplicate column handling (e.g. if kalshi_strike_price was in both)\n",
    "                df_all_decisions = df_all_decisions.loc[:,~df_all_decisions.columns.duplicated(keep='first')]\n",
    "                print(f\"Shape after removing duplicated columns post-feature-merge: {df_all_decisions.shape}\")\n",
    "\n",
    "\n",
    "                # Verify merge - count NaNs in a few key merged features\n",
    "                print(\"Checking NaNs in sample merged features:\")\n",
    "                for col in model_input_feature_names[:5] + model_input_feature_names[-5:]: # Check first 5 and last 5 features\n",
    "                    if col in df_all_decisions.columns:\n",
    "                        nan_count = df_all_decisions[col].isnull().sum()\n",
    "                        total_count = len(df_all_decisions)\n",
    "                        if nan_count == total_count and total_count > 0:\n",
    "                             print(f\"  CRITICAL WARNING: Merged feature '{col}' is ALL NaNs. Merge likely failed for this column or data missing.\")\n",
    "                        elif nan_count > 0:\n",
    "                             print(f\"  Merged feature '{col}' has {nan_count} NaNs (out of {total_count}).\")\n",
    "                    else:\n",
    "                        print(f\"WARNING: Expected feature '{col}' not found after merge and duplicate removal.\")\n",
    "            else:\n",
    "                print(\"df_all_decisions is empty, skipping merge with input features.\")\n",
    "        else:\n",
    "            print(f\"Feature file has no 'decision_point_ts_utc' column or it's not Unix timestamp. Cannot merge features.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or merging training features CSV {PATH_TO_TRAINING_FEATURES_CSV}: {e}\")\n",
    "        df_input_features = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"Training features CSV not found at: {PATH_TO_TRAINING_FEATURES_CSV}. Cannot load input features.\")\n",
    "\n",
    "\n",
    "# --- Final DataFrame Info and Executed Trades Summary ---\n",
    "if not df_all_decisions.empty:\n",
    "    print(f\"\\nShape of df_all_decisions after all potential merges: {df_all_decisions.shape}\")\n",
    "    df_all_decisions.info(verbose=True, show_counts=True)\n",
    "    # print(df_all_decisions.head().to_string()) # Can be very wide if many features merged\n",
    "\n",
    "    df_executed_trades = df_all_decisions[\n",
    "        (df_all_decisions['num_contracts_sim'] > 0) &\n",
    "        (df_all_decisions['executed_trade_action'].isin(['BUY_YES', 'BUY_NO']))\n",
    "    ].copy()\n",
    "    print(f\"\\nNumber of actual executed trades: {len(df_executed_trades)}\")\n",
    "    if not df_executed_trades.empty:\n",
    "        print(\"Sample of executed trades (with potentially more columns if features merged):\")\n",
    "        cols_to_show_initial = ['decision_timestamp_utc', 'market_ticker', 'predicted_proba_yes',\n",
    "                                'executed_trade_action', 'num_contracts_sim', 'simulated_entry_price_cents',\n",
    "                                'pnl_cents', 'actual_market_result', 'edge_for_yes', 'edge_for_no', 'considered_action']\n",
    "        \n",
    "        sample_feature_cols_to_show = []\n",
    "        if model_input_feature_names: # If feature names were loaded\n",
    "             sample_feature_cols_to_show = [fc for fc in model_input_feature_names[:3] + model_input_feature_names[-2:] if fc in df_executed_trades.columns] # first 3, last 2\n",
    "        \n",
    "        cols_to_show = cols_to_show_initial + sample_feature_cols_to_show\n",
    "        \n",
    "        existing_cols_to_show = [col for col in cols_to_show if col in df_executed_trades.columns]\n",
    "        # Remove duplicates from existing_cols_to_show while preserving order\n",
    "        seen = set()\n",
    "        existing_cols_to_show = [x for x in existing_cols_to_show if not (x in seen or seen.add(x))]\n",
    "\n",
    "        print(df_executed_trades[existing_cols_to_show].head().to_string())\n",
    "else:\n",
    "    print(\"df_all_decisions is empty. Cannot proceed with EDA.\")\n",
    "\n",
    "# Define BTC_ATR_WINDOW for Cell 7 if not already (e.g., from feature engineering script params)\n",
    "if 'BTC_ATR_WINDOW' not in globals():\n",
    "    BTC_ATR_WINDOW = 14 # Default, should match what was used in feature_engineering.ipynb\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Overall Performance Summary\n",
    "\n",
    "if not df_all_decisions.empty and not df_executed_trades.empty:\n",
    "    total_pnl = df_executed_trades['pnl_cents'].sum() / 100\n",
    "    total_contracts = df_executed_trades['num_contracts_sim'].sum()\n",
    "    num_trades = len(df_executed_trades)\n",
    "\n",
    "    print(f\"--- Overall Backtest Performance ---\")\n",
    "    print(f\"Total P&L: ${total_pnl:.2f}\")\n",
    "    print(f\"Total Executed Trades: {num_trades}\")\n",
    "    print(f\"Total Contracts Traded: {total_contracts}\")\n",
    "    if num_trades > 0:\n",
    "        print(f\"Average P&L per Trade: ${total_pnl / num_trades:.2f}\")\n",
    "    if total_contracts > 0:\n",
    "        print(f\"Average P&L per Contract: ${total_pnl / total_contracts:.4f}\")\n",
    "\n",
    "    # P&L by Trade Action\n",
    "    pnl_by_action = df_executed_trades.groupby('executed_trade_action')['pnl_cents'].sum() / 100\n",
    "    print(\"\\nP&L by Trade Action:\")\n",
    "    print(pnl_by_action)\n",
    "\n",
    "    # Win/Loss Analysis\n",
    "    df_executed_trades['is_win'] = np.where(df_executed_trades['pnl_cents'] > 0, 1, 0)\n",
    "    win_rate = df_executed_trades['is_win'].mean()\n",
    "    print(f\"\\nOverall Win Rate (Executed Trades): {win_rate:.2%}\")\n",
    "\n",
    "    win_rate_by_action = df_executed_trades.groupby('executed_trade_action')['is_win'].mean()\n",
    "    print(\"\\nWin Rate by Trade Action:\")\n",
    "    print(win_rate_by_action)\n",
    "    \n",
    "    # Cumulative P&L Plot\n",
    "    if 'decision_timestamp_utc' in df_executed_trades.columns:\n",
    "        df_executed_trades_sorted = df_executed_trades.sort_values(by='decision_timestamp_utc')\n",
    "        df_executed_trades_sorted['cumulative_pnl_usd'] = df_executed_trades_sorted['pnl_cents'].cumsum() / 100\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df_executed_trades_sorted['decision_timestamp_utc'], df_executed_trades_sorted['cumulative_pnl_usd'])\n",
    "        plt.title('Cumulative P&L Over Time (Executed Trades)')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Cumulative P&L (USD)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping cumulative P&L plot as 'decision_timestamp_utc' is missing or trades are empty.\")\n",
    "else:\n",
    "    print(\"No executed trades to analyze for overall performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Analysis of Model Probabilities and Edge\n",
    "\n",
    "if not df_all_decisions.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(df_all_decisions['predicted_proba_yes'].dropna(), bins=50, kde=False, label='All Decisions P(YES)')\n",
    "    if not df_executed_trades.empty:\n",
    "        sns.histplot(df_executed_trades['predicted_proba_yes'].dropna(), bins=50, kde=False, color='red', alpha=0.7, label='Executed Trades P(YES)')\n",
    "    plt.title('Distribution of Model Predicted P(YES)')\n",
    "    plt.xlabel('Predicted P(YES)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nDescriptive statistics for predicted_proba_yes (All Decisions):\")\n",
    "    print(df_all_decisions['predicted_proba_yes'].describe())\n",
    "    \n",
    "    if not df_executed_trades.empty:\n",
    "        print(\"\\nDescriptive statistics for predicted_proba_yes (Executed Trades):\")\n",
    "        print(df_executed_trades['predicted_proba_yes'].describe())\n",
    "\n",
    "    edge_cols_exist = all(col in df_all_decisions.columns for col in ['edge_for_yes', 'edge_for_no'])\n",
    "    if edge_cols_exist:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df_all_decisions['edge_for_yes'].dropna(), bins=50, kde=False, label='Edge for YES')\n",
    "        if not df_executed_trades.empty and not df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_YES'].empty: # Check if df is not empty\n",
    "             sns.histplot(df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_YES']['edge_for_yes'].dropna(), \n",
    "                          bins=20, kde=False, color='green', alpha=0.7, label='Executed BUY_YES Edge')\n",
    "        plt.title('Distribution of Edge for YES')\n",
    "        plt.xlabel('Edge (P_model - P_market_ask)')\n",
    "        # Use the variable defined in Cell 1\n",
    "        plt.axvline(STRATEGY_EDGE_THRESHOLD_FOR_TRADE, color='r', linestyle='--', label=f'Edge Threshold ({STRATEGY_EDGE_THRESHOLD_FOR_TRADE})')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(df_all_decisions['edge_for_no'].dropna(), bins=50, kde=False, label='Edge for NO')\n",
    "        if not df_executed_trades.empty and not df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_NO'].empty: # Check if df is not empty\n",
    "            sns.histplot(df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_NO']['edge_for_no'].dropna(), \n",
    "                         bins=20, kde=False, color='orange', alpha=0.7, label='Executed BUY_NO Edge')\n",
    "        plt.title('Distribution of Edge for NO')\n",
    "        plt.xlabel('Edge (P_model_no - P_market_no_bid)')\n",
    "        # Use the variable defined in Cell 1\n",
    "        plt.axvline(STRATEGY_EDGE_THRESHOLD_FOR_TRADE, color='r', linestyle='--', label=f'Edge Threshold ({STRATEGY_EDGE_THRESHOLD_FOR_TRADE})')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nDescriptive statistics for edge_for_yes (All Decisions):\")\n",
    "        print(df_all_decisions['edge_for_yes'].describe())\n",
    "        print(\"\\nDescriptive statistics for edge_for_no (All Decisions):\")\n",
    "        print(df_all_decisions['edge_for_no'].describe())\n",
    "        \n",
    "        if not df_executed_trades.empty:\n",
    "            if not df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_YES'].empty:\n",
    "                print(\"\\nDescriptive statistics for edge_for_yes (Executed BUY_YES Trades):\")\n",
    "                print(df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_YES']['edge_for_yes'].describe())\n",
    "            else:\n",
    "                print(\"\\nNo BUY_YES trades executed to describe edge for.\")\n",
    "            \n",
    "            if not df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_NO'].empty:\n",
    "                print(\"\\nDescriptive statistics for edge_for_no (Executed BUY_NO Trades):\")\n",
    "                print(df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_NO']['edge_for_no'].describe())\n",
    "            else:\n",
    "                print(\"\\nNo BUY_NO trades executed to describe edge for.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\nEdge columns ('edge_for_yes', 'edge_for_no') not found in df_all_decisions. Ensure decision_eval_logs were merged correctly in Cell 1.\")\n",
    "\n",
    "else:\n",
    "    print(\"No data in df_all_decisions for probability/edge analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Analysis of Trade Reasons and Market Conditions\n",
    "\n",
    "if not df_all_decisions.empty:\n",
    "    print(\"\\n--- Analysis of Non-Executed Decisions ---\")\n",
    "    # executed_trade_action column in df_all_decisions will have reasons like 'NO_TRADE_THRESHOLD_NOT_MET' etc.\n",
    "    # from the main trades log.\n",
    "    # The 'considered_action' from df_eval_details (now merged) is more granular for the initial model output.\n",
    "    \n",
    "    if 'considered_action' in df_all_decisions.columns:\n",
    "        print(\"\\nBreakdown of 'considered_action' (from detailed eval logs):\")\n",
    "        print(df_all_decisions['considered_action'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    print(\"\\nBreakdown of 'executed_trade_action' (final outcome in main log):\")\n",
    "    print(df_all_decisions['executed_trade_action'].value_counts(normalize=True) * 100)\n",
    "\n",
    "\n",
    "    if not df_executed_trades.empty:\n",
    "        print(\"\\n--- Analysis of Executed Trades ---\")\n",
    "        # When are trades being made in terms of time_until_market_close_min?\n",
    "        # This requires 'time_until_market_close_min' to be in your features CSV and loaded into df_all_decisions\n",
    "        # If it was part of feature_vector_series, it would be a column if your training features CSV included it.\n",
    "        # For this, we might need to re-extract it or ensure it's in the original CSV.\n",
    "        # Let's assume 'decision_timestamp_utc' and 'market_close_time_iso' (from merge if needed) are available.\n",
    "        \n",
    "        # To get time_until_market_close_min for executed trades, we need market_close_time_iso\n",
    "        # This would typically come from merging with an outcomes file or being part of the feature set.\n",
    "        # For simplicity, let's analyze by hour of day for now.\n",
    "        \n",
    "        df_executed_trades['hour_of_day'] = df_executed_trades['decision_timestamp_utc'].dt.hour\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(data=df_executed_trades, x='hour_of_day', hue='executed_trade_action')\n",
    "        plt.title('Number of Executed Trades by Hour of Day (UTC)')\n",
    "        plt.show()\n",
    "\n",
    "        # P&L vs. Predicted Probability\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # For BUY_YES trades\n",
    "        buy_yes_trades = df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_YES']\n",
    "        if not buy_yes_trades.empty:\n",
    "            sns.scatterplot(data=buy_yes_trades, x='predicted_proba_yes', y='pnl_cents', alpha=0.5, label='BUY_YES')\n",
    "        \n",
    "        # For BUY_NO trades (plot against P(NO) = 1 - P(YES) for consistency on x-axis)\n",
    "        buy_no_trades = df_executed_trades[df_executed_trades['executed_trade_action'] == 'BUY_NO'].copy()\n",
    "        if not buy_no_trades.empty:\n",
    "            buy_no_trades['predicted_proba_no'] = 1 - buy_no_trades['predicted_proba_yes']\n",
    "            # To plot on same P(YES) scale, let's use P(YES) for x-axis for NO trades too.\n",
    "            # Their PNL is based on P(NO) being correct.\n",
    "            sns.scatterplot(data=buy_no_trades, x='predicted_proba_yes', y='pnl_cents', alpha=0.5, label='BUY_NO (P(YES) shown)', marker='x')\n",
    "\n",
    "        plt.title('P&L vs. Model Predicted P(YES) for Executed Trades')\n",
    "        plt.xlabel('Model Predicted P(YES)')\n",
    "        plt.ylabel('P&L (cents)')\n",
    "        plt.axhline(0, color='grey', linestyle='--')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # If edge columns exist, plot P&L vs Edge\n",
    "        if edge_cols_exist:\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            plt.subplot(1,2,1)\n",
    "            if not buy_yes_trades.empty:\n",
    "                sns.scatterplot(data=buy_yes_trades, x='edge_for_yes', y='pnl_cents', alpha=0.5)\n",
    "            plt.title('BUY_YES: P&L vs. Edge for Yes')\n",
    "            plt.xlabel('Edge for YES')\n",
    "            plt.ylabel('P&L (cents)')\n",
    "            plt.axhline(0, color='grey', linestyle='--')\n",
    "\n",
    "            plt.subplot(1,2,2)\n",
    "            if not buy_no_trades.empty:\n",
    "                 sns.scatterplot(data=buy_no_trades, x='edge_for_no', y='pnl_cents', alpha=0.5)\n",
    "            plt.title('BUY_NO: P&L vs. Edge for No')\n",
    "            plt.xlabel('Edge for NO')\n",
    "            plt.ylabel('P&L (cents)')\n",
    "            plt.axhline(0, color='grey', linestyle='--')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"No executed trades to analyze for market conditions.\")\n",
    "else:\n",
    "    print(\"df_all_decisions is empty. Cannot perform further EDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Focus on Losing Trades\n",
    "\n",
    "if not df_executed_trades.empty:\n",
    "    df_losing_trades = df_executed_trades[df_executed_trades['pnl_cents'] < 0].copy()\n",
    "    print(f\"\\n--- Analysis of {len(df_losing_trades)} Losing Trades ---\")\n",
    "\n",
    "    if not df_losing_trades.empty:\n",
    "        print(\"Breakdown of Losing Trades by Action:\")\n",
    "        print(df_losing_trades['executed_trade_action'].value_counts())\n",
    "\n",
    "        print(\"\\nDescriptive Stats for P(YES) in Losing Trades:\")\n",
    "        print(df_losing_trades['predicted_proba_yes'].describe())\n",
    "        \n",
    "        if edge_cols_exist:\n",
    "            print(\"\\nDescriptive Stats for Edge (YES) in Losing BUY_YES Trades:\")\n",
    "            print(df_losing_trades[df_losing_trades['executed_trade_action']=='BUY_YES']['edge_for_yes'].describe())\n",
    "            print(\"\\nDescriptive Stats for Edge (NO) in Losing BUY_NO Trades:\")\n",
    "            print(df_losing_trades[df_losing_trades['executed_trade_action']=='BUY_NO']['edge_for_no'].describe())\n",
    "\n",
    "\n",
    "        # Plot P(chosen side) for losing trades\n",
    "        df_losing_trades['prob_chosen_side_for_loss'] = df_losing_trades['model_prob_chosen_side']\n",
    "        \n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.histplot(data=df_losing_trades, x='prob_chosen_side_for_loss', hue='executed_trade_action', bins=30, kde=True)\n",
    "        plt.title('Distribution of Model Probability for Chosen Side (Losing Trades)')\n",
    "        plt.xlabel('Model P(Chosen Side)')\n",
    "        plt.show()\n",
    "        \n",
    "        # What was the actual outcome when we lost?\n",
    "        print(\"\\nActual Outcomes for Losing Trades:\")\n",
    "        print(pd.crosstab(df_losing_trades['executed_trade_action'], df_losing_trades['actual_market_result']))\n",
    "        \n",
    "        # Display some examples of large losses\n",
    "        print(\"\\nExamples of Large Losing Trades (Top 10 by Loss Amount):\")\n",
    "        print(df_losing_trades.sort_values(by='pnl_cents').head(10)[\n",
    "            ['decision_timestamp_utc', 'market_ticker', 'executed_trade_action', 'predicted_proba_yes', 'model_prob_chosen_side', 'simulated_entry_price_cents', 'actual_market_result', 'pnl_cents']\n",
    "        ].to_string())\n",
    "\n",
    "    else:\n",
    "        print(\"No losing trades to analyze.\")\n",
    "else:\n",
    "    print(\"No executed trades to analyze for losses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Detailed Analysis of P(YES) Buckets\n",
    "\n",
    "if not df_all_decisions.empty and 'predicted_proba_yes' in df_all_decisions.columns:\n",
    "    print(\"\\n--- Analysis of P(YES) Buckets (All Decisions) ---\")\n",
    "    \n",
    "    bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.55, 0.6, 0.65, 0.70, 0.75, 0.80, 0.85, 0.9, 0.95, 1.01] \n",
    "    labels = [f\"{bins[i]:.2f}-{bins[i+1]:.2f}\" for i in range(len(bins)-1)]\n",
    "    df_all_decisions['proba_yes_bucket'] = pd.cut(df_all_decisions['predicted_proba_yes'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "    bucket_analysis = df_all_decisions.groupby('proba_yes_bucket', observed=False).agg(\n",
    "        total_decisions=('actual_market_result', 'count'),\n",
    "        actual_yes_outcomes=('actual_market_result', lambda x: (x == 'yes').sum())\n",
    "    )\n",
    "    bucket_analysis['empirical_p_yes'] = bucket_analysis['actual_yes_outcomes'] / bucket_analysis['total_decisions']\n",
    "    bucket_analysis['avg_predicted_p_yes'] = df_all_decisions.groupby('proba_yes_bucket', observed=False)['predicted_proba_yes'].mean()\n",
    "\n",
    "    # Handle potential NaN results from division by zero if a bucket has no decisions\n",
    "    bucket_analysis.fillna(0, inplace=True)\n",
    "\n",
    "    print(\"\\nModel P(YES) vs. Empirical P(YES) in Backtest Data:\")\n",
    "    print(bucket_analysis)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax1 = bucket_analysis['avg_predicted_p_yes'].plot(kind='bar', alpha=0.7, label='Avg. Model P(YES) in Bucket', color='skyblue')\n",
    "    ax2 = bucket_analysis['empirical_p_yes'].plot(kind='line', marker='o', linewidth=2, label='Empirical P(YES) in Bucket', color='red', secondary_y=False, ax=ax1) # Plot on same y-axis\n",
    "    \n",
    "    # Add y=x line for perfect calibration\n",
    "    lims = [\n",
    "        np.min([ax1.get_xlim(), ax1.get_ylim()]),  # min of both axes\n",
    "        np.max([ax1.get_xlim(), ax1.get_ylim()]),  # max of both axes\n",
    "    ]\n",
    "    ax1.plot(lims, lims, 'k--', alpha=0.75, zorder=0, label='Perfect Calibration')\n",
    "\n",
    "\n",
    "    plt.title('Model P(YES) vs. Empirical P(YES) by Predicted Probability Bucket')\n",
    "    plt.xlabel('Predicted P(YES) Bucket by Model')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    ax1.legend(loc='upper left')\n",
    "    # ax2.legend(loc='upper right') # Not needed if plotted on same axis\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if not df_executed_trades.empty and 'predicted_proba_yes' in df_executed_trades.columns:\n",
    "        actionable_min_prob = df_executed_trades['predicted_proba_yes'].min()\n",
    "        actionable_max_prob = df_executed_trades['predicted_proba_yes'].max()\n",
    "        print(f\"\\nExecuted trades occurred in P(YES) range: [{actionable_min_prob:.4f} - {actionable_max_prob:.4f}]\")\n",
    "        \n",
    "        # Filter buckets based on the range of executed trades\n",
    "        # Accessing CategoricalIndex interval properties\n",
    "        if isinstance(bucket_analysis.index, pd.CategoricalIndex) and isinstance(bucket_analysis.index.categories, pd.IntervalIndex):\n",
    "            actionable_bucket_data = bucket_analysis[\n",
    "                (bucket_analysis.index.categories.left >= actionable_min_prob*0.95) & \n",
    "                (bucket_analysis.index.categories.right <= actionable_max_prob*1.05)\n",
    "            ]\n",
    "            if not actionable_bucket_data.empty:\n",
    "                print(\"\\nCalibration in Actionable Range (approximate):\")\n",
    "                print(actionable_bucket_data)\n",
    "            else:\n",
    "                print(f\"No buckets fully within the approximate actionable range based on current binning.\")\n",
    "        else:\n",
    "            print(\"Could not perform actionable range analysis due to unexpected index type for buckets.\")\n",
    "            \n",
    "else:\n",
    "    print(\"df_all_decisions is empty or 'predicted_proba_yes' column is missing for bucket analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Feature Analysis for Specific Trade Outcomes\n",
    "\n",
    "if not df_all_decisions.empty and not df_executed_trades.empty:\n",
    "    # --- Compare features of Losing BUY_YES vs. Winning BUY_YES (if any) ---\n",
    "    losing_buy_yes = df_executed_trades[(df_executed_trades['executed_trade_action'] == 'BUY_YES') & (df_executed_trades['pnl_cents'] < 0)]\n",
    "    winning_buy_yes = df_executed_trades[(df_executed_trades['executed_trade_action'] == 'BUY_YES') & (df_executed_trades['pnl_cents'] > 0)]\n",
    "\n",
    "    if not losing_buy_yes.empty:\n",
    "        print(f\"\\n--- Losing BUY_YES Trades ({len(losing_buy_yes)}) ---\")\n",
    "        print(\"Losing BUY_YES - P(YES) stats:\")\n",
    "        print(losing_buy_yes['predicted_proba_yes'].describe())\n",
    "        if 'edge_for_yes' in losing_buy_yes.columns:\n",
    "            print(\"Losing BUY_YES - Edge for YES stats:\")\n",
    "            print(losing_buy_yes['edge_for_yes'].describe())\n",
    "            \n",
    "        if 'kalshi_yes_ask_at_decision_t' in losing_buy_yes.columns:\n",
    "            plt.figure(figsize=(8,5))\n",
    "            sns.histplot(losing_buy_yes['kalshi_yes_ask_at_decision_t'].dropna(), bins=20, kde=True)\n",
    "            plt.title('Kalshi YES Ask Price for Losing BUY_YES Trades')\n",
    "            plt.xlabel('YES Ask (cents)')\n",
    "            plt.show()\n",
    "\n",
    "    if not winning_buy_yes.empty:\n",
    "        print(f\"\\n--- Winning BUY_YES Trades ({len(winning_buy_yes)}) ---\")\n",
    "        print(\"Winning BUY_YES - P(YES) stats:\")\n",
    "        print(winning_buy_yes['predicted_proba_yes'].describe())\n",
    "        if 'edge_for_yes' in winning_buy_yes.columns:\n",
    "            print(\"Winning BUY_YES - Edge for YES stats:\")\n",
    "            print(winning_buy_yes['edge_for_yes'].describe())\n",
    "            \n",
    "        if 'kalshi_yes_ask_at_decision_t' in winning_buy_yes.columns:\n",
    "            plt.figure(figsize=(8,5))\n",
    "            sns.histplot(winning_buy_yes['kalshi_yes_ask_at_decision_t'].dropna(), bins=20, kde=True, color='green')\n",
    "            plt.title('Kalshi YES Ask Price for Winning BUY_YES Trades')\n",
    "            plt.xlabel('YES Ask (cents)')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo winning BUY_YES trades found in this backtest period.\")\n",
    "        \n",
    "    # --- Compare high P(YES) predictions that resolved YES vs. NO ---\n",
    "    # Use the variable defined in Cell 1\n",
    "    high_conf_yes_preds = df_all_decisions[df_all_decisions['predicted_proba_yes'] > STRATEGY_MIN_MODEL_PROB_FOR_CONSIDERATION].copy()\n",
    "    \n",
    "    if not high_conf_yes_preds.empty:\n",
    "        print(f\"\\n--- Decisions where model predicted P(YES) > {STRATEGY_MIN_MODEL_PROB_FOR_CONSIDERATION} ({len(high_conf_yes_preds)}) ---\")\n",
    "        \n",
    "        # Ensure 'proba_yes_bucket' is available for crosstab\n",
    "        if 'proba_yes_bucket' not in high_conf_yes_preds.columns and 'predicted_proba_yes' in high_conf_yes_preds.columns:\n",
    "            # Re-apply binning if needed (bins/labels should be defined from Cell 6 or redefined here)\n",
    "            bins_cell7 = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.55, 0.6, 0.65, 0.70, 0.75, 0.80, 0.85, 0.9, 0.95, 1.01] \n",
    "            labels_cell7 = [f\"{bins_cell7[i]:.2f}-{bins_cell7[i+1]:.2f}\" for i in range(len(bins_cell7)-1)]\n",
    "            high_conf_yes_preds['proba_yes_bucket'] = pd.cut(high_conf_yes_preds['predicted_proba_yes'], bins=bins_cell7, labels=labels_cell7, right=False, include_lowest=True)\n",
    "\n",
    "\n",
    "        if 'proba_yes_bucket' in high_conf_yes_preds.columns:\n",
    "            actual_outcomes_for_high_conf_yes = pd.crosstab(high_conf_yes_preds['proba_yes_bucket'], high_conf_yes_preds['actual_market_result'], dropna=False)\n",
    "            print(\"Actual outcomes when model was confident YES (by P(YES) bucket):\")\n",
    "            print(actual_outcomes_for_high_conf_yes)\n",
    "        else:\n",
    "            print(\"Cannot create crosstab: 'proba_yes_bucket' missing from high_conf_yes_preds.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"No decisions found where model predicted P(YES) > {STRATEGY_MIN_MODEL_PROB_FOR_CONSIDERATION}\")\n",
    "\n",
    "else:\n",
    "    print(\"df_all_decisions or df_executed_trades is empty for detailed feature analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW Cell 7: Analyze Input Features for High-Confidence Predictions\n",
    "\n",
    "if 'df_all_decisions' not in globals() or df_all_decisions.empty:\n",
    "    print(\"df_all_decisions is not available. Please run previous cells.\")\n",
    "elif 'model_input_feature_names' not in globals() or not model_input_feature_names:\n",
    "    print(\"model_input_feature_names not loaded (likely feature_columns_classifier_v2.json missing or empty). Cannot perform feature analysis.\")\n",
    "else:\n",
    "    print(\"\\n--- Analyzing Input Features for High-Confidence Predictions ---\")\n",
    "    \n",
    "    # Define \"high confidence\" threshold (can be adjusted)\n",
    "    HIGH_CONF_THRESHOLD = 0.85 # Example: Only look at predictions where P(YES) > 0.85\n",
    "    print(f\"Using HIGH_CONF_THRESHOLD for P(YES): {HIGH_CONF_THRESHOLD}\")\n",
    "\n",
    "    df_high_conf_decisions = df_all_decisions[df_all_decisions['predicted_proba_yes'] > HIGH_CONF_THRESHOLD].copy()\n",
    "    print(f\"Number of decisions with P(YES) > {HIGH_CONF_THRESHOLD}: {len(df_high_conf_decisions)}\")\n",
    "\n",
    "    if not df_high_conf_decisions.empty:\n",
    "        df_high_conf_yes_outcome = df_high_conf_decisions[df_high_conf_decisions['actual_market_result'] == 'yes']\n",
    "        df_high_conf_no_outcome = df_high_conf_decisions[df_high_conf_decisions['actual_market_result'] == 'no']\n",
    "\n",
    "        print(f\"  Outcomes: {len(df_high_conf_yes_outcome)} actual YES, {len(df_high_conf_no_outcome)} actual NO\")\n",
    "\n",
    "        if not df_high_conf_yes_outcome.empty and not df_high_conf_no_outcome.empty:\n",
    "            # --- Compare distributions of key features ---\n",
    "            # Select a subset of features you suspect might be important or problematic\n",
    "            # Or iterate through all model_input_feature_names if you want to be exhaustive (many plots)\n",
    "            \n",
    "            features_to_compare = [\n",
    "                'btc_price_t_minus_1', 'btc_mom_5m', 'btc_mom_60m', \n",
    "                'btc_vol_15m', 'btc_sma_50m', 'btc_price_vs_sma_50m', \n",
    "                'btc_rsi', f'btc_atr_{BTC_ATR_WINDOW if \"BTC_ATR_WINDOW\" in globals() else 14}', # Use global if defined\n",
    "                'kalshi_mid_price', 'kalshi_spread', 'kalshi_mid_chg_10m', 'kalshi_mid_vol_10m',\n",
    "                'distance_to_strike', 'distance_to_strike_norm_atr', \n",
    "                'kalshi_vs_btc_implied_spread',\n",
    "                'time_until_market_close_min'\n",
    "            ]\n",
    "            \n",
    "            # Filter features_to_compare to only those that exist in the dataframe\n",
    "            features_to_compare = [f for f in features_to_compare if f in df_high_conf_decisions.columns]\n",
    "            if not features_to_compare:\n",
    "                 print(\"WARNING: None of the selected 'features_to_compare' exist in the DataFrame. Input features might not have merged correctly.\")\n",
    "            \n",
    "            num_features_to_plot = len(features_to_compare)\n",
    "            if num_features_to_plot > 0:\n",
    "                # Dynamic subplot layout\n",
    "                cols_subplot = 3\n",
    "                rows_subplot = (num_features_to_plot + cols_subplot - 1) // cols_subplot\n",
    "                \n",
    "                plt.figure(figsize=(cols_subplot * 5, rows_subplot * 4))\n",
    "                for i, feature_name in enumerate(features_to_compare):\n",
    "                    plt.subplot(rows_subplot, cols_subplot, i + 1)\n",
    "                    sns.kdeplot(df_high_conf_yes_outcome[feature_name].dropna(), label='Actual YES Outcome', fill=True, alpha=0.5)\n",
    "                    sns.kdeplot(df_high_conf_no_outcome[feature_name].dropna(), label='Actual NO Outcome', fill=True, alpha=0.5)\n",
    "                    plt.title(f'{feature_name} (P(Model YES) > {HIGH_CONF_THRESHOLD})')\n",
    "                    plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # --- Print descriptive statistics for comparison ---\n",
    "                print(f\"\\n--- Descriptive Statistics for Features (P(Model YES) > {HIGH_CONF_THRESHOLD}) ---\")\n",
    "                for feature_name in features_to_compare:\n",
    "                    print(f\"\\nFeature: {feature_name}\")\n",
    "                    stats_yes = df_high_conf_yes_outcome[feature_name].describe()\n",
    "                    stats_no = df_high_conf_no_outcome[feature_name].describe()\n",
    "                    comparison_stats = pd.DataFrame({'Actual_YES': stats_yes, 'Actual_NO': stats_no})\n",
    "                    print(comparison_stats)\n",
    "            else:\n",
    "                print(\"No features selected or available for comparison plotting.\")\n",
    "        else:\n",
    "            print(\"Not enough data for both YES and NO outcomes in high-confidence predictions to compare features.\")\n",
    "    else:\n",
    "        print(f\"No decisions found with P(YES) > {HIGH_CONF_THRESHOLD} to analyze features.\")\n",
    "\n",
    "# It's also useful to look at features of winning vs losing EXECUTED trades\n",
    "if 'df_executed_trades' in globals() and not df_executed_trades.empty:\n",
    "    df_winning_executed = df_executed_trades[df_executed_trades['pnl_cents'] > 0]\n",
    "    df_losing_executed = df_executed_trades[df_executed_trades['pnl_cents'] < 0]\n",
    "\n",
    "    if not df_winning_executed.empty and not df_losing_executed.empty and model_input_feature_names:\n",
    "        print(\"\\n--- Analyzing Input Features for Winning vs. Losing EXECUTED Trades ---\")\n",
    "        \n",
    "        # Use the same features_to_compare list, or define a new one\n",
    "        # features_to_compare_executed = features_to_compare # or a different list\n",
    "        features_to_compare_executed = [f for f in features_to_compare if f in df_executed_trades.columns]\n",
    "\n",
    "\n",
    "        num_features_to_plot_exec = len(features_to_compare_executed)\n",
    "        if num_features_to_plot_exec > 0:\n",
    "            cols_subplot_exec = 3\n",
    "            rows_subplot_exec = (num_features_to_plot_exec + cols_subplot_exec - 1) // cols_subplot_exec\n",
    "            \n",
    "            plt.figure(figsize=(cols_subplot_exec * 5, rows_subplot_exec * 4))\n",
    "            for i, feature_name in enumerate(features_to_compare_executed):\n",
    "                plt.subplot(rows_subplot_exec, cols_subplot_exec, i + 1)\n",
    "                sns.kdeplot(df_winning_executed[feature_name].dropna(), label='Winning Executed', fill=True, alpha=0.5, color='green')\n",
    "                sns.kdeplot(df_losing_executed[feature_name].dropna(), label='Losing Executed', fill=True, alpha=0.5, color='red')\n",
    "                plt.title(f'{feature_name} (Executed Trades)')\n",
    "                plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"\\n--- Descriptive Statistics for Features (Winning vs. Losing Executed Trades) ---\")\n",
    "            for feature_name in features_to_compare_executed:\n",
    "                print(f\"\\nFeature: {feature_name}\")\n",
    "                stats_win_exec = df_winning_executed[feature_name].describe()\n",
    "                stats_lose_exec = df_losing_executed[feature_name].describe()\n",
    "                comparison_stats_exec = pd.DataFrame({'Winning_Executed': stats_win_exec, 'Losing_Executed': stats_lose_exec})\n",
    "                print(comparison_stats_exec)\n",
    "        else:\n",
    "            print(\"No features selected or available for comparison plotting for executed trades.\")\n",
    "    else:\n",
    "        print(\"Not enough winning/losing executed trades or model_input_feature_names not available to compare features.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
