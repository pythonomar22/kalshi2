{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup for backtest.ipynb\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# import os # No longer strictly needed here\n",
    "\n",
    "# Add the 'notebooks' directory to sys.path\n",
    "notebooks_dir = Path.cwd().parent \n",
    "if str(notebooks_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebooks_dir))\n",
    "\n",
    "import pandas as pd\n",
    "# import numpy as np # Not directly used in this cell\n",
    "import logging\n",
    "import datetime as dt # For Cell 4\n",
    "import matplotlib.pyplot as plt # For Cell 4\n",
    "\n",
    "from backtest.backtest_engine import run_backtest \n",
    "import backtest.backtest_utils as utils\n",
    "import backtest.backtest_config as config # To access config variables\n",
    "\n",
    "# --- Logging Setup for the Notebook ---\n",
    "notebook_logger = logging.getLogger(\"backtest_notebook_per_minute\") # MODIFIED\n",
    "if not notebook_logger.handlers:\n",
    "    notebook_logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    notebook_logger.addHandler(ch)\n",
    "else:\n",
    "    notebook_logger.setLevel(logging.INFO)\n",
    "\n",
    "# notebook_logger.info(\"Backtesting Notebook Setup Complete (for Per-Minute Model).\")\n",
    "# notebook_logger.info(f\"Backtest on markets resolving: {config.BACKTEST_START_DATE_STR} to {config.BACKTEST_END_DATE_STR}\")\n",
    "# # notebook_logger.info(f\"Decision Time: EACH MINUTE (driven by feature data rows)\") # Old DECISION_TIME... not used\n",
    "# notebook_logger.info(f\"Model to be used from: {config.MODEL_DIR}\") # This now points to .../logreg_per_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Model and Data\n",
    "\n",
    "try:\n",
    "    notebook_logger.info(\"Loading PER-MINUTE model, scaler, and feature names...\")\n",
    "    model, scaler, model_feature_names = utils.load_model_and_dependencies() # Uses updated paths from config\n",
    "    notebook_logger.info(\"Model components loaded successfully.\")\n",
    "\n",
    "    notebook_logger.info(\"Loading PER-MINUTE features data...\")\n",
    "    all_features_df = utils.load_features_for_backtest() # Uses updated pattern from config\n",
    "    notebook_logger.info(f\"Per-minute features data loaded successfully with {len(all_features_df)} decision points.\")\n",
    "    \n",
    "    if all_features_df.empty:\n",
    "        notebook_logger.critical(\"Features DataFrame is empty. Aborting backtest.\")\n",
    "        raise SystemExit(\"Features DataFrame empty.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    notebook_logger.critical(\"Essential file not found (model, scaler, features, or data). Aborting backtest.\")\n",
    "    # raise\n",
    "except Exception as e:\n",
    "    notebook_logger.critical(f\"An unexpected error occurred during loading: {e}\", exc_info=True)\n",
    "    # raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Run the Backtest\n",
    "\n",
    "if 'model' in locals() and 'all_features_df' in locals() and not all_features_df.empty:\n",
    "    notebook_logger.info(\"Proceeding to run the backtest engine with per-minute decision model...\")\n",
    "    try:\n",
    "        total_pnl_cents, total_trades = run_backtest(all_features_df, model, scaler, model_feature_names)\n",
    "        \n",
    "        notebook_logger.info(\"--- Main Backtest Execution Finished (Per-Minute Model) ---\")\n",
    "        notebook_logger.info(f\"Overall P&L from backtest engine: {total_pnl_cents / 100.0 :.2f} USD\")\n",
    "        notebook_logger.info(f\"Total trades considered/made by engine: {total_trades}\") # \"Made\" is more accurate if action != HOLD\n",
    "        notebook_logger.info(f\"Daily trade logs are in: {config.LOG_DIR}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        notebook_logger.error(f\"An error occurred during run_backtest: {e}\", exc_info=True)\n",
    "else:\n",
    "    notebook_logger.error(\"Model or features data not loaded. Cannot run backtest.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: (Optional) Basic Analysis of Trade Logs\n",
    "\n",
    "# This cell can be expanded to load the daily CSV logs and perform more detailed analysis,\n",
    "# create equity curves, calculate Sharpe ratio, etc.\n",
    "# Ensure matplotlib is imported if not already done in Cell 1\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # ensure imported if not already\n",
    "from pathlib import Path # ensure imported for config.LOG_DIR\n",
    "\n",
    "if config.LOG_DIR.exists():\n",
    "    log_files = list(config.LOG_DIR.glob(\"*_trades.csv\"))\n",
    "    if log_files:\n",
    "        notebook_logger.info(f\"Found {len(log_files)} trade log files:\")\n",
    "        for lf in sorted(log_files): notebook_logger.info(f\" - {lf.name}\")\n",
    "        \n",
    "        all_daily_logs_df = pd.DataFrame()\n",
    "        for lf in sorted(log_files):\n",
    "            try:\n",
    "                daily_df = pd.read_csv(lf)\n",
    "                if not daily_df.empty:\n",
    "                    daily_df['trade_date_file'] = lf.stem.split('_trades')[0] # Date from filename\n",
    "                    all_daily_logs_df = pd.concat([all_daily_logs_df, daily_df], ignore_index=True)\n",
    "            except pd.errors.EmptyDataError: notebook_logger.warning(f\"Log file {lf.name} is empty.\")\n",
    "            except Exception as e: notebook_logger.error(f\"Error reading log file {lf.name}: {e}\")\n",
    "        \n",
    "        if not all_daily_logs_df.empty:\n",
    "            notebook_logger.info(f\"\\nLoaded a total of {len(all_daily_logs_df)} trades from daily logs.\")\n",
    "            if not all_daily_logs_df.empty: display(all_daily_logs_df.head()) # Check before display\n",
    "            \n",
    "            pnl_from_logs = all_daily_logs_df['pnl_cents'].sum()\n",
    "            notebook_logger.info(f\"Total P&L calculated from concatenated log files: {pnl_from_logs / 100.0:.2f} USD\")\n",
    "\n",
    "            if 'trade_execution_time_utc' in all_daily_logs_df.columns and not all_daily_logs_df['trade_execution_time_utc'].isnull().all():\n",
    "                try:\n",
    "                    all_daily_logs_df['trade_datetime_utc'] = pd.to_datetime(all_daily_logs_df['trade_execution_time_utc'])\n",
    "                    all_daily_logs_df.sort_values(by='trade_datetime_utc', inplace=True) # Sort by actual trade time\n",
    "                    all_daily_logs_df['cumulative_pnl_cents'] = all_daily_logs_df['pnl_cents'].cumsum()\n",
    "                    \n",
    "                    plt.figure(figsize=(12,6))\n",
    "                    all_daily_logs_df.set_index('trade_datetime_utc')['cumulative_pnl_cents'].plot()\n",
    "                    plt.title('Cumulative P&L Over Backtest Period (Per-Minute Decisions)')\n",
    "                    plt.xlabel('Trade Execution Time (UTC)')\n",
    "                    plt.ylabel('Cumulative P&L (cents)')\n",
    "                    plt.grid(True)\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    notebook_logger.error(f\"Error plotting P&L curve: {e}\")\n",
    "            else:\n",
    "                notebook_logger.warning(\"Column 'trade_execution_time_utc' not found or all null in logs, cannot plot P&L curve by time.\")\n",
    "        else:\n",
    "            notebook_logger.info(\"No trade data loaded from log files for analysis.\")\n",
    "    else:\n",
    "        notebook_logger.info(\"No trade log files found in the log directory.\")\n",
    "else:\n",
    "    notebook_logger.warning(f\"Log directory {config.LOG_DIR} does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
