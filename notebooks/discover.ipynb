{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 19:48:10,279 - INFO - kalshi_fetch_20250518_194810.<module> - KALSHI: Running in PRODUCTION mode.\n",
      "2025-05-18 19:48:10,335 - INFO - kalshi_fetch_20250518_194810.load_private_key - Private key loaded successfully from /Users/omarabul-hassan/Desktop/projects/kalshi/key.pem\n",
      "2025-05-18 19:48:10,336 - INFO - kalshi_fetch_20250518_194810.<module> - Kalshi client setup complete. Private key loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "import base64\n",
    "import json\n",
    "from pathlib import Path\n",
    "from cryptography.hazmat.primitives import serialization, hashes\n",
    "from cryptography.hazmat.primitives.asymmetric import padding, rsa\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "from dotenv import load_dotenv # Make sure to load it if .env is not in current dir: load_dotenv('path/to/.env')\n",
    "import logging\n",
    "import re \n",
    "\n",
    "# --- Logging Setup ---\n",
    "logger_name = f\"kalshi_fetch_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\" # Added YYYYMMDD for uniqueness\n",
    "logger = logging.getLogger(logger_name)\n",
    "if not logger.handlers: # Avoid adding multiple handlers if re-running cell\n",
    "    logger.setLevel(logging.INFO) \n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s.%(funcName)s - %(message)s')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "else: # If handlers exist, just ensure level is set (e.g. if you change it)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv() # Loads .env file from the current directory or a specified path\n",
    "KALSHI_API_KEY_ID = os.getenv(\"KALSHI_API_KEY_ID\")\n",
    "KALSHI_PRIVATE_KEY_PATH = os.getenv(\"KALSHI_PRIVATE_KEY_PATH\")\n",
    "KALSHI_BASE_URL = \"\" # Will be set based on IS_DEMO_MODE\n",
    "\n",
    "IS_DEMO_MODE = os.getenv(\"KALSHI_DEMO_MODE\", \"false\").lower() == \"true\"\n",
    "if IS_DEMO_MODE:\n",
    "    logger.info(\"KALSHI: Running in DEMO mode.\")\n",
    "    KALSHI_BASE_URL = \"https://demo-api.kalshi.co\" # Standard demo base URL\n",
    "    # For demo, you might need different API keys if they are separate\n",
    "    KALSHI_DEMO_API_KEY_ID = os.getenv(\"KALSHI_DEMO_API_KEY_ID\")\n",
    "    KALSHI_DEMO_PRIVATE_KEY_PATH = os.getenv(\"KALSHI_DEMO_PRIVATE_KEY_PATH\")\n",
    "    if KALSHI_DEMO_API_KEY_ID: KALSHI_API_KEY_ID = KALSHI_DEMO_API_KEY_ID\n",
    "    if KALSHI_DEMO_PRIVATE_KEY_PATH: KALSHI_PRIVATE_KEY_PATH = KALSHI_DEMO_PRIVATE_KEY_PATH\n",
    "else:\n",
    "    logger.info(\"KALSHI: Running in PRODUCTION mode.\")\n",
    "    # The Kalshi API docs you provided use \"https://api.elections.kalshi.com\"\n",
    "    # For general trading API v2, \"https://api.kalshi.com\" is often the base.\n",
    "    # We will use the one from your setup/docs for now.\n",
    "    KALSHI_BASE_URL = \"https://api.elections.kalshi.com\" \n",
    "    # If KALSHI_BASE_URL = \"https://api.elections.kalshi.com\" doesn't work for non-election\n",
    "    # markets like Bitcoin, you might need to change it to \"https://api.kalshi.com\"\n",
    "    # and the full path would be, e.g., \"https://api.kalshi.com/trade-api/v2/events\".\n",
    "\n",
    "# --- Auth Functions ---\n",
    "private_key_global = None # Initialize as global\n",
    "\n",
    "def load_private_key(file_path: str) -> rsa.RSAPrivateKey | None:\n",
    "    global private_key_global # Declare intention to modify global\n",
    "    if not file_path: # Handle case where path might be None\n",
    "        logger.error(\"Private key file path is not provided.\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as key_file:\n",
    "            private_key_global = serialization.load_pem_private_key(key_file.read(), password=None)\n",
    "        logger.info(f\"Private key loaded successfully from {file_path}\")\n",
    "        return private_key_global\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Private key file not found: {file_path}\")\n",
    "        private_key_global = None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading private key from {file_path}: {e}\")\n",
    "        private_key_global = None\n",
    "        return None\n",
    "\n",
    "def sign_pss_text(private_key: rsa.RSAPrivateKey, text: str) -> str | None:\n",
    "    if not private_key:\n",
    "        logger.error(\"Private key not available for signing.\")\n",
    "        return None\n",
    "    message = text.encode('utf-8')\n",
    "    try:\n",
    "        signature = private_key.sign(\n",
    "            message, padding.PSS(mgf=padding.MGF1(hashes.SHA256()), salt_length=padding.PSS.DIGEST_LENGTH),\n",
    "            hashes.SHA256()\n",
    "        )\n",
    "        return base64.b64encode(signature).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during signing: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_kalshi_auth_headers(method: str, path: str) -> dict | None:\n",
    "    if not private_key_global:\n",
    "        logger.error(\"Global private_key_global not loaded. Cannot create auth headers.\")\n",
    "        return None\n",
    "    if not KALSHI_API_KEY_ID:\n",
    "        logger.error(\"Global KALSHI_API_KEY_ID not set. Cannot create auth headers.\")\n",
    "        return None\n",
    "        \n",
    "    timestamp_ms_str = str(int(time.time() * 1000))\n",
    "    # Ensure path starts with '/' for consistency if endpoint_path might vary\n",
    "    if not path.startswith('/'):\n",
    "        path = '/' + path\n",
    "    message_to_sign = timestamp_ms_str + method.upper() + path\n",
    "    signature = sign_pss_text(private_key_global, message_to_sign)\n",
    "    if signature is None: return None\n",
    "    \n",
    "    return {\n",
    "        'accept': 'application/json',\n",
    "        'KALSHI-ACCESS-KEY': KALSHI_API_KEY_ID,\n",
    "        'KALSHI-ACCESS-SIGNATURE': signature,\n",
    "        'KALSHI-ACCESS-TIMESTAMP': timestamp_ms_str\n",
    "    }\n",
    "\n",
    "# --- Initialize private key ---\n",
    "if not (KALSHI_API_KEY_ID and KALSHI_PRIVATE_KEY_PATH):\n",
    "    logger.critical(\"CRITICAL: KALSHI_API_KEY_ID or KALSHI_PRIVATE_KEY_PATH not found in .env file or environment variables.\")\n",
    "else:\n",
    "    # Check if path is absolute, if not, make it relative to a known location (e.g., script dir)\n",
    "    # For simplicity, assuming KALSHI_PRIVATE_KEY_PATH is correctly set (e.g., absolute or relative to cwd)\n",
    "    expanded_path = Path(KALSHI_PRIVATE_KEY_PATH).expanduser().resolve()\n",
    "    if not expanded_path.exists():\n",
    "        logger.critical(f\"CRITICAL: Private key file does not exist at resolved path: {expanded_path}\")\n",
    "        # Attempt to load anyway, load_private_key will log the FileNotFoundError\n",
    "    load_private_key(str(expanded_path))\n",
    "\n",
    "\n",
    "if private_key_global:\n",
    "    logger.info(\"Kalshi client setup complete. Private key loaded.\")\n",
    "else:\n",
    "    logger.error(\"Kalshi client setup failed: Private key could not be loaded. API calls will fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Kalshi API Request Function\n",
    "\n",
    "def kalshi_api_get_request(endpoint_path: str, params: dict = None) -> dict | None:\n",
    "    \"\"\"\n",
    "    Makes an authenticated GET request to the Kalshi API.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_path (str): The API endpoint path (e.g., \"/trade-api/v2/events\").\n",
    "                             It should start with a '/'.\n",
    "        params (dict, optional): Query parameters for the request. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        dict | None: The JSON response as a dictionary, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    if not private_key_global:\n",
    "        logger.error(\"Private key is not loaded. Cannot make API request.\")\n",
    "        return None\n",
    "    if not KALSHI_BASE_URL:\n",
    "        logger.error(\"KALSHI_BASE_URL is not set. Cannot make API request.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure endpoint_path starts with a '/'\n",
    "    if not endpoint_path.startswith('/'):\n",
    "        endpoint_path = '/' + endpoint_path\n",
    "        \n",
    "    full_url = f\"{KALSHI_BASE_URL}{endpoint_path}\"\n",
    "    \n",
    "    # The path for signing should not include the base URL, just the endpoint path like \"/trade-api/v2/events\"\n",
    "    auth_headers = get_kalshi_auth_headers(\"GET\", endpoint_path)\n",
    "    if not auth_headers:\n",
    "        logger.error(f\"Failed to generate authentication headers for path: {endpoint_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Making GET request to: {full_url} with params: {params}\")\n",
    "        response = requests.get(full_url, headers=auth_headers, params=params, timeout=20) # Added timeout\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4XX or 5XX)\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - Status: {response.status_code} - Response: {response.text}\")\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        logger.error(f\"Connection error occurred: {conn_err} to {full_url}\")\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        logger.error(f\"Timeout error occurred: {timeout_err} for {full_url}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        logger.error(f\"An error occurred during the request: {req_err} for {full_url}\")\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"Failed to decode JSON response from {full_url}. Response text: {response.text[:500]}...\") # Log snippet\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Fetch and Display Filtered Bitcoin Markets\n",
    "\n",
    "# --- Configuration for this cell ---\n",
    "# From the example event KXBTCD-25MAY1523, the series ticker is KXBTCD.\n",
    "TARGET_SERIES_TICKER = \"KXBTCD\" \n",
    "\n",
    "# Regex to match event titles like \"Bitcoin price today at 11pm EDT?\"\n",
    "# This is made more general to capture variations like \"Bitcoin price on May 20 at...\"\n",
    "EVENT_TITLE_REGEX_PATTERN = r\"Bitcoin price .*? at \\d{1,2}(?:am|pm)? EDT\\?\"\n",
    "EVENT_TITLE_REGEX = re.compile(EVENT_TITLE_REGEX_PATTERN, re.IGNORECASE)\n",
    "\n",
    "logger.info(f\"Attempting to fetch events for series: {TARGET_SERIES_TICKER} matching pattern: '{EVENT_TITLE_REGEX_PATTERN}'\")\n",
    "\n",
    "all_matching_events_markets = []\n",
    "cursor = None\n",
    "page_count = 0\n",
    "max_pages_to_fetch = 10  # Safety break for pagination to avoid infinite loops during testing\n",
    "fetched_event_count = 0\n",
    "\n",
    "while page_count < max_pages_to_fetch:\n",
    "    page_count += 1\n",
    "    logger.info(f\"Fetching page {page_count} of events for series '{TARGET_SERIES_TICKER}'...\")\n",
    "    \n",
    "    params = {\n",
    "        \"series_ticker\": TARGET_SERIES_TICKER,\n",
    "        \"status\": \"closed,settled\",  # We want historical data for backtesting\n",
    "        \"limit\": 50,                 # Number of events per page (max 200 for /events)\n",
    "        \"with_nested_markets\": \"true\" # Include market data directly in the event response\n",
    "    }\n",
    "    if cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "\n",
    "    # The endpoint for events is /trade-api/v2/events\n",
    "    response_data = kalshi_api_get_request(\"/trade-api/v2/events\", params)\n",
    "\n",
    "    if not response_data:\n",
    "        logger.error(\"Failed to fetch events or response was empty. Stopping pagination.\")\n",
    "        break\n",
    "\n",
    "    events = response_data.get(\"events\", [])\n",
    "    if not events:\n",
    "        logger.info(\"No more events found for this series and status on this page.\")\n",
    "        break\n",
    "    \n",
    "    fetched_event_count += len(events)\n",
    "\n",
    "    for event in events:\n",
    "        event_ticker = event.get(\"ticker\")\n",
    "        event_title = event.get(\"title\")\n",
    "        \n",
    "        if event_title and EVENT_TITLE_REGEX.search(event_title):\n",
    "            logger.info(f\"MATCH: Event '{event_title}' (Ticker: {event_ticker})\")\n",
    "            markets_data = event.get(\"markets\", [])\n",
    "            if markets_data:\n",
    "                event_market_info = {\n",
    "                    \"event_ticker\": event_ticker,\n",
    "                    \"event_title\": event_title,\n",
    "                    \"open_date\": event.get(\"open_date\"),\n",
    "                    \"close_date\": event.get(\"close_date\"),\n",
    "                    \"settlement_date\": event.get(\"settlement_date\"),\n",
    "                    \"markets\": []\n",
    "                }\n",
    "                for market in markets_data:\n",
    "                    market_details = {\n",
    "                        \"market_ticker\": market.get(\"ticker\"),\n",
    "                        \"subtitle\": market.get(\"subtitle\"), # e.g., \"$60000 or more\"\n",
    "                        \"status\": market.get(\"status\"),\n",
    "                        \"result\": market.get(\"result\"), # 'yes' or 'no' for binary markets after settlement\n",
    "                        \"yes_price_latest\": market.get(\"yes_price\"), \n",
    "                        \"no_price_latest\": market.get(\"no_price\"),\n",
    "                    }\n",
    "                    event_market_info[\"markets\"].append(market_details)\n",
    "                all_matching_events_markets.append(event_market_info)\n",
    "            else:\n",
    "                logger.info(f\"  - Event '{event_title}' (Ticker: {event_ticker}) matched title but had no nested markets in this response.\")\n",
    "        # else: # For debugging non-matches\n",
    "        #     if event_title:\n",
    "        #         logger.debug(f\"NO MATCH: Event '{event_title}' (Ticker: {event_ticker})\")\n",
    "        #     else:\n",
    "        #         logger.debug(f\"NO MATCH: Event with no title (Ticker: {event_ticker})\")\n",
    "\n",
    "    cursor = response_data.get(\"cursor\")\n",
    "    if not cursor:\n",
    "        logger.info(\"No more pages (cursor is null). End of event list.\")\n",
    "        break\n",
    "    \n",
    "    logger.info(f\"Fetched {len(events)} events on this page. Moving to next page with cursor: {cursor[:10]}...\")\n",
    "    time.sleep(0.5) # Be respectful to the API, small delay between paginated requests\n",
    "\n",
    "logger.info(f\"Total events scanned from series '{TARGET_SERIES_TICKER}': {fetched_event_count} over {page_count} page(s).\")\n",
    "\n",
    "# --- Displaying the collected market information ---\n",
    "if all_matching_events_markets:\n",
    "    print(f\"\\n--- Found {len(all_matching_events_markets)} Bitcoin Hourly Events (Closed/Settled) Matching Pattern ---\")\n",
    "    for event_data in sorted(all_matching_events_markets, key=lambda x: x.get(\"close_date\") or \"\", reverse=True): # Sort by close_date\n",
    "        print(f\"\\nEvent: {event_data['event_title']} (Ticker: {event_data['event_ticker']})\")\n",
    "        print(f\"  Closes: {event_data.get('close_date')}, Settles: {event_data.get('settlement_date')}\")\n",
    "        if event_data['markets']:\n",
    "            for m_info in event_data['markets']:\n",
    "                print(f\"  Market Ticker: {m_info['market_ticker']}\")\n",
    "                print(f\"    Subtitle: {m_info['subtitle']}\")\n",
    "                print(f\"    Status: {m_info['status']}, Result: {m_info['result']}\")\n",
    "        else:\n",
    "            print(\"  No markets listed for this event.\")\n",
    "else:\n",
    "    logger.warning(f\"No events matching the title regex '{EVENT_TITLE_REGEX.pattern}' were found for series '{TARGET_SERIES_TICKER}' with status 'closed,settled'.\")\n",
    "    logger.warning(\"Possible reasons: \")\n",
    "    logger.warning(\"1. The series_ticker is incorrect or has no such events.\")\n",
    "    logger.warning(\"2. The EVENT_TITLE_REGEX is too restrictive or doesn't match Kalshi's naming convention for these events.\")\n",
    "    logger.warning(\"3. No 'closed' or 'settled' events of this type exist, or they are very old and require more pagination.\")\n",
    "    logger.warning(\"4. API connection or authentication issues (check logs from Cell 1 and Cell 2).\")\n",
    "    logger.warning(f\"5. The base URL '{KALSHI_BASE_URL}' might not serve this type of market data (see notes in Cell 1).\")\n",
    "    print(f\"\\nNo events matching the title regex '{EVENT_TITLE_REGEX_PATTERN}' were found for series '{TARGET_SERIES_TICKER}'.\")\n",
    "    print(\"Please check the logs and configurations.\")\n",
    "\n",
    "# You can now work with `all_matching_events_markets` list of dictionaries.\n",
    "# For example, to create a DataFrame of all market tickers:\n",
    "# market_list_for_df = []\n",
    "# for event_data in all_matching_events_markets:\n",
    "#     for m_info in event_data['markets']:\n",
    "#         market_list_for_df.append({\n",
    "#             'event_ticker': event_data['event_ticker'],\n",
    "#             'event_title': event_data['event_title'],\n",
    "#             'market_ticker': m_info['market_ticker'],\n",
    "#             'market_subtitle': m_info['subtitle'],\n",
    "#             'market_status': m_info['status'],\n",
    "#             'market_result': m_info['result'],\n",
    "#             'event_close_date': event_data.get('close_date')\n",
    "#         })\n",
    "# if market_list_for_df:\n",
    "#    markets_df = pd.DataFrame(market_list_for_df)\n",
    "#    print(\"\\n--- DataFrame of Markets ---\")\n",
    "#    print(markets_df.head())\n",
    "#    print(f\"Total markets found: {len(markets_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 19:36:47,312 - INFO - kalshi_fetch_20250515_192458.<module> - Successfully saved 36455 unique market tickers to kalshi_btc_hourly_market_tickers_KXBTCD_20250515_193647.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved 36455 unique market tickers to: kalshi_btc_hourly_market_tickers_KXBTCD_20250515_193647.csv\n",
      "Sample of saved tickers:\n",
      "                 market_ticker\n",
      "0  KXBTCD-25MAY1522-T112249.99\n",
      "1  KXBTCD-25MAY1522-T111999.99\n",
      "2  KXBTCD-25MAY1522-T111749.99\n",
      "3  KXBTCD-25MAY1522-T111499.99\n",
      "4  KXBTCD-25MAY1522-T111249.99\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Extract Market Tickers and Save to CSV\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# --- Configuration for this cell ---\n",
    "# Ensure `all_matching_events_markets` variable exists from the previous cell execution\n",
    "if 'all_matching_events_markets' not in globals() or not all_matching_events_markets:\n",
    "    logger.error(\"The 'all_matching_events_markets' list is not defined or is empty.\")\n",
    "    print(\"Please ensure Cell 3 has been run successfully and found markets before running this cell.\")\n",
    "else:\n",
    "    market_tickers_list = []\n",
    "    for event_data in all_matching_events_markets:\n",
    "        if event_data.get('markets'): # Check if the 'markets' key exists and is not empty\n",
    "            for market_info in event_data['markets']:\n",
    "                if market_info.get('market_ticker'): # Check if 'market_ticker' key exists\n",
    "                    market_tickers_list.append(market_info['market_ticker'])\n",
    "    \n",
    "    if market_tickers_list:\n",
    "        # Create a DataFrame with a single column for the market tickers\n",
    "        tickers_df = pd.DataFrame(market_tickers_list, columns=['market_ticker'])\n",
    "        \n",
    "        # Remove any duplicate tickers, if any (shouldn't be if data is clean)\n",
    "        tickers_df.drop_duplicates(inplace=True)\n",
    "        \n",
    "        # Define the output CSV file name\n",
    "        # Adding a timestamp to the filename to avoid overwriting and keep track of versions\n",
    "        timestamp_str = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"kalshi_btc_hourly_market_tickers_{TARGET_SERIES_TICKER}_{timestamp_str}.csv\"\n",
    "        \n",
    "        try:\n",
    "            tickers_df.to_csv(csv_filename, index=False)\n",
    "            logger.info(f\"Successfully saved {len(tickers_df)} unique market tickers to {csv_filename}\")\n",
    "            print(f\"\\nSuccessfully saved {len(tickers_df)} unique market tickers to: {csv_filename}\")\n",
    "            print(\"Sample of saved tickers:\")\n",
    "            print(tickers_df.head().to_string())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving market tickers to CSV: {e}\")\n",
    "            print(f\"An error occurred while saving to CSV: {e}\")\n",
    "            \n",
    "    else:\n",
    "        logger.warning(\"No market tickers were extracted from 'all_matching_events_markets'. CSV not created.\")\n",
    "        print(\"No market tickers found to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Fetch Historical Candlestick Data for Markets (Save per Market, Limited N)\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime as dt\n",
    "from datetime import timezone # Explicit import for timezone.utc\n",
    "import glob # To find the latest tickers CSV\n",
    "import os # For creating directory\n",
    "# Ensure tqdm is imported if not already: from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration & Constants for this cell ---\n",
    "if 'TARGET_SERIES_TICKER' not in globals():\n",
    "    TARGET_SERIES_TICKER = \"KXBTCD\" \n",
    "    logger.info(f\"TARGET_SERIES_TICKER was not in globals, set to default: {TARGET_SERIES_TICKER}\")\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# !!!!! SET THE NUMBER OF MARKETS TO PROCESS HERE !!!!!\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "NUMBER_OF_MARKETS_TO_PROCESS = 1000 # <--- CHANGE THIS VALUE AS NEEDED\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "PERIOD_MINUTES = 1\n",
    "MAX_PERIODS_PER_REQUEST = 4900 \n",
    "API_DELAY_SECONDS = 0.75 \n",
    "\n",
    "# Create a directory for individual market CSVs\n",
    "DATA_OUTPUT_DIR_BASE = \"market_candlestick_data\"\n",
    "# Add a suffix to the directory name if processing a limited number, for clarity\n",
    "dir_suffix = f\"_first_{NUMBER_OF_MARKETS_TO_PROCESS}_markets\" if NUMBER_OF_MARKETS_TO_PROCESS > 0 else \"\"\n",
    "INDIVIDUAL_MARKET_DATA_DIR = os.path.join(DATA_OUTPUT_DIR_BASE, f\"{TARGET_SERIES_TICKER}_candlesticks_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}{dir_suffix}\")\n",
    "os.makedirs(INDIVIDUAL_MARKET_DATA_DIR, exist_ok=True)\n",
    "logger.info(f\"Individual market CSVs will be saved in: {INDIVIDUAL_MARKET_DATA_DIR}\")\n",
    "\n",
    "# --- Helper Function to parse ISO date strings to UTC Unix timestamp ---\n",
    "def parse_iso_to_unix_timestamp(date_string: str | None) -> int | None:\n",
    "    if not date_string:\n",
    "        return None\n",
    "    try:\n",
    "        if date_string.endswith('Z'):\n",
    "            dt_obj = dt.datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n",
    "        else: \n",
    "            dt_obj = dt.datetime.fromisoformat(date_string)\n",
    "            if dt_obj.tzinfo is None: \n",
    "                 dt_obj = dt_obj.replace(tzinfo=timezone.utc)\n",
    "        return int(dt_obj.timestamp())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing date string '{date_string}': {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Helper Function to Fetch Market Details (Open/Close Dates) ---\n",
    "def fetch_market_details(market_ticker: str) -> dict | None:\n",
    "    logger.info(f\"Fetching details for market: {market_ticker}\")\n",
    "    api_path = f\"/trade-api/v2/markets/{market_ticker}\"\n",
    "    response_data = kalshi_api_get_request(api_path) \n",
    "    if response_data and \"market\" in response_data:\n",
    "        market_data = response_data[\"market\"]\n",
    "        details = {\n",
    "            \"open_ts\": parse_iso_to_unix_timestamp(market_data.get(\"open_time\")), \n",
    "            \"close_ts\": parse_iso_to_unix_timestamp(market_data.get(\"close_time\")),\n",
    "            \"expiration_ts\": parse_iso_to_unix_timestamp(market_data.get(\"expiration_time\")),\n",
    "            \"expected_expiration_ts\": parse_iso_to_unix_timestamp(market_data.get(\"expected_expiration_time\")),\n",
    "            \"status\": market_data.get(\"status\")\n",
    "        }\n",
    "        if not details[\"open_ts\"] or not details[\"close_ts\"]:\n",
    "             logger.warning(f\"Could not parse open_ts or close_ts for {market_ticker} from 'open_time'/'close_time'. API Response market_data: {market_data}\")\n",
    "             return None\n",
    "        logger.info(f\"Details for {market_ticker}: Open: {details['open_ts']} ({market_data.get('open_time')}), Close: {details['close_ts']} ({market_data.get('close_time')}), Status: {details['status']}\")\n",
    "        return details\n",
    "    else:\n",
    "        logger.error(f\"Failed to fetch or parse details for market: {market_ticker}. Response: {response_data}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Candlestick Fetching Function (Adapted from Professor's Snippet) ---\n",
    "fetch_candlesticks_for_market_first_candle_logged_this_run = False\n",
    "\n",
    "def fetch_candlesticks_for_market(market_ticker: str, \n",
    "                                  series_ticker: str, \n",
    "                                  start_ts_s: int, \n",
    "                                  end_ts_s: int, \n",
    "                                  period_minutes: int) -> list: # Return list, not DataFrame\n",
    "    global fetch_candlesticks_for_market_first_candle_logged_this_run\n",
    "    all_candlesticks_for_this_market = []\n",
    "    if not all([market_ticker, series_ticker, start_ts_s, end_ts_s, period_minutes]):\n",
    "        logger.error(f\"Missing one or more required parameters for fetching candlesticks for {market_ticker}.\")\n",
    "        return all_candlesticks_for_this_market\n",
    "\n",
    "    current_start_ts = start_ts_s\n",
    "    total_duration_seconds = end_ts_s - start_ts_s\n",
    "    period_seconds = period_minutes * 60\n",
    "    if period_seconds == 0: \n",
    "        logger.error(\"Period_seconds is zero, aborting candlestick fetch.\")\n",
    "        return []\n",
    "    total_expected_intervals = (total_duration_seconds // period_seconds) + 1 if total_duration_seconds >= 0 else 0\n",
    "    \n",
    "    logger.info(f\"Preparing to fetch candlesticks for {market_ticker} from {dt.datetime.fromtimestamp(start_ts_s, tz=timezone.utc)} to {dt.datetime.fromtimestamp(end_ts_s, tz=timezone.utc)}\")\n",
    "\n",
    "    with tqdm(total=total_expected_intervals, desc=f\"Candles: {market_ticker[:30]}...\", leave=False, unit=\"candle\") as pbar: # Truncate ticker in desc\n",
    "        while current_start_ts <= end_ts_s:\n",
    "            chunk_max_duration_seconds = (MAX_PERIODS_PER_REQUEST - 1) * period_minutes * 60 \n",
    "            chunk_end_ts = min(end_ts_s, current_start_ts + chunk_max_duration_seconds)\n",
    "            api_path = f\"/trade-api/v2/series/{series_ticker}/markets/{market_ticker}/candlesticks\"\n",
    "            params = {\"start_ts\": current_start_ts, \"end_ts\": chunk_end_ts, \"period_interval\": period_minutes}\n",
    "            logger.debug(f\"Requesting chunk for {market_ticker}: start_ts={current_start_ts}, end_ts={chunk_end_ts}\")\n",
    "            auth_headers = get_kalshi_auth_headers(\"GET\", api_path) \n",
    "            if auth_headers is None:\n",
    "                logger.error(f\"Failed to get auth headers for {market_ticker}. Skipping chunk.\")\n",
    "                pbar.update(MAX_PERIODS_PER_REQUEST) \n",
    "                current_start_ts = chunk_end_ts + (period_minutes * 60) \n",
    "                time.sleep(API_DELAY_SECONDS)\n",
    "                continue\n",
    "            try:\n",
    "                response = requests.get(f\"{KALSHI_BASE_URL}{api_path}\", headers=auth_headers, params=params, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                api_response_data = response.json()\n",
    "                candlesticks_from_api = api_response_data.get(\"candlesticks\", [])\n",
    "                if not fetch_candlesticks_for_market_first_candle_logged_this_run and candlesticks_from_api:\n",
    "                    logger.info(\"Structure of the first candlestick received from API (this session):\")\n",
    "                    logger.info(json.dumps(candlesticks_from_api[0], indent=2))\n",
    "                    fetch_candlesticks_for_market_first_candle_logged_this_run = True\n",
    "\n",
    "                if candlesticks_from_api:\n",
    "                    for candle_data in candlesticks_from_api:\n",
    "                        ts = candle_data.get(\"end_period_ts\")\n",
    "                        if ts is None or ts < current_start_ts or ts > end_ts_s : \n",
    "                            continue \n",
    "                        trade_price_info = candle_data.get(\"price\", {}) or {} \n",
    "                        yes_bid_info = candle_data.get(\"yes_bid\", {}) or {}\n",
    "                        yes_ask_info = candle_data.get(\"yes_ask\", {}) or {}\n",
    "                        all_candlesticks_for_this_market.append({\n",
    "                            \"market_ticker\": market_ticker, \n",
    "                            \"series_ticker\": series_ticker,\n",
    "                            \"timestamp_s\": ts,\n",
    "                            \"datetime_utc\": dt.datetime.fromtimestamp(ts, tz=timezone.utc).isoformat(),\n",
    "                            \"trade_open_cents\": trade_price_info.get(\"open\"), \"trade_high_cents\": trade_price_info.get(\"high\"),\n",
    "                            \"trade_low_cents\": trade_price_info.get(\"low\"), \"trade_close_cents\": trade_price_info.get(\"close\"),\n",
    "                            \"yes_bid_open_cents\": yes_bid_info.get(\"open\"), \"yes_bid_high_cents\": yes_bid_info.get(\"high\"),\n",
    "                            \"yes_bid_low_cents\": yes_bid_info.get(\"low\"), \"yes_bid_close_cents\": yes_bid_info.get(\"close\"),\n",
    "                            \"yes_ask_open_cents\": yes_ask_info.get(\"open\"), \"yes_ask_high_cents\": yes_ask_info.get(\"high\"),\n",
    "                            \"yes_ask_low_cents\": yes_ask_info.get(\"low\"), \"yes_ask_close_cents\": yes_ask_info.get(\"close\"),\n",
    "                            \"volume\": candle_data.get(\"volume\"), \"open_interest\": candle_data.get(\"open_interest\")\n",
    "                        })\n",
    "                    pbar.update(len(candlesticks_from_api) if candlesticks_from_api else (chunk_end_ts - current_start_ts) // period_seconds)\n",
    "                else:\n",
    "                    pbar.update(max(1, (chunk_end_ts - current_start_ts + 1) // period_seconds))\n",
    "\n",
    "                if candlesticks_from_api:\n",
    "                    last_ts_in_chunk = candlesticks_from_api[-1].get(\"end_period_ts\", chunk_end_ts)\n",
    "                    current_start_ts = last_ts_in_chunk + (period_minutes * 60)\n",
    "                else:\n",
    "                    current_start_ts = chunk_end_ts + (period_minutes * 60)\n",
    "            except requests.exceptions.HTTPError as http_err:\n",
    "                logger.error(f\"HTTP error for {market_ticker} chunk: {http_err} - Status: {response.status_code} - Response: {response.text[:200]}\")\n",
    "                pbar.update(MAX_PERIODS_PER_REQUEST) \n",
    "                current_start_ts = chunk_end_ts + (period_minutes * 60) \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Generic error for {market_ticker} chunk: {e}\")\n",
    "                pbar.update(MAX_PERIODS_PER_REQUEST) \n",
    "                current_start_ts = chunk_end_ts + (period_minutes * 60)\n",
    "            \n",
    "            time.sleep(API_DELAY_SECONDS) \n",
    "            if current_start_ts > chunk_end_ts and chunk_end_ts == end_ts_s:\n",
    "                 break\n",
    "    logger.info(f\"Fetched {len(all_candlesticks_for_this_market)} candlesticks for {market_ticker}.\")\n",
    "    return all_candlesticks_for_this_market\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "all_tickers_from_csv = []\n",
    "# Find the latest CSV file with market tickers\n",
    "try:\n",
    "    list_of_files = glob.glob(f\"kalshi_btc_hourly_market_tickers_{TARGET_SERIES_TICKER}_*.csv\") \n",
    "    if not list_of_files:\n",
    "        raise FileNotFoundError(f\"No market ticker CSV files found matching pattern: kalshi_btc_hourly_market_tickers_{TARGET_SERIES_TICKER}_*.csv\")\n",
    "    latest_tickers_csv = max(list_of_files, key=os.path.getctime)\n",
    "    logger.info(f\"Reading market tickers from: {latest_tickers_csv}\")\n",
    "    market_tickers_df = pd.read_csv(latest_tickers_csv)\n",
    "    all_tickers_from_csv = market_tickers_df['market_ticker'].unique().tolist()\n",
    "    logger.info(f\"Found {len(all_tickers_from_csv)} unique market tickers in CSV.\")\n",
    "except FileNotFoundError as fnf_err:\n",
    "    logger.error(f\"CRITICAL: {fnf_err}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"CRITICAL: Error reading or processing the tickers CSV: {e}\")\n",
    "\n",
    "# Determine the actual list of tickers to process based on the limit\n",
    "if not all_tickers_from_csv:\n",
    "    tickers_to_fetch = []\n",
    "    logger.warning(\"No tickers loaded from CSV. Nothing to process.\")\n",
    "elif NUMBER_OF_MARKETS_TO_PROCESS > 0 and NUMBER_OF_MARKETS_TO_PROCESS < len(all_tickers_from_csv):\n",
    "    tickers_to_fetch = all_tickers_from_csv[:NUMBER_OF_MARKETS_TO_PROCESS]\n",
    "    logger.info(f\"Processing the first {NUMBER_OF_MARKETS_TO_PROCESS} markets out of {len(all_tickers_from_csv)} available.\")\n",
    "else:\n",
    "    tickers_to_fetch = all_tickers_from_csv # Process all if limit is 0 or >= total\n",
    "    logger.info(f\"Processing all {len(all_tickers_from_csv)} available markets (limit not restrictive).\")\n",
    "\n",
    "\n",
    "processed_markets_count = 0\n",
    "failed_markets_count = 0\n",
    "total_markets_to_attempt = len(tickers_to_fetch)\n",
    "\n",
    "# --- Outer loop for progress across all tickers ---\n",
    "# Update tqdm total based on the actual number of markets we will process\n",
    "with tqdm(total=total_markets_to_attempt, desc=\"Total Markets Progress\", unit=\"market\") as market_pbar:\n",
    "    for market_ticker in tickers_to_fetch:\n",
    "        market_pbar.set_postfix_str(f\"{market_ticker[:25]}...\") \n",
    "        \n",
    "        market_csv_filename = os.path.join(INDIVIDUAL_MARKET_DATA_DIR, f\"{market_ticker}.csv\")\n",
    "        if os.path.exists(market_csv_filename):\n",
    "            logger.info(f\"Data for {market_ticker} already exists at {market_csv_filename}. Skipping.\")\n",
    "            market_pbar.update(1)\n",
    "            processed_markets_count +=1 \n",
    "            continue\n",
    "        \n",
    "        details = fetch_market_details(market_ticker)\n",
    "        time.sleep(API_DELAY_SECONDS) \n",
    "\n",
    "        if details and details[\"open_ts\"] and details[\"close_ts\"]:\n",
    "            if details[\"open_ts\"] > details[\"close_ts\"]:\n",
    "                logger.warning(f\"Market {market_ticker} has open_ts ({details['open_ts']}) after close_ts ({details['close_ts']}). Skipping.\")\n",
    "                failed_markets_count += 1\n",
    "                market_pbar.update(1)\n",
    "                continue\n",
    "            if details[\"status\"] not in [\"closed\", \"settled\", \"finalized\"]:\n",
    "                 logger.warning(f\"Market {market_ticker} status is '{details['status']}', not 'closed' or 'settled'. Candlestick data might be partial or unavailable. Proceeding with caution.\")\n",
    "            \n",
    "            candlesticks_list = fetch_candlesticks_for_market(\n",
    "                market_ticker=market_ticker,\n",
    "                series_ticker=TARGET_SERIES_TICKER,\n",
    "                start_ts_s=details[\"open_ts\"],\n",
    "                end_ts_s=details[\"close_ts\"], \n",
    "                period_minutes=PERIOD_MINUTES\n",
    "            )\n",
    "            if candlesticks_list:\n",
    "                market_df = pd.DataFrame(candlesticks_list)\n",
    "                market_df.sort_values(by=['timestamp_s'], inplace=True)\n",
    "                try:\n",
    "                    market_df.to_csv(market_csv_filename, index=False)\n",
    "                    logger.info(f\"Successfully saved {len(market_df)} candlesticks for {market_ticker} to {market_csv_filename}\")\n",
    "                    processed_markets_count += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error saving data for {market_ticker} to CSV {market_csv_filename}: {e}\")\n",
    "                    failed_markets_count += 1\n",
    "            else:\n",
    "                logger.info(f\"No candlestick data returned or processed for {market_ticker}. No CSV created for this market.\")\n",
    "        else:\n",
    "            logger.warning(f\"Could not get valid open/close times for {market_ticker}. Skipping candlestick fetch and CSV save.\")\n",
    "            failed_markets_count += 1\n",
    "        \n",
    "        market_pbar.update(1) \n",
    "\n",
    "logger.info(\"--- Candlestick Data Fetching Complete ---\")\n",
    "logger.info(f\"Total markets attempted in this run: {total_markets_to_attempt}\")\n",
    "logger.info(f\"Markets successfully processed and CSV saved (or existed): {processed_markets_count}\")\n",
    "logger.info(f\"Markets failed or skipped: {failed_markets_count}\")\n",
    "logger.info(f\"Individual market CSVs are located in: {INDIVIDUAL_MARKET_DATA_DIR}\")\n",
    "print(f\"\\n--- Data Fetching Complete ---\")\n",
    "print(f\"Data for individual markets saved in directory: {INDIVIDUAL_MARKET_DATA_DIR}\")\n",
    "print(f\"Successfully processed: {processed_markets_count} markets.\")\n",
    "print(f\"Failed/Skipped: {failed_markets_count} markets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 14:36:29,648 - INFO - kalshi_fetch_20250515_192458.fetch_market_details - Fetching details for market: KXBTCD-25MAY1509-T100249.99\n",
      "2025-05-18 14:36:29,658 - INFO - kalshi_fetch_20250515_192458.kalshi_api_get_request - Making GET request to: https://api.elections.kalshi.com/trade-api/v2/markets/KXBTCD-25MAY1509-T100249.99 with params: None\n",
      "2025-05-18 14:36:29,889 - INFO - kalshi_fetch_20250515_192458.fetch_market_details - Details for KXBTCD-25MAY1509-T100249.99: Open: 1747310400 (2025-05-15T12:00:00Z), Close: 1747314000 (2025-05-15T13:00:00Z), Status: finalized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker: KXBTCD-25MAY1509-T100249.99\n",
      "  Raw API open_time used: None\n",
      "  Parsed Open TS: 1747310400 -> UTC: 2025-05-15T12:00:00+00:00\n",
      "  Parsed Close TS: 1747314000 -> UTC: 2025-05-15T13:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# In a new notebook cell, after running Cells 1 & 2\n",
    "# Make sure KALSHI_BASE_URL and private_key_global are set\n",
    "\n",
    "problem_ticker = \"KXBTCD-25MAY1509-T100249.99\" \n",
    "details = fetch_market_details(problem_ticker) # fetch_market_details is from Cell 5 of notebook\n",
    "if details:\n",
    "    open_dt = dt.datetime.fromtimestamp(details[\"open_ts\"], tz=timezone.utc)\n",
    "    close_dt = dt.datetime.fromtimestamp(details[\"close_ts\"], tz=timezone.utc)\n",
    "    print(f\"Ticker: {problem_ticker}\")\n",
    "    print(f\"  Raw API open_time used: {details.get('raw_open_time_from_api_if_logged')}\") # If you logged this in Cell 5\n",
    "    print(f\"  Parsed Open TS: {details['open_ts']} -> UTC: {open_dt.isoformat()}\")\n",
    "    print(f\"  Parsed Close TS: {details['close_ts']} -> UTC: {close_dt.isoformat()}\")\n",
    "else:\n",
    "    print(f\"Could not fetch details for {problem_ticker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 14:50:05,534 - INFO - kalshi_fetch_20250515_192458.<module> - Found latest data directory: market_candlestick_data/KXBTCD_candlesticks_20250515_205224_first_1000_markets\n",
      "2025-05-16 14:50:05,538 - INFO - kalshi_fetch_20250515_192458.<module> - Found 999 CSV files to organize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Organizing data from: market_candlestick_data/KXBTCD_candlesticks_20250515_205224_first_1000_markets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae09236d8aa4defb265e92df90db704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Organizing Files by Date:   0%|          | 0/999 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 14:50:05,658 - INFO - kalshi_fetch_20250515_192458.<module> - --- File Organization Complete ---\n",
      "2025-05-16 14:50:05,659 - INFO - kalshi_fetch_20250515_192458.<module> - Total files processed: 999\n",
      "2025-05-16 14:50:05,659 - INFO - kalshi_fetch_20250515_192458.<module> - Files successfully moved: 999\n",
      "2025-05-16 14:50:05,659 - INFO - kalshi_fetch_20250515_192458.<module> - Files failed or skipped: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- File Organization Complete ---\n",
      "Successfully moved 999 files into date subdirectories.\n",
      "Failed to organize 0 files.\n",
      "Check the directory market_candlestick_data/KXBTCD_candlesticks_20250515_205224_first_1000_markets for new date folders.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Organize Market CSVs into Date Directories\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil # For moving files\n",
    "import re\n",
    "import datetime as dt\n",
    "from pathlib import Path # Useful for path manipulation\n",
    "# Ensure tqdm is imported if not already: from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_OUTPUT_DIR_BASE = \"market_candlestick_data\" # Base directory used in Cell 5\n",
    "TARGET_SERIES_TICKER = \"KXBTCD\" # Ensure this is consistent\n",
    "\n",
    "# --- Find the latest data directory ---\n",
    "# This will find the directory like 'market_candlestick_data/KXBTCD_candlesticks_YYYYMMDD_HHMMSS_suffix'\n",
    "data_directories = glob.glob(os.path.join(DATA_OUTPUT_DIR_BASE, f\"{TARGET_SERIES_TICKER}_candlesticks_*\"))\n",
    "if not data_directories:\n",
    "    logger.critical(f\"CRITICAL: No data directories found in {DATA_OUTPUT_DIR_BASE} matching pattern {TARGET_SERIES_TICKER}_candlesticks_*\")\n",
    "    print(f\"\\nCRITICAL: Could not find a data directory to organize. Please run Cell 5 first.\")\n",
    "else:\n",
    "    # Get the most recently created directory\n",
    "    latest_data_dir = max(data_directories, key=os.path.getctime)\n",
    "    logger.info(f\"Found latest data directory: {latest_data_dir}\")\n",
    "    print(f\"\\nOrganizing data from: {latest_data_dir}\")\n",
    "\n",
    "    # --- Function to extract date component from ticker (re-using logic from Cell 5) ---\n",
    "    # Note: This function assumes the ticker format includes YYMMMDDHH\n",
    "    def get_date_from_ticker_filename(filename: str) -> str | None:\n",
    "        # Expects filenames like KXBTCD-25MAY1520-T94249.99.csv\n",
    "        # Extracts the part like \"25MAY15\"\n",
    "        # Remove the .csv extension first\n",
    "        base_name = filename.replace('.csv', '')\n",
    "        # Use regex to find the YYMMMDD part between the first '-' and the next digit block (the hour)\n",
    "        match = re.search(r'-(\\d{2}[A-Z]{3}\\d{2})\\d{2}-', base_name) \n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        # Fallback for slightly different patterns if any (less likely for these tickers)\n",
    "        match = re.search(r'-(\\d{2}[A-Z]{3}\\d{2})', base_name) # Look for YYMMMDD at the end if no hour/strike is parsed\n",
    "        if match:\n",
    "             logger.warning(f\"Used fallback date extraction for {filename}\")\n",
    "             return match.group(1)\n",
    "\n",
    "        logger.warning(f\"Could not extract date component from filename: {filename}\")\n",
    "        return None\n",
    "\n",
    "    # --- Get all CSV files in the latest data directory ---\n",
    "    all_csv_files = list(Path(latest_data_dir).glob(\"*.csv\"))\n",
    "    if not all_csv_files:\n",
    "        logger.warning(f\"No CSV files found in {latest_data_dir}. Nothing to organize.\")\n",
    "        print(f\"No CSV files found in {latest_data_dir}. Nothing to organize.\")\n",
    "    else:\n",
    "        logger.info(f\"Found {len(all_csv_files)} CSV files to organize.\")\n",
    "        \n",
    "        # --- Iterate and move files ---\n",
    "        moved_count = 0\n",
    "        failed_count = 0\n",
    "\n",
    "        with tqdm(total=len(all_csv_files), desc=\"Organizing Files by Date\", unit=\"file\") as pbar:\n",
    "            for file_path in all_csv_files:\n",
    "                file_name = file_path.name # Get just the filename (e.g., \"KXBTCD-25MAY1520-T94249.99.csv\")\n",
    "                \n",
    "                date_component_str = get_date_from_ticker_filename(file_name)\n",
    "                \n",
    "                if date_component_str:\n",
    "                    try:\n",
    "                        # Parse the YYMMMDD string into a date object (e.g., '25MAY15' -> datetime(2025, 5, 15))\n",
    "                        date_obj = dt.datetime.strptime(date_component_str, \"%y%b%d\").date()\n",
    "                        # Format the date as YYYY-MM-DD for the subdirectory name\n",
    "                        date_subdir_name = date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                        # Create the full path for the date subdirectory\n",
    "                        date_subdir_path = os.path.join(latest_data_dir, date_subdir_name)\n",
    "                        os.makedirs(date_subdir_path, exist_ok=True) # Create the subdirectory if it doesn't exist\n",
    "\n",
    "                        # Define source and destination paths\n",
    "                        source_path = file_path\n",
    "                        destination_path = os.path.join(date_subdir_path, file_name)\n",
    "\n",
    "                        # Move the file\n",
    "                        shutil.move(source_path, destination_path)\n",
    "                        # logger.debug(f\"Moved {file_name} to {date_subdir_path}\")\n",
    "                        moved_count += 1\n",
    "                    except ValueError:\n",
    "                        logger.error(f\"Could not parse date string '{date_component_str}' from filename {file_name}. Skipping.\")\n",
    "                        failed_count += 1\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error organizing file {file_name}: {e}. Skipping.\")\n",
    "                        failed_count += 1\n",
    "                else:\n",
    "                    # get_date_from_ticker_filename already logged a warning\n",
    "                    failed_count += 1\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        logger.info(\"--- File Organization Complete ---\")\n",
    "        logger.info(f\"Total files processed: {len(all_csv_files)}\")\n",
    "        logger.info(f\"Files successfully moved: {moved_count}\")\n",
    "        logger.info(f\"Files failed or skipped: {failed_count}\")\n",
    "        print(f\"\\n--- File Organization Complete ---\")\n",
    "        print(f\"Successfully moved {moved_count} files into date subdirectories.\")\n",
    "        print(f\"Failed to organize {failed_count} files.\")\n",
    "        print(f\"Check the directory {latest_data_dir} for new date folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 14:55:13,882 - INFO - kalshi_fetch_20250515_192458.<module> - Using source directory from previous cell: market_candlestick_data/KXBTCD_candlesticks_20250515_205224_first_1000_markets\n",
      "2025-05-16 14:55:13,883 - INFO - kalshi_fetch_20250515_192458.<module> - Organized data will be placed under: organized_market_data\n",
      "2025-05-16 14:55:13,884 - INFO - kalshi_fetch_20250515_192458.<module> - Found 999 CSV files in market_candlestick_data/KXBTCD_candlesticks_20250515_205224_first_1000_markets to organize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4c49b9b32f4ce88546fe15d6203cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Organizing Files:   0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 14:55:13,977 - INFO - kalshi_fetch_20250515_192458.<module> - --- File Organization Complete ---\n",
      "2025-05-16 14:55:13,978 - INFO - kalshi_fetch_20250515_192458.<module> - Files successfully moved: 999\n",
      "2025-05-16 14:55:13,978 - INFO - kalshi_fetch_20250515_192458.<module> - Files skipped (e.g., already existed at target or error): 0\n",
      "2025-05-16 14:55:13,978 - INFO - kalshi_fetch_20250515_192458.<module> - Files whose names failed to parse: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- File Organization Complete ---\n",
      "Moved 999 files into the new structure under 'organized_market_data'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Organize Market CSVs into Date/Hour Folders\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil # For moving files\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration for this cell ---\n",
    "\n",
    "# This should be the directory where Cell 5 saved the individual market CSVs.\n",
    "# Let's try to get it dynamically if `INDIVIDUAL_MARKET_DATA_DIR` is still in scope.\n",
    "# Otherwise, you might need to set it manually.\n",
    "if 'INDIVIDUAL_MARKET_DATA_DIR' in globals() and os.path.exists(INDIVIDUAL_MARKET_DATA_DIR):\n",
    "    SOURCE_DATA_DIR = INDIVIDUAL_MARKET_DATA_DIR\n",
    "    logger.info(f\"Using source directory from previous cell: {SOURCE_DATA_DIR}\")\n",
    "else:\n",
    "    # !! MANUALLY SET THIS IF THE ABOVE IS NOT FOUND !!\n",
    "    # Example: SOURCE_DATA_DIR = \"/Users/omarabul-hassan/Desktop/projects/kalshi/notebooks/market_candlestick_data/KXBTCD_candlesticks_20250515_205224_first_1000_markets\"\n",
    "    # Try to find the latest one if not in scope\n",
    "    try:\n",
    "        list_of_data_dirs = glob.glob(os.path.join(\"market_candlestick_data\", f\"{TARGET_SERIES_TICKER}_candlesticks_*\"))\n",
    "        if not list_of_data_dirs:\n",
    "            raise FileNotFoundError(\"No candlestick data directories found.\")\n",
    "        SOURCE_DATA_DIR = max(list_of_data_dirs, key=os.path.getctime)\n",
    "        logger.info(f\"Dynamically found latest source directory: {SOURCE_DATA_DIR}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not find INDIVIDUAL_MARKET_DATA_DIR automatically. Please set SOURCE_DATA_DIR manually. Error: {e}\")\n",
    "        SOURCE_DATA_DIR = None # Needs to be set\n",
    "\n",
    "# Base directory for the new organized structure\n",
    "BASE_ORGANIZED_DIR = \"organized_market_data\" \n",
    "os.makedirs(BASE_ORGANIZED_DIR, exist_ok=True)\n",
    "logger.info(f\"Organized data will be placed under: {BASE_ORGANIZED_DIR}\")\n",
    "\n",
    "# Regex to parse the market ticker filename\n",
    "# Example: KXBTCD-25MAY1522-T104499.99.csv\n",
    "# We need to capture:\n",
    "# 1. The full date part (e.g., 25MAY15)\n",
    "# 2. The hour part (e.g., 22)\n",
    "TICKER_FILENAME_REGEX = re.compile(r\"^(KXBTCD)-(\\d{2}[A-Z]{3}\\d{2})(\\d{2})-(T\\d+\\.\\d{2})\\.csv$\")\n",
    "# Group 1: Series (KXBTCD)\n",
    "# Group 2: Date (e.g., 25MAY15)\n",
    "# Group 3: Hour (e.g., 22)\n",
    "# Group 4: Strike (e.g., T104499.99)\n",
    "\n",
    "moved_files_count = 0\n",
    "skipped_files_count = 0\n",
    "failed_to_parse_count = 0\n",
    "\n",
    "if SOURCE_DATA_DIR and os.path.isdir(SOURCE_DATA_DIR):\n",
    "    csv_files = [f for f in os.listdir(SOURCE_DATA_DIR) if f.endswith('.csv')]\n",
    "    logger.info(f\"Found {len(csv_files)} CSV files in {SOURCE_DATA_DIR} to organize.\")\n",
    "\n",
    "    for filename in tqdm(csv_files, desc=\"Organizing Files\"):\n",
    "        match = TICKER_FILENAME_REGEX.match(filename)\n",
    "        if match:\n",
    "            series_ticker_part = match.group(1) # Should be KXBTCD\n",
    "            date_part = match.group(2)          # e.g., 25MAY15\n",
    "            hour_part = match.group(3)          # e.g., 22\n",
    "            # strike_part = match.group(4)      # e.g., T104499.99 (not used for dir structure but good to have)\n",
    "\n",
    "            # Create the target directory structure: BASE_ORGANIZED_DIR / DATE / HOUR\n",
    "            target_date_dir = os.path.join(BASE_ORGANIZED_DIR, date_part)\n",
    "            target_hour_dir = os.path.join(target_date_dir, hour_part)\n",
    "            \n",
    "            os.makedirs(target_hour_dir, exist_ok=True) # Create dirs if they don't exist\n",
    "\n",
    "            source_filepath = os.path.join(SOURCE_DATA_DIR, filename)\n",
    "            target_filepath = os.path.join(target_hour_dir, filename)\n",
    "\n",
    "            try:\n",
    "                if not os.path.exists(target_filepath): # Avoid error if re-running and file already moved\n",
    "                    shutil.move(source_filepath, target_filepath)\n",
    "                    # logger.debug(f\"Moved: {source_filepath} -> {target_filepath}\")\n",
    "                    moved_files_count += 1\n",
    "                else:\n",
    "                    logger.warning(f\"File already exists at target, skipping move: {target_filepath}\")\n",
    "                    # If you want to count these as \"skipped because exists\" rather than an error:\n",
    "                    skipped_files_count += 1 \n",
    "                    # Optionally, you could delete the source if it's a duplicate from a re-run attempt\n",
    "                    # os.remove(source_filepath) \n",
    "                    # logger.info(f\"Removed duplicate source: {source_filepath}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error moving file {filename}: {e}\")\n",
    "                skipped_files_count += 1\n",
    "        else:\n",
    "            logger.warning(f\"Filename {filename} did not match expected pattern. Skipping.\")\n",
    "            failed_to_parse_count += 1\n",
    "            \n",
    "    logger.info(\"--- File Organization Complete ---\")\n",
    "    logger.info(f\"Files successfully moved: {moved_files_count}\")\n",
    "    logger.info(f\"Files skipped (e.g., already existed at target or error): {skipped_files_count}\")\n",
    "    logger.info(f\"Files whose names failed to parse: {failed_to_parse_count}\")\n",
    "    print(f\"\\n--- File Organization Complete ---\")\n",
    "    print(f\"Moved {moved_files_count} files into the new structure under '{BASE_ORGANIZED_DIR}'.\")\n",
    "    if skipped_files_count > 0:\n",
    "        print(f\"Skipped {skipped_files_count} files (e.g., target existed or move error).\")\n",
    "    if failed_to_parse_count > 0:\n",
    "        print(f\"Could not parse filename for {failed_to_parse_count} files.\")\n",
    "\n",
    "else:\n",
    "    logger.error(f\"SOURCE_DATA_DIR '{SOURCE_DATA_DIR}' is not set or is not a valid directory. Please check the path.\")\n",
    "    print(f\"Error: SOURCE_DATA_DIR '{SOURCE_DATA_DIR}' is not valid. Please set it correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market outcomes saved to: market_candlestick_data/kalshi_btc_hourly_market_outcomes.csv\n"
     ]
    }
   ],
   "source": [
    "# In your Jupyter Notebook, after Cell 3 has run\n",
    "market_outcomes_data = []\n",
    "if 'all_matching_events_markets' in globals() and all_matching_events_markets:\n",
    "    for event_data in all_matching_events_markets:\n",
    "        event_close_date_iso = event_data.get('close_date') # This is the event resolution time\n",
    "        for m_info in event_data['markets']:\n",
    "            market_outcomes_data.append({\n",
    "                'market_ticker': m_info['market_ticker'],\n",
    "                'result': m_info['result'], # 'yes' or 'no'\n",
    "                'event_resolution_time_iso': event_close_date_iso\n",
    "            })\n",
    "    market_outcomes_df = pd.DataFrame(market_outcomes_data)\n",
    "    outcomes_csv_path = os.path.join(os.path.dirname(INDIVIDUAL_MARKET_DATA_DIR), \"kalshi_btc_hourly_market_outcomes.csv\") # Save it one level up from the specific run\n",
    "    market_outcomes_df.to_csv(outcomes_csv_path, index=False)\n",
    "    print(f\"Market outcomes saved to: {outcomes_csv_path}\")\n",
    "else:\n",
    "    print(\"Please run Cell 3 first to populate 'all_matching_events_markets'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
